{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Edge_Probing_Full.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r5t_gYLZLT9S",
        "dmefJ5pCqq7D",
        "zJ6Ic5hySOY_",
        "mT6GHy0Kf8Ku"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenfayyaz/edge-probe/blob/main/Edge_Probing_Full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUqdV6uy7PZJ"
      },
      "source": [
        "# Installations & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljSyeFv7JTtf"
      },
      "source": [
        "! nproc\n",
        "! lscpu\n",
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C9hC77g65Je"
      },
      "source": [
        "# ! gdown --id ***\n",
        "# ! tar -xzf ontonotes_data.tar.gz\n",
        "\n",
        "! rm -r ./edge-probing-datasets\n",
        "! git clone https://github.com/mohsenfayyaz/edge-probing-datasets.git\n",
        "\n",
        "# ! git clone https://github.com/mohsenfayyaz/disk-cache.git\n",
        "# ! mv disk-cache disk_cache\n",
        "\n",
        "! pip install datasets\n",
        "! pip install transformers\n",
        "! pip install sentencepiece\n",
        "! pip install diskcache\n",
        "# ! pip install tensorflow==1.15\n",
        "# ! pip install \"tensorflow_hub>=0.6.0\"\n",
        "# ! pip install allennlp\n",
        "# ! pip install allennlp==0.9.0\n",
        "\n",
        "# ! pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTbdjm8I7mka"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import torch\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os\n",
        "import json\n",
        "import gc\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from abc import ABC, abstractmethod\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import f1_score\n",
        "import sklearn\n",
        "import psutil  # RAM usage\n",
        "import re\n",
        "import random\n",
        "import datasets\n",
        "import copy\n",
        "import pickle\n",
        "# import diskcache as dc\n",
        "\n",
        "# import shutil\n",
        "# shutil.rmtree('caches', ignore_errors=True)\n",
        "# shutil.rmtree('cache_tmp', ignore_errors=True)\n",
        "# disk_cache = dc.Cache('cache_tmp', size_limit=int(100e9))\n",
        "# disk_cache = dc.Index('cache_tmp', size_limit=int(150e9), sqlite_cache_size=-int(6e9), sqlite_mmap_size=int(6e9))\n",
        "# disk_cache[\"a\"] = [2.31548*i**2 for i in range(768*13*3*32)]\n",
        "# %timeit disk_cache[\"a\"]\n",
        "\n",
        "# from disk_cache.DiskCache import DiskCache\n",
        "# disk_cache = DiskCache(\"mohsen_cache\")\n",
        "# disk_cache[\"a\"] = [2.31548*i**2 for i in range(768*13*3*32)]\n",
        "# %timeit disk_cache[\"a\"]\n",
        "\n",
        "# import shelve\n",
        "# disk_cache = shelve.open(\"cache_shelve\")\n",
        "# import wandb\n",
        "# wandb.init()\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41xul5KY7xvt"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ3e1TgD9fbe"
      },
      "source": [
        "class Dataset_info:\n",
        "    def __init__(self, dataset_name, num_of_spans, max_span_length=5, ignore_classes=[], manual_text=None):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.num_of_spans = num_of_spans\n",
        "        self.ignore_classes = ignore_classes  # ignore other class in rel (semeval)\n",
        "        self.manual_text = manual_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x1M223x7zcD"
      },
      "source": [
        "# model_checkpoint = \"bert-base-uncased\"\n",
        "# model_checkpoint = \"bert-base-cased\"\n",
        "# model_checkpoint = \"bert-large-uncased\"\n",
        "# model_checkpoint = \"distilbert-base-cased\"\n",
        "# model_checkpoint = \"distilbert-base-uncased\"\n",
        "# model_checkpoint = \"google/bert_uncased_L-12_H-768_A-12\"\n",
        "# model_checkpoint = \"roberta-base\"\n",
        "# model_checkpoint = \"roberta-large\"\n",
        "\n",
        "# model_checkpoint = \"bert-base-multilingual-uncased\"\n",
        "model_checkpoint = \"xlm-roberta-base\"\n",
        "\n",
        "# model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "\n",
        "# model_checkpoint = \"google/electra-base-discriminator\"\n",
        "# model_checkpoint = \"google/electra-large-discriminator\"\n",
        "\n",
        "# model_checkpoint = 'xlnet-base-cased'\n",
        "# model_checkpoint = 'mohsenfayyaz/xlnet-base-cased-zihangdai'\n",
        "# model_checkpoint = \"xlnet-large-cased\"\n",
        "\n",
        "# model_checkpoint = \"albert-base-v2\"\n",
        "# model_checkpoint = \"albert-large-v2\"\n",
        "# model_checkpoint = \"albert-xlarge-v2\"\n",
        "# model_checkpoint = \"albert-xxlarge-v2\"\n",
        "\n",
        "# model_checkpoint = \"howey/bert-base-uncased-rte\"\n",
        "\n",
        "# model_checkpoint = \"TehranNLP-org/bert-base-uncased-avg-mnli-2e-5-21\"\n",
        "# model_checkpoint = \"TehranNLP-org/xlnet-base-cased-avg-mnli-2e-5-21\"\n",
        "# model_checkpoint = \"TehranNLP-org/electra-base-avg-mnli-2e-5-21\"\n",
        "\n",
        "# model_checkpoint = \"YituTech/conv-bert-base\"\n",
        "\n",
        "# model_checkpoint = \"t5-small\"\n",
        "# model_checkpoint = \"t5-base\"\n",
        "# model_checkpoint = \"t5-large\"\n",
        "\n",
        "# model_checkpoint = \"facebook/bart-base\"\n",
        "# model_checkpoint = \"facebook/bart-large-xsum\"\n",
        "\n",
        "# model_checkpoint = \"gpt2\"\n",
        "# model_checkpoint = \"google/pegasus-arxiv\"\n",
        "# model_checkpoint = \"google/t5-large-ssm\"\n",
        "# model_checkpoint = \"google/t5-small-ssm-nq\"\n",
        "# model_checkpoint = \"google/mt5-base\"\n",
        "# model_checkpoint = \"google/pegasus-large\"\n",
        "# model_checkpoint = \"microsoft/mpnet-base\"\n",
        "\n",
        "# model_checkpoint = \"glove.twitter.27B.200d\"\n",
        "\n",
        "# model_checkpoint = \"elmo\"\n",
        "\n",
        "\n",
        "# model_checkpoint = 'TehranNLP-org/electra-base-mrpc-2e-5-42'\n",
        "# model_checkpoint = 'TehranNLP-org/roberta-base-mrpc-2e-5-42'\n",
        "\n",
        "\n",
        "\n",
        "## LINGUISTIC KNOWLEDGE ##\n",
        "# my_dataset_info = Dataset_info(\"const\", num_of_spans=1)  # Dependency Labeling\n",
        "# my_dataset_info = Dataset_info(\"ud\", num_of_spans=2)  # Dependency Labeling\n",
        "# my_dataset_info = Dataset_info(\"ner\", num_of_spans=1)  # Named Entity Labeling\n",
        "# my_dataset_info = Dataset_info(\"srl\", num_of_spans=2)  # Semantic Role Labeling\n",
        "# my_dataset_info = Dataset_info(\"coref\", num_of_spans=2)  # Coreference Ontonotes\n",
        "# my_dataset_info = Dataset_info(\"dpr\", num_of_spans=2)  # Coreference Winograd\n",
        "# my_dataset_info = Dataset_info(\"semeval\", num_of_spans=2, ignore_classes=[\"Other\"])  # Relation Classification\n",
        "# my_dataset_info = Dataset_info(\"semeval\", num_of_spans=2)  # Relation Classification\n",
        "\n",
        "### TOXICICITY ##\n",
        "# my_dataset_info = Dataset_info(\"hatexplain\", num_of_spans=1)\n",
        "# my_dataset_info = Dataset_info(\"hatexplain-fullspan\", num_of_spans=1)\n",
        "# my_dataset_info = Dataset_info(\"offenseval2019\", num_of_spans=1)\n",
        "# my_dataset_info = Dataset_info(\"jigsaw_bias\", num_of_spans=1)\n",
        "\n",
        "### METAPHOR ###\n",
        "# my_dataset_info = Dataset_info(\"vua_verb\", num_of_spans=1)\n",
        "# my_dataset_info = Dataset_info(\"vua_pos\", num_of_spans=1)\n",
        "# my_dataset_info = Dataset_info(\"trofi\", num_of_spans=1)\n",
        "# my_dataset_info = Dataset_info(\"trofi_nospan\", num_of_spans=1)\n",
        "\n",
        "# my_dataset_info = Dataset_info(\"lcc\", num_of_spans=2)\n",
        "# my_dataset_info = Dataset_info(\"lcc_src_concept\", num_of_spans=1, ignore_classes=[\"OTHER\", 'ANIMAL', 'EMOTION_EXPERIENCER', 'STORY', 'INSANITY', 'NATURAL_PHYSICAL_FORCE', 'CLOTHING', 'JOURNEY', 'CROP', 'DOWNWARD_MOVEMENT', 'OBJECT_HANDLING', 'ENERGY', 'MEDICINE', 'UPWARD_MOVEMENT', 'LIGHT', 'LEADER', 'TOOL', 'MONSTER', 'BUSINESS', 'COMPETITION', 'FOOD', 'A_GOD', 'HIGH_LOCATION', 'SCIENCE', 'MAGIC', 'PATHWAY', 'LIFE_STAGE', 'SIZE', 'WEAKNESS', 'GIFT', 'ADDICTION', 'ENSLAVEMENT', 'THEFT', 'BARRIER', 'FAMILY', 'MAZE', 'FIRE', 'PORTAL', 'VEHICLE', 'LOW_LOCATION', 'FURNISHINGS', 'CONTAMINATION', 'FABRIC', 'GAP', 'FORWARD_MOVEMENT', 'PLIABILITY', 'A_RIGHT', 'WEATHER', 'SERVANT', 'TEMPERATURE', 'RULE_ENFORCER', 'CONTAINER', 'MORAL_DUTY', 'FORCEFUL_EXTRACTION', 'FACTORY', 'BLOOD_STREAM', 'ACCIDENT', 'POSITION_AND_CHANGE_OF_POSITION_ON_A_SCALE', 'BACKWARD_MOVEMENT', 'DARKNESS', 'DESIRE', 'AVERSION', 'OBESITY', 'PARASITE', 'HAZARDOUS_GEOGRAPHIC_FEATURE', 'STAGE', 'DESTROYER', 'BATTLE', 'PHYSICAL_OBJECT', 'IMPURITY', 'HIGH_POINT', 'GEOGRAPHIC_FEATURE', 'GOAL_DIRECTED', 'VERTICAL_SCALE', 'DELICACY', 'MOVEMENT_ON_A_VERTICAL_SCALE', 'RACE', 'LOW_POINT', 'TRIBUTE', 'SCHISM', 'INDUSTRY', 'BLOOD_SYSTEM', 'EMPLOYEE', 'GREED', 'GOURMET_CUISINE', 'CONTROL', 'MATERIAL'])  ## Use Second(Target) Span to Predict Src\n",
        "# my_dataset_info = Dataset_info(\"lcc_src_target_concept\", num_of_spans=2, ignore_classes=[\"OTHER\"])\n",
        "\n",
        "# my_dataset_info = Dataset_info(\"lcc_fa\", num_of_spans=2)\n",
        "# my_dataset_info = Dataset_info(\"lcc_es\", num_of_spans=2)\n",
        "# my_dataset_info = Dataset_info(\"lcc_ru\", num_of_spans=2)\n",
        "\n",
        "# Cross-Lingual\n",
        "# my_dataset_info = Dataset_info(\"lcc_en_fa\", num_of_spans=2)\n",
        "# my_dataset_info = Dataset_info(\"lcc_en_es\", num_of_spans=2)\n",
        "# my_dataset_info = Dataset_info(\"lcc_en_ru\", num_of_spans=2)\n",
        "# my_dataset_info = Dataset_info(\"lcc_es_fa\", num_of_spans=2)\n",
        "# my_dataset_info = Dataset_info(\"lcc_es_ru\", num_of_spans=2)\n",
        "my_dataset_info = Dataset_info(\"lcc_fa_ru\", num_of_spans=2)\n",
        "\n",
        "# my_dataset_info = Dataset_info(\"lcc_en+fa_fa\", num_of_spans=2)\n",
        "# my_dataset_info = Dataset_info(\"lcc_fa_en\", num_of_spans=2)\n",
        "# my_dataset_info = Dataset_info(\"lcc_en+es_es\", num_of_spans=2)\n",
        "# my_dataset_info = Dataset_info(\"lcc_en+ru_ru\", num_of_spans=2)\n",
        "\n",
        "POOL_METHOD = \"attn\"  # 'max', 'attn'\n",
        "BATCH_SIZE = 32\n",
        "SEED = 0\n",
        "LEARNING_RATE = 5e-5\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "CACHE_LEN = 500\n",
        "GPU_CACHE_LEN = 600  # 600\n",
        "RAM_CACHE_LEN = 1500  # 2000\n",
        "SEQ2SEQ_MODEL = \"t5\" in model_checkpoint or \"pegasus\" in model_checkpoint or \"bart\" in model_checkpoint\n",
        "\n",
        "print(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j89KOFP8mhGD"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "set_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF_EvIUJWMWy"
      },
      "source": [
        "# Glove\n",
        "if \"glove\" in model_checkpoint and not os.path.exists(\"./glove.twitter.27B.200d.txt\"):\n",
        "    ! pip install glove-python-binary\n",
        "    import glove\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ! cp \"./drive/MyDrive/Colab Notebooks/NLP/NLP Project/dataset/glove.twitter.27B.zip\" \"./\"\n",
        "    ! unzip glove*.zip\n",
        "    word_embedding = glove.Glove.load_stanford(\"./glove.twitter.27B.200d.txt\")\n",
        "    \n",
        "if os.path.exists(\"./glove.twitter.27B.200d.txt\"):\n",
        "    print(word_embedding.word_vectors.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRAmX2Cl8XHh"
      },
      "source": [
        "# Prepare Dataset & Spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxwmMoFV8YwW"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "if \"glove\" in model_checkpoint:\n",
        "    model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
        "    tokenizer.padding_side = 'right'\n",
        "    model = AutoModel.from_pretrained(model_checkpoint)\n",
        "\n",
        "# model.save_pretrained(model_checkpoint)\n",
        "# tokenizer.save_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxlLr5yvkLiK"
      },
      "source": [
        "class Utils:\n",
        "    def one_hot(idx, length):\n",
        "        import numpy as np\n",
        "        o = np.zeros(length, dtype=np.int8)\n",
        "        o[idx] = 1\n",
        "        return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJHdbDyeCZOJ"
      },
      "source": [
        "class Dataset_handler:\n",
        "    def __init__(self, dataset_info: Dataset_info):\n",
        "        self.dataset = datasets.DatasetDict()\n",
        "        self.tokenized_dataset = None\n",
        "        self.dataset_info = dataset_info\n",
        "        self.labels_list = None\n",
        "        # CACHE\n",
        "        self.global_cache_counter = 0\n",
        "        self.cache_last_hashable_input = \"\"\n",
        "\n",
        "        if dataset_info.dataset_name == \"dpr\":\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/dpr_data/train.json', data_type=\"train\")\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/dpr_data/dev.json', data_type=\"dev\")\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/dpr_data/test.json', data_type=\"test\")\n",
        "        elif dataset_info.dataset_name == \"const\":\n",
        "            frac = 0.01\n",
        "            self.json_to_dataset('./ontonotes_data/const/train.json', data_type=\"train\", fraction = frac, sample_from_head=True)\n",
        "            self.json_to_dataset('./ontonotes_data/const/conll-2012-test.json', data_type=\"dev\", fraction = frac, sample_from_head=True)\n",
        "            self.json_to_dataset('./ontonotes_data/const/test.json', data_type=\"test\", fraction = frac, sample_from_head=True)\n",
        "        elif dataset_info.dataset_name == \"ud\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/ud_data/en_ewt-ud-train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/ud_data/en_ewt-ud-dev.json', data_type=\"dev\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/ud_data/en_ewt-ud-test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"semeval\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/semeval_data/train.0.85.json', data_type=\"train\", fraction = frac, ignore_classes = self.dataset_info.ignore_classes)\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/semeval_data/test.json', data_type=\"dev\", fraction = 0.01, ignore_classes = self.dataset_info.ignore_classes)\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/semeval_data/test.json', data_type=\"test\", fraction = frac, ignore_classes = self.dataset_info.ignore_classes)\n",
        "        elif dataset_info.dataset_name == \"srl\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./ontonotes_data/srl/train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/srl/conll-2012-test.json', data_type=\"dev\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/srl/test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"ner\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./ontonotes_data/ner/train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/ner/conll-2012-test.json', data_type=\"dev\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/ner/test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"coref\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./ontonotes_data/coref/train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/coref/development.json', data_type=\"dev\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/coref/test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"offenseval2019\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/offenseval2019/train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/offenseval2019/dev.json', data_type=\"dev\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/offenseval2019/test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"hatexplain\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/hatexplain/train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/hatexplain/dev.json', data_type=\"dev\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/hatexplain/test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"hatexplain-fullspan\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/hatexplain/train.json', data_type=\"train\", fraction = frac, to_sentence_span=True)\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/hatexplain/dev.json', data_type=\"dev\", fraction = frac, to_sentence_span=True)\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/hatexplain/test.json', data_type=\"test\", fraction = frac, to_sentence_span=True)\n",
        "        elif dataset_info.dataset_name == \"jigsaw_bias\":\n",
        "            train_frac = 0.1\n",
        "            test_frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/jigsaw_bias/train100.json', data_type=\"train\", fraction = train_frac, keep_order=False)\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/jigsaw_bias/test100.json', data_type=\"dev\", fraction = 0.001)\n",
        "            self.json_to_dataset('./edge-probing-datasets/toxicity/jigsaw_bias/test100.json', data_type=\"test\", fraction = test_frac)\n",
        "        elif dataset_info.dataset_name == \"vua_verb\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/vua/verb_train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/vua/verb_test.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/vua/verb_test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"vua_pos\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/vua/pos_train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/vua/pos_test.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/vua/pos_test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"trofi\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/trofi/train.json', data_type=\"train\", fraction = frac, keep_order=False)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/trofi/test.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/trofi/test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"trofi_nospan\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/trofi/train.json', data_type=\"train\", fraction = frac, keep_order=False, to_sentence_span=True)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/trofi/test.json', data_type=\"dev\", fraction = 0.01, to_sentence_span=True)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/trofi/test.json', data_type=\"test\", fraction = frac, to_sentence_span=True)\n",
        "        elif dataset_info.dataset_name == \"lcc\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_train10_current.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_test10_current.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_src_concept\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_train10_src_concept_current.json', data_type=\"train\", fraction = frac, ignore_classes = self.dataset_info.ignore_classes)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_test10_src_concept_current.json', data_type=\"dev\", fraction = 0.01, ignore_classes = self.dataset_info.ignore_classes)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_test10_src_concept_current.json', data_type=\"test\", fraction = frac, ignore_classes = self.dataset_info.ignore_classes)\n",
        "        elif dataset_info.dataset_name == \"lcc_src_target_concept\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_train10_src_concept_current.json', data_type=\"train\", fraction = frac, ignore_classes = self.dataset_info.ignore_classes)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_test10_src_concept_current.json', data_type=\"dev\", fraction = 0.01, ignore_classes = self.dataset_info.ignore_classes)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_test10_src_concept_current.json', data_type=\"test\", fraction = frac, ignore_classes = self.dataset_info.ignore_classes)\n",
        "        elif dataset_info.dataset_name == \"lcc_fa\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/fa/fa_train10_current.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/fa/fa_test10_current.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/fa/fa_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_es\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/es/es_train10_current.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/es/es_test10_current.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/es/es_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_ru\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/ru/ru_train10_current.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/ru/ru_test10_current.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/ru/ru_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_en_fa\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_train10_current.json', data_type=\"train\", fraction = 0.4650858)\n",
        "            # self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_train10_current.json', data_type=\"train\", fraction = 1)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_test10_current.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/fa/fa_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_en_es\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_train10_current.json', data_type=\"train\", fraction = 0.72926148)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_test10_current.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/es/es_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_en_ru\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_train10_current.json', data_type=\"train\", fraction = 0.439840076)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_test10_current.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/ru/ru_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_es_fa\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/es/es_train10_current.json', data_type=\"train\", fraction = 0.63774912)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/es/es_test10_current.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/fa/fa_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_es_ru\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/es/es_train10_current.json', data_type=\"train\", fraction = 0.60313081856)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/es/es_test10_current.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/ru/ru_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_fa_ru\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/fa/fa_train10_current.json', data_type=\"train\", fraction = 0.9457179930)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/fa/fa_test10_current.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/ru/ru_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_fa_en\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/fa/fa_train10_current.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/fa/fa_test10_current.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/en/en_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_en+fa_fa\":\n",
        "            frac = 1\n",
        "            self.merge_files(['./edge-probing-datasets/metaphor/lcc/fa/fa_train10_current.json', \n",
        "                              './edge-probing-datasets/metaphor/lcc/en/en_train10_current.json'],\n",
        "                             \"merged.json\")\n",
        "            self.json_to_dataset('merged.json', data_type=\"train\", keep_order=False, fraction = frac)\n",
        "            self.json_to_dataset('merged.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/fa/fa_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_en+es_es\":\n",
        "            frac = 1\n",
        "            self.merge_files(['./edge-probing-datasets/metaphor/lcc/es/es_train10_current.json', \n",
        "                              './edge-probing-datasets/metaphor/lcc/en/en_train10_current.json'],\n",
        "                             \"merged.json\")\n",
        "            self.json_to_dataset('merged.json', data_type=\"train\", keep_order=False, fraction = frac)\n",
        "            self.json_to_dataset('merged.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/es/es_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"lcc_en+ru_ru\":\n",
        "            frac = 1\n",
        "            self.merge_files(['./edge-probing-datasets/metaphor/lcc/ru/ru_train10_current.json', \n",
        "                              './edge-probing-datasets/metaphor/lcc/en/en_train10_current.json'],\n",
        "                             \"merged.json\")\n",
        "            self.json_to_dataset('merged.json', data_type=\"train\", keep_order=False, fraction = frac)\n",
        "            self.json_to_dataset('merged.json', data_type=\"dev\", fraction = 0.01)\n",
        "            self.json_to_dataset('./edge-probing-datasets/metaphor/lcc/ru/ru_test10_current.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"manual\":\n",
        "            frac = 1\n",
        "            f = open(\"./manual_dataset.json\", \"w\")\n",
        "            f.write('{\"text\": \"' + dataset_info.manual_text + '\", \"targets\": [{\"span1\": [0, 0], \"label\": \"' + my_dataset_handler.labels_list[0] + '\"}]}')\n",
        "            f.write('\\n{\"text\": \"' + dataset_info.manual_text + '\", \"targets\": [{\"span1\": [0, 0], \"label\": \"' + my_dataset_handler.labels_list[1] + '\"}]}')\n",
        "            f.close()\n",
        "            self.json_to_dataset('./manual_dataset.json', data_type=\"train\", fraction = frac, to_sentence_span=True)\n",
        "            self.json_to_dataset('./manual_dataset.json', data_type=\"dev\", fraction = frac, to_sentence_span=True)\n",
        "            self.json_to_dataset('./manual_dataset.json', data_type=\"test\", fraction = frac, to_sentence_span=True)\n",
        "        else:\n",
        "            throw(\"Error: Unkown dataset name!\")\n",
        "\n",
        "        print(\"⌛ Tokenizing Dataset and Adding One Hot Representation of Labels\")\n",
        "        self.tokenized_dataset = self.tokenize_input_and_one_hot_labels(self.dataset)\n",
        "        # self.tokenized_dataset = self.tokenize_dataset(self.dataset)\n",
        "        # print(\"⌛ Adding One Hot Representation of Labels\")\n",
        "        # self.tokenized_dataset = self.one_hot_dataset_labels(self.tokenized_dataset)\n",
        "        \n",
        "\n",
        "    # Private:\n",
        "    def merge_files(self, file_addresses: [], output_address: str):\n",
        "        data = \"\"\n",
        "        for file_address in file_addresses:\n",
        "            with open(file_address) as fp:\n",
        "                data += fp.read()\n",
        "        with open (output_address, 'w') as fp:\n",
        "            fp.write(data)\n",
        "\n",
        "    def json_to_dataset(self, json_path, data_type=\"train\", fraction=1, ignore_classes=[], keep_order=False, sample_from_head=False, to_sentence_span=False):\n",
        "        data_df = self.json_to_df(json_path, to_sentence_span)\n",
        "        data_df = data_df[~data_df[\"label\"].isin(ignore_classes)]\n",
        "        if sample_from_head:\n",
        "            data_df = data_df.head(int(len(data_df) * fraction))\n",
        "        else:\n",
        "            if keep_order:\n",
        "                data_df = data_df.sample(frac=fraction, random_state=SEED).sort_index().reset_index(drop=True)\n",
        "            else:\n",
        "                data_df = data_df.sample(frac=fraction, random_state=SEED).reset_index(drop=True)\n",
        "        self.dataset[data_type] = datasets.Dataset.from_pandas(data_df)\n",
        "        return self.dataset\n",
        "    \n",
        "    def tokenize_input_and_one_hot_labels(self, dataset):\n",
        "        train_df = pd.DataFrame(dataset[\"train\"][\"label\"], columns=['label'])\n",
        "        dev_df = pd.DataFrame(dataset[\"dev\"][\"label\"], columns=['label'])\n",
        "        test_df = pd.DataFrame(dataset[\"test\"][\"label\"], columns=['label'])\n",
        "        self.labels_list = list(set(train_df[\"label\"].unique()).union\n",
        "                               (set(dev_df[\"label\"].unique())).union\n",
        "                               (set(test_df[\"label\"].unique())))\n",
        "        self.label_to_index = dict()\n",
        "        for idx, l in enumerate(self.labels_list):\n",
        "            self.label_to_index[l] = idx\n",
        "        \n",
        "        if \"glove\" in model_checkpoint or \"elmo\" in model_checkpoint:\n",
        "            tokenized_one_hot_dataset = dataset.map(tokenize_and_one_hot_glove,\n",
        "                                                    fn_kwargs={\"label_to_index\": self.label_to_index,\n",
        "                                                            \"labels_len\": len(self.label_to_index),\n",
        "                                                            \"one_hot_func\": Utils.one_hot,\n",
        "                                                            \"num_of_spans\": self.dataset_info.num_of_spans\n",
        "                                                            },\n",
        "                                                    batched=False,\n",
        "                                                    num_proc=None)\n",
        "        else:\n",
        "            tokenized_one_hot_dataset = dataset.map(tokenize_and_one_hot,\n",
        "                                                    fn_kwargs={\"label_to_index\": self.label_to_index,\n",
        "                                                            \"labels_len\": len(self.label_to_index),\n",
        "                                                            \"tokenizer\": tokenizer,\n",
        "                                                            \"one_hot_func\": Utils.one_hot,\n",
        "                                                            \"num_of_spans\": self.dataset_info.num_of_spans\n",
        "                                                            },\n",
        "                                                    batched=False,\n",
        "                                                    num_proc=None)\n",
        "        return tokenized_one_hot_dataset\n",
        "\n",
        "    # Preprocesses\n",
        "    def lcc_preprocess(self, target, instance):\n",
        "        if \"lcc\" in self.dataset_info.dataset_name and \"src\" not in self.dataset_info.dataset_name:\n",
        "            target[\"label\"] = float(target[\"label\"])\n",
        "            if 0.0 <= target[\"label\"] < 0.5:\n",
        "                target[\"label\"] = \"Non-metaphor\"\n",
        "            elif 1.5 < target[\"label\"] <= 3.0:\n",
        "                target[\"label\"] = \"Metaphor\"\n",
        "            else:\n",
        "                return None, None\n",
        "        return target, instance\n",
        "    \n",
        "    def lcc_src_concept_preprocess(self, target, instance):\n",
        "        if self.dataset_info.dataset_name == \"lcc_src_concept\":\n",
        "            target[\"span1\"] = target[\"span2\"]\n",
        "            score = float(target[\"score\"])\n",
        "            if score >= 2:\n",
        "                return target, instance\n",
        "            else:\n",
        "                return None, None\n",
        "        return target, instance\n",
        "\n",
        "    def hatexplain_preprocess(self, target, instance):\n",
        "        # if self.dataset_info.dataset_name == \"hatexplain\":\n",
        "            # span_len = target[\"span1\"][1] - target[\"span1\"][0]\n",
        "            # if target[\"label\"] == \"Normal\":\n",
        "            #     hatexplain_distribution_normal[span_len] += 1\n",
        "            # else:\n",
        "            #     hatexplain_distribution_toxic[span_len] += 1\n",
        "            \n",
        "            # instance[\"text\"] = re.sub(r'[^A-Za-z0-9 ]+', '', instance[\"text\"])  # Alphanumeric + Space\n",
        "            # if True or target[\"label\"] == \"Normal\":\n",
        "            #     e = len(instance[\"text\"].split())\n",
        "            #     target[\"span1\"] = [0, e]\n",
        "        return target, instance\n",
        "\n",
        "    def hatexplain_fullspan_preprocess(self, target, instance):\n",
        "        if self.dataset_info.dataset_name == \"hatexplain-fullspan\":\n",
        "            instance[\"text\"] = re.sub(r'[^A-Za-z0-9 ]+', '', instance[\"text\"])  # Alphanumeric + Space\n",
        "        return target, instance\n",
        "    \n",
        "    def lcc_src_target_concept_preprocess(self, target, instance):\n",
        "        if self.dataset_info.dataset_name == \"lcc_src_target_concept\":\n",
        "            target[\"label\"] = \"(\" + target[\"label\"] + \",\" + instance[\"targetConcept\"] + \")\"\n",
        "        return target, instance\n",
        "\n",
        "    def json_to_df(self, json_path, to_sentence_span=False):\n",
        "        pre_processes = [self.lcc_preprocess, self.lcc_src_concept_preprocess, \n",
        "                         self.hatexplain_fullspan_preprocess, \n",
        "                         self.lcc_src_target_concept_preprocess,\n",
        "                         self.hatexplain_preprocess]\n",
        "        with open(json_path, encoding='utf-8') as file:\n",
        "            c = 0\n",
        "            data_list = list()\n",
        "            for line in file:\n",
        "                # print(c, end=\",\")\n",
        "                c += 1\n",
        "                instance = json.loads(line)\n",
        "\n",
        "                if self.cache_last_hashable_input != repr(instance[\"text\"]):\n",
        "                    self.cache_last_hashable_input = repr(instance[\"text\"])\n",
        "                    self.global_cache_counter += 1\n",
        "\n",
        "                for target in instance[\"targets\"]:\n",
        "                    for pre_process in pre_processes:\n",
        "                        target, instance = pre_process(target, instance)\n",
        "                    if target == None:\n",
        "                        break\n",
        "                    if self.dataset_info.num_of_spans == 2:\n",
        "                        data_list.append({\"text\": instance[\"text\"],\n",
        "                                        \"span1\": target[\"span1\"],\n",
        "                                        \"span2\": target[\"span2\"],\n",
        "                                        \"label\": str(target[\"label\"]),\n",
        "                                        \"cache_id\": self.global_cache_counter})\n",
        "                    elif self.dataset_info.num_of_spans == 1:\n",
        "                        if to_sentence_span:\n",
        "                            target[\"span1\"][0] = 0\n",
        "                            target[\"span1\"][-1] = len(instance[\"text\"].split())\n",
        "                        data_list.append({\"text\": instance[\"text\"],\n",
        "                                        \"span1\": target[\"span1\"],\n",
        "                                        \"label\": str(target[\"label\"]),\n",
        "                                        \"cache_id\": self.global_cache_counter})\n",
        "        return pd.DataFrame.from_dict(data_list)\n",
        "\n",
        "def tokenize_and_one_hot_glove(examples, **fn_kwargs):\n",
        "    # tokenize and align spans\n",
        "    one_hot_func = fn_kwargs[\"one_hot_func\"]\n",
        "    num_of_spans = fn_kwargs[\"num_of_spans\"]\n",
        "    tokenized_inputs = {\"text\": examples[\"text\"].lower().split()}\n",
        "\n",
        "    tokenized_inputs[\"span1\"] = [examples[\"span1\"][0], examples[\"span1\"][1]]\n",
        "    tokenized_inputs[\"span1_len\"] = tokenized_inputs[\"span1\"][1] - tokenized_inputs[\"span1\"][0]\n",
        "    if num_of_spans == 2:\n",
        "        tokenized_inputs[\"span2\"] = [examples[\"span2\"][0], examples[\"span2\"][1]]\n",
        "        tokenized_inputs[\"span2_len\"] = tokenized_inputs[\"span2\"][1] - tokenized_inputs[\"span2\"][0]\n",
        "    # One hot\n",
        "    label_to_index = fn_kwargs[\"label_to_index\"]\n",
        "    labels_len = fn_kwargs[\"labels_len\"]\n",
        "    tokenized_inputs[\"one_hot_label\"] = one_hot_func(label_to_index[examples[\"label\"]], labels_len)\n",
        "    return tokenized_inputs\n",
        "\n",
        "cached_tokenized_input = {}\n",
        "cached_onehot = {}\n",
        "def tokenize_and_one_hot(examples, **fn_kwargs):\n",
        "    # tokenize and align spans\n",
        "    thread_tokenizer = fn_kwargs[\"tokenizer\"]\n",
        "    one_hot_func = fn_kwargs[\"one_hot_func\"]\n",
        "    num_of_spans = fn_kwargs[\"num_of_spans\"]\n",
        "    \n",
        "    if repr(examples[\"text\"]) in cached_tokenized_input:\n",
        "        tokenized_inputs = cached_tokenized_input[repr(examples[\"text\"])]\n",
        "    else:\n",
        "        tokenized_inputs = thread_tokenizer(examples[\"text\"].split(), is_split_into_words=True)  # Must be splitted for tokenizer to word_ids works fine. (test e-mail!)\n",
        "        # cached_tokenized_input = {}  # Free For RAM (Just Last One Cached)\n",
        "        cached_tokenized_input[repr(examples[\"text\"])] = tokenized_inputs\n",
        "    # tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, is_split_into_words=True, padding=\"max_length\", max_length=210)\n",
        "    def align_span(word_ids, start_word_id, end_word_id):\n",
        "        span = [0, 0]\n",
        "        if start_word_id not in word_ids:\n",
        "            print(\"Warning: There is no\", start_word_id, \"in\", word_ids, examples[\"text\"].split(), examples[\"label\"])\n",
        "            start_word_id -= 1\n",
        "        span[0] = word_ids.index(start_word_id)  # First occurance\n",
        "        if end_word_id - 1 not in word_ids[::-1]:\n",
        "            print(\"Warning: There is no\", end_word_id - 1, \"in\", word_ids, examples[\"text\"].split(), examples[\"label\"])\n",
        "            end_word_id -= 1\n",
        "        span[1] = len(word_ids) - 1 - word_ids[::-1].index(end_word_id - 1) + 1  # Last occurance (+1 for open range)\n",
        "        return span\n",
        "\n",
        "    # tokenized_inputs[\"span1\"] = [0, 0]\n",
        "    # tokenized_inputs[\"span1\"][0] = word_ids.index(examples[\"span1\"][0])  # First occurance\n",
        "    # tokenized_inputs[\"span1\"][1] = len(word_ids) - 1 - word_ids[::-1].index(examples[\"span1\"][1] - 1) + 1  # Last occurance (+1 for open range)\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "    tokenized_inputs[\"span1\"] = align_span(word_ids, examples[\"span1\"][0], examples[\"span1\"][1])\n",
        "    tokenized_inputs[\"span1_len\"] = tokenized_inputs[\"span1\"][1] - tokenized_inputs[\"span1\"][0]\n",
        "    if num_of_spans == 2:\n",
        "        # tokenized_inputs[\"span2\"] = [0, 0]\n",
        "        # tokenized_inputs[\"span2\"][0] = word_ids.index(examples[\"span2\"][0])  # First occurance\n",
        "        # tokenized_inputs[\"span2\"][1] = len(word_ids) - 1 - word_ids[::-1].index(examples[\"span2\"][1] - 1) + 1  # Last occurance\n",
        "        tokenized_inputs[\"span2\"] = align_span(word_ids, examples[\"span2\"][0], examples[\"span2\"][1])\n",
        "        tokenized_inputs[\"span2_len\"] = tokenized_inputs[\"span2\"][1] - tokenized_inputs[\"span2\"][0]\n",
        "    # One hot\n",
        "    label_to_index = fn_kwargs[\"label_to_index\"]\n",
        "    labels_len = fn_kwargs[\"labels_len\"]\n",
        "    if examples[\"label\"] in cached_onehot:\n",
        "        tokenized_inputs[\"one_hot_label\"] = cached_onehot[examples[\"label\"]]\n",
        "    else:\n",
        "        tokenized_inputs[\"one_hot_label\"] = one_hot_func(label_to_index[examples[\"label\"]], labels_len)\n",
        "        cached_onehot[examples[\"label\"]] = tokenized_inputs[\"one_hot_label\"]\n",
        "    return tokenized_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEYV7wo1LiaL"
      },
      "source": [
        "my_dataset_handler = Dataset_handler(my_dataset_info);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmmRDDuFjd2"
      },
      "source": [
        "# Check\n",
        "rnd_idx = np.random.randint(1000)\n",
        "# rnd_idx = 58000\n",
        "part = \"train\"\n",
        "display(pd.DataFrame(my_dataset_handler.tokenized_dataset['train'][0:3]))\n",
        "display(pd.DataFrame(my_dataset_handler.tokenized_dataset['test'][0:3]))\n",
        "print(\"idx =\", rnd_idx)\n",
        "print(my_dataset_handler.tokenized_dataset)\n",
        "print(\"Original Spans:\", my_dataset_handler.dataset[part][rnd_idx])\n",
        "print(\"Tokenized Spans:\", my_dataset_handler.tokenized_dataset[part][rnd_idx])\n",
        "if \"glove\" in model_checkpoint or \"elmo\" in model_checkpoint:\n",
        "    test_tokens = my_dataset_handler.tokenized_dataset[part][rnd_idx][\"text\"]\n",
        "else:\n",
        "    test_tokens = tokenizer.convert_ids_to_tokens(my_dataset_handler.tokenized_dataset[part][rnd_idx][\"input_ids\"])\n",
        "print(test_tokens)\n",
        "\n",
        "s10, s11 = my_dataset_handler.tokenized_dataset[part][rnd_idx][\"span1\"][0], my_dataset_handler.tokenized_dataset[part][rnd_idx][\"span1\"][-1]\n",
        "print(\"span1:\", s10, s11, test_tokens[s10:s11])\n",
        "if my_dataset_info.num_of_spans == 2:\n",
        "    s20, s21 = my_dataset_handler.tokenized_dataset[part][rnd_idx][\"span2\"][0], my_dataset_handler.tokenized_dataset[part][rnd_idx][\"span2\"][-1]\n",
        "    print(\"span2:\", s20, s21, test_tokens[s20:s21])\n",
        "print(\"label:\", my_dataset_handler.tokenized_dataset[part][rnd_idx][\"label\"])\n",
        "\n",
        "stats = pd.DataFrame(my_dataset_handler.tokenized_dataset[part][\"label\"], columns=['label'])[\"label\"].value_counts()\n",
        "print(stats.to_string())\n",
        "print(list(stats.index))\n",
        "print(\"|Labels| =\", len(stats))\n",
        "stats.plot(kind='barh', color=\"green\", figsize=(10, 9));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB5knhdSN9kB"
      },
      "source": [
        "round((list(stats)[1] / sum(stats))*100,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5t_gYLZLT9S"
      },
      "source": [
        "# Edge Probe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okFPTNCIkUru"
      },
      "source": [
        "class SpanRepr(ABC, nn.Module):\n",
        "    \"\"\"Abstract class describing span representation.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, use_proj=False, proj_dim=256):\n",
        "        super(SpanRepr, self).__init__()\n",
        "        self.input_dim = input_dim  # embedding dim or proj dim\n",
        "        self.proj_dim = proj_dim\n",
        "        self.use_proj = use_proj\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, spans, attention_mask):\n",
        "        \"\"\" \n",
        "        input:\n",
        "            spans: [batch_size, layers, span_max_len, proj_dim/embedding_dim] ~ [32, 13, 4, 256]\n",
        "            attention_mask: [batch_size, span_max_len] ~ [32, 4]\n",
        "        returns:\n",
        "            [32, 13, 256]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_input_dim(self):\n",
        "        return self.input_dim\n",
        "\n",
        "class MaxSpanRepr(SpanRepr, nn.Module):\n",
        "    \"\"\"Class implementing the max-pool span representation.\"\"\"\n",
        "\n",
        "    def forward(self, spans, attention_mask):\n",
        "        # TODO: Vectorize this\n",
        "        # for i in range(len(attention_mask)):\n",
        "        #     for j in range(len(attention_mask[i])):\n",
        "        #         if attention_mask[i][j] == 0:\n",
        "        #             spans[i, :, j, :] = -1e10\n",
        "\n",
        "        span_masks_shape = attention_mask.shape\n",
        "        span_masks = attention_mask.reshape(\n",
        "            span_masks_shape[0],\n",
        "            1,\n",
        "            span_masks_shape[1],\n",
        "            1\n",
        "        ).expand_as(spans)\n",
        "        attention_spans = spans * span_masks - 1e10 * (1 - span_masks)\n",
        "\n",
        "        max_span_repr, max_idxs = torch.max(attention_spans, dim=-2)\n",
        "        # print(max_span_repr.shape)\n",
        "        return max_span_repr\n",
        "\n",
        "class AttnSpanRepr(SpanRepr, nn.Module):\n",
        "    \"\"\"Class implementing the attention-based span representation.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, use_proj=False, proj_dim=256, use_endpoints=False):\n",
        "        \"\"\"If use_endpoints is true then concatenate the end points to attention-pooled span repr.\n",
        "        Otherwise just return the attention pooled term. (use_endpoints Not Implemented)\n",
        "        \"\"\"\n",
        "        super(AttnSpanRepr, self).__init__(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "        self.use_endpoints = use_endpoints\n",
        "        # input_dim is embedding_dim or proj dim\n",
        "        # print(\"input_dim\", input_dim)\n",
        "        self.attention_params = nn.Linear(input_dim, 1)  # Learn a weight for each token: z(k)i = W(k)att e(k)i\n",
        "        self.last_attention_wts = None\n",
        "        # Initialize weight to zero weight\n",
        "        # self.attention_params.weight.data.fill_(0)\n",
        "        # self.attention_params.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, spans, attention_mask):\n",
        "        \"\"\" \n",
        "        input:\n",
        "            spans: [batch_size, layers, span_max_len, proj_dim/embedding_dim] ~ [32, 13, 4, 256]\n",
        "            attention_mask: [batch_size, span_max_len] ~ [32, 4]\n",
        "        returns:\n",
        "            [32, 13, 256]\n",
        "        \"\"\"\n",
        "        # if self.use_proj:\n",
        "        #     encoded_input = self.proj(encoded_input)\n",
        "\n",
        "        # span_mask = get_span_mask(start_ids, end_ids, encoded_input.shape[1])\n",
        "        # attn_mask = torch.zeros(spans.shape, device=DEVICE)\n",
        "        # print(datetime.datetime.now().time(), \"a1\")\n",
        "        # print(attention_mask.shape)\n",
        "        # for i in range(len(attention_mask)):\n",
        "        #     for j in range(len(attention_mask[i])):\n",
        "        #         if attention_mask[i][j] == 0:\n",
        "        #             attn_mask[i, :, j, :] = -1e10\n",
        "\n",
        "        span_masks_shape = attention_mask.shape\n",
        "        span_masks = attention_mask.reshape(\n",
        "            span_masks_shape[0],\n",
        "            1,\n",
        "            span_masks_shape[1],\n",
        "            1\n",
        "        ).expand_as(spans)\n",
        "        attn_mask = - 1e10 * (1 - span_masks)\n",
        "        \n",
        "        # print(datetime.datetime.now().time(), \"a2\")\n",
        "\n",
        "        # attn_mask = (1 - span_mask) * (-1e10)\n",
        "        attn_logits = self.attention_params(spans) + attn_mask  # Decreasing the attention of padded spans by -1e10\n",
        "        attention_wts = nn.functional.softmax(attn_logits, dim=-2)\n",
        "        attention_term = torch.sum(attention_wts * spans, dim=-2)\n",
        "\n",
        "        self.last_attention_wts = attention_wts   # Save for later analysis\n",
        "        \n",
        "        # if self.use_endpoints:\n",
        "        #     batch_size = encoded_input.shape[0]\n",
        "        #     h_start = encoded_input[torch.arange(batch_size), start_ids, :]\n",
        "        #     h_end = encoded_input[torch.arange(batch_size), end_ids, :]\n",
        "        #     return torch.cat([h_start, h_end, attention_term], dim=1)\n",
        "        # else:\n",
        "        #     return attention_term\n",
        "\n",
        "        # print(spans.shape, attn_mask.shape)\n",
        "        # print(\"attn_mask\", attn_mask.shape)\n",
        "        # print(attn_mask[sidx, :, :, 0:2])\n",
        "        # print(\"attn_logits\", attn_logits.shape)\n",
        "        # print(attn_logits[sidx])\n",
        "        # print(\"attention_wts\", attention_wts.shape)\n",
        "        # print(attention_wts[sidx, :, :, 0:2])\n",
        "        # print(\"attention_term\", attention_term.shape)\n",
        "        # print(attention_term[sidx, :, 0:2])\n",
        "        return attention_term.float()\n",
        "\n",
        "def get_span_module(input_dim, method=\"max\", use_proj=False, proj_dim=256):\n",
        "    \"\"\"Initializes the appropriate span representation class and returns the object.\n",
        "    \"\"\"\n",
        "    if method == \"avg\":\n",
        "        return AvgSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"max\":\n",
        "        return MaxSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"diff\":\n",
        "        return DiffSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"diff_sum\":\n",
        "        return DiffSumSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"endpoint\":\n",
        "        return EndPointRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"coherent\":\n",
        "        return CoherentSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"coherent_original\":\n",
        "        return CoherentOrigSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"attn\":\n",
        "        return AttnSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"coref\":\n",
        "        return AttnSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim, use_endpoints=True)\n",
        "    else:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYq1nlkAOKzw"
      },
      "source": [
        "class Edge_probe_model(nn.Module):\n",
        "    def __init__(self, num_of_spans, num_layers, input_span_len, embedding_dim, \n",
        "                 num_classes, pool_method='max', use_proj=True, proj_dim=256, \n",
        "                 hidden_dim=256, device='cuda', normalize_layers=False, use_cross_entropy=False):\n",
        "        super(Edge_probe_model, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        self.num_of_spans = num_of_spans\n",
        "        self.weighing_params = nn.Parameter(torch.ones(self.num_layers))\n",
        "        self.input_dim = embedding_dim * num_of_spans\n",
        "        self.use_proj = use_proj\n",
        "        self.proj_dim = proj_dim\n",
        "        self.normalize_layers = normalize_layers\n",
        "\n",
        "        ## Projection\n",
        "        if use_proj:\n",
        "            # Apply a projection layer to output of pretrained models\n",
        "            # print(embedding_dim, num_layers, proj_dim)\n",
        "            self.proj1 = nn.Linear(embedding_dim, proj_dim)\n",
        "            if self.num_of_spans == 2:\n",
        "                self.proj2 = nn.Linear(embedding_dim, proj_dim)\n",
        "            # Update the input_dim\n",
        "            self.input_dim = proj_dim * num_of_spans\n",
        "\n",
        "        ## Pooling\n",
        "        self.pool_method = pool_method\n",
        "        input_dim = proj_dim if use_proj else embedding_dim\n",
        "        self.span1_pooling_net = get_span_module(input_dim, method=pool_method).to(device)\n",
        "        if self.num_of_spans == 2:\n",
        "            self.span2_pooling_net = get_span_module(input_dim, method=pool_method).to(device)\n",
        "\n",
        "        ## Classification\n",
        "        label_net_list = [\n",
        "            nn.Linear(self.input_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, self.num_classes)        \n",
        "        ]\n",
        "        if use_cross_entropy:\n",
        "            self.training_criterion = nn.CrossEntropyLoss()\n",
        "        else:\n",
        "            self.training_criterion = nn.BCELoss()\n",
        "            label_net_list.append(nn.Sigmoid())\n",
        "\n",
        "        self.label_net = nn.Sequential(*label_net_list)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=0)\n",
        "\n",
        "    def forward(self, spans_torch_dict):\n",
        "        span1_reprs = spans_torch_dict[\"span1\"]\n",
        "        span1_attention_mask = spans_torch_dict[\"span1_attention_mask\"]\n",
        "        if self.num_of_spans == 2:\n",
        "            span2_reprs = spans_torch_dict[\"span2\"]\n",
        "            span2_attention_mask = spans_torch_dict[\"span2_attention_mask\"]\n",
        "        # print(span1_reprs.shape)\n",
        "        \n",
        "        ## Projection\n",
        "        if self.use_proj:\n",
        "            span1_reprs = self.proj1(span1_reprs)\n",
        "            if self.num_of_spans == 2:\n",
        "                span2_reprs = self.proj2(span2_reprs)\n",
        "        \n",
        "        ## Pooling\n",
        "        pooled_span1 = self.span1_pooling_net(span1_reprs, span1_attention_mask)\n",
        "        if self.num_of_spans == 2:\n",
        "            pooled_span2 = self.span2_pooling_net(span2_reprs, span2_attention_mask)\n",
        "\n",
        "        # print(my_dataset_handler.tokenized_dataset[\"train\"][0])\n",
        "        # print(\"SPAN1\", span1_reprs[2, :, :, 0:5])\n",
        "        # print(\"SPAN2\", span2_reprs[2, :, :, 0:5])\n",
        "        # print(\"MAX1\", pooled_span1[2, :, 0:5])\n",
        "        # print(\"MAX2\", pooled_span2[2, :, 0:5])\n",
        "        # raise \"E\"\n",
        "        if self.normalize_layers:\n",
        "            pooled_span1 = torch.nn.functional.normalize(pooled_span1, dim=-1)\n",
        "            if self.num_of_spans == 2:\n",
        "                pooled_span2 = torch.nn.functional.normalize(pooled_span2, dim=-1)\n",
        "\n",
        "        if self.num_of_spans == 2:\n",
        "            output = torch.cat((pooled_span1, pooled_span2), dim=-1)\n",
        "        elif self.num_of_spans == 1:\n",
        "            output = pooled_span1\n",
        "        # print(output.shape)  # torch.Size([32, 13, 512])\n",
        "\n",
        "        ## Mixing Weights\n",
        "        wtd_encoded_repr = 0\n",
        "        soft_weight = nn.functional.softmax(self.weighing_params, dim=0)\n",
        "        for i in range(self.num_layers):\n",
        "            # print(i, output[:, i, :].shape, torch.norm(output[:, i, :]), torch.norm(s1))\n",
        "            # print(output[:, i, :][0, 0:10])\n",
        "            # print(s1[0, 0:10])\n",
        "            wtd_encoded_repr += soft_weight[i] * output[:, i, :]\n",
        "        # wtd_encoded_repr += soft_weight[-1] * encoded_layers[:, -1, :]\n",
        "        output = wtd_encoded_repr\n",
        "\n",
        "        ## Classification\n",
        "        pred_label = self.label_net(output)\n",
        "        pred_label = torch.squeeze(pred_label, dim=-1)\n",
        "        return pred_label\n",
        "\n",
        "    def summary(self, do_print=True):\n",
        "        summary_str = str(self)\n",
        "        pytorch_total_params = sum(p.numel() for p in self.parameters())\n",
        "        pytorch_total_params_trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        summary_str += f\"\\n Total Parameters:     {pytorch_total_params}\"\n",
        "        summary_str += f\"\\n Trainable Parameters: {pytorch_total_params_trainable}\"\n",
        "        summary_str += f\"\\n Pool Method: {self.pool_method}\"\n",
        "        summary_str += f\"\\n Projection: {self.use_proj}, {self.proj_dim}\"\n",
        "        summary_str += f\"\\n normalize_layers: {self.normalize_layers}\"\n",
        "        if do_print:\n",
        "            print(summary_str)\n",
        "        return summary_str\n",
        "        # print(\"Total Parameters:    \", pytorch_total_params)\n",
        "        # print(\"Trainable Parameters:\", pytorch_total_params_trainable)\n",
        "        # print(\"Pool Method:\", self.pool_method)\n",
        "        # print(\"Projection:\", self.use_proj, self.proj_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj9U93HAx09A"
      },
      "source": [
        "gpu_cache = dict()\n",
        "ram_cache = dict()\n",
        "class Trainer(ABC):\n",
        "    \"\"\" Abstract Trainer Class \"\"\"\n",
        "    def span_dict_to_device(self, spans_torch_dict, device=\"cuda\"):\n",
        "        new_dict = copy.deepcopy(spans_torch_dict)\n",
        "        new_dict[\"span1\"] = new_dict[\"span1\"].to(device)\n",
        "        new_dict[\"span1_attention_mask\"] = new_dict[\"span1_attention_mask\"].to(device)\n",
        "        if self.num_of_spans == 2:\n",
        "            new_dict[\"span2\"] = new_dict[\"span2\"].to(device)\n",
        "            new_dict[\"span2_attention_mask\"] = new_dict[\"span2_attention_mask\"].to(device)\n",
        "        return new_dict\n",
        "\n",
        "    def prepare_batch_data(self, tokenized_dataset, start_idx, end_idx, pad=False, cache_prefix=None):\n",
        "        # self.vprint(\"Extracting From Model\")\n",
        "        if cache_prefix is not None:\n",
        "            cache_id = f\"{cache_prefix}{start_idx}-{end_idx}\"\n",
        "            if cache_id in gpu_cache:\n",
        "                return gpu_cache[cache_id]\n",
        "            if cache_id in ram_cache:\n",
        "                return self.span_dict_to_device(ram_cache[cache_id], \"cuda\")\n",
        "\n",
        "        span_representations_dict = self.extract_embeddings(tokenized_dataset, start_idx, end_idx, pad=True)\n",
        "        # self.vprint(\"To Device\")\n",
        "        span1_torch = torch.stack(span_representations_dict[\"span1\"]).float().to(self.MLP_device)  # (batch_size, #layers, max_span_len, embd_dim)\n",
        "        span1_attention_mask_torch = torch.stack(span_representations_dict[\"span1_attention_mask\"])\n",
        "        one_hot_labels_torch = torch.tensor(np.array(span_representations_dict[\"one_hot_label\"]))\n",
        "        if self.num_of_spans == 2:\n",
        "            span2_torch = torch.stack(span_representations_dict[\"span2\"]).float().to(self.MLP_device)\n",
        "            span2_attention_mask_torch = torch.stack(span_representations_dict[\"span1_attention_mask\"])\n",
        "            spans_torch_dict = {\"span1\": span1_torch, \n",
        "                                \"span2\": span2_torch, \n",
        "                                \"span1_attention_mask\": span1_attention_mask_torch, \n",
        "                                \"span2_attention_mask\": span2_attention_mask_torch, \n",
        "                                \"one_hot_labels\": one_hot_labels_torch}\n",
        "        elif self.num_of_spans == 1:\n",
        "            spans_torch_dict = {\"span1\": span1_torch, \n",
        "                                \"span1_attention_mask\": span1_attention_mask_torch, \n",
        "                                \"one_hot_labels\": one_hot_labels_torch}\n",
        "\n",
        "        if cache_prefix is not None:\n",
        "            if len(gpu_cache) < GPU_CACHE_LEN:\n",
        "                gpu_cache[cache_id] = spans_torch_dict\n",
        "                print(cache_id, end=\"|\")\n",
        "            elif len(ram_cache) < RAM_CACHE_LEN:\n",
        "                ram_cache[cache_id] = self.span_dict_to_device(spans_torch_dict, \"cpu\")\n",
        "                print(cache_id, end=\",\")\n",
        "        return spans_torch_dict\n",
        "\n",
        "    def get_language_model_properties(self):\n",
        "        span_representations_dict = self.extract_embeddings(self.dataset_handler.tokenized_dataset[\"train\"], 0, 3, pad=True)\n",
        "        for i in span_representations_dict[\"span1\"]:\n",
        "            print(i.shape)\n",
        "        span1_torch = span_representations_dict[\"span1\"]\n",
        "        num_layers = span1_torch[0].shape[0]\n",
        "        span_len = span1_torch[0].shape[1]\n",
        "        embedding_dim = span1_torch[0].shape[2]\n",
        "        # if self.verbose:\n",
        "        #     display(pd.DataFrame(span_representations_dict))\n",
        "        return num_layers, span_len, embedding_dim, len(self.dataset_handler.labels_list)\n",
        "\n",
        "    def pad_span(self, span_repr, max_len):\n",
        "        \"\"\" pad spans in embeddings to max_len \n",
        "        input:\n",
        "            span_representation: df with shape (#layers, span_len, embedding_dim)\n",
        "        returns:\n",
        "            padded_spans: np with shape (batch_len, num_layers, max_len, embedding_dim)\n",
        "            attention_mask: np with shape (max_len), values = 1: data, 0: padding\n",
        "        \"\"\"\n",
        "        shape = span_repr.shape\n",
        "        num_layers = shape[0]\n",
        "        span_original_len = shape[1]\n",
        "        embedding_dim = shape[2]\n",
        "        # padded_span_repr = np.zeros((num_layers, max_len, embedding_dim))\n",
        "        # if span_original_len > max_len:\n",
        "        #     raise Exception(f\"Error: {span_original_len} is more than max_span_len {max_len}\\n{span_repr.shape}\")\n",
        "\n",
        "        # attention_mask = torch.tensor(np.array([1] * span_original_len + [0] * (max_len - span_original_len)), dtype=torch.int8, device=self.device)\n",
        "        attention_mask = torch.ones(max_len, dtype=torch.int8, device=self.device)\n",
        "        attention_mask[span_original_len:] = 0\n",
        "\n",
        "        padded_span_repr = torch.cat((span_repr, torch.zeros((num_layers, max_len - span_original_len, embedding_dim), device=self.device)), axis=1)\n",
        "        # assert attention_mask.shape == (max_len, ), f\"{attention_mask}, {attention_mask.shape} != ({max_len}, )\"\n",
        "        # assert padded_span_repr.shape == (num_layers, max_len, embedding_dim)\n",
        "        return padded_span_repr, attention_mask\n",
        "\n",
        "    def init_span_dict(self, num_of_spans, pad):\n",
        "        if num_of_spans == 2:\n",
        "            span_repr = {\"span1\": [], \"span2\": [], \"label\": [], \"one_hot_label\": []}\n",
        "        else:\n",
        "            span_repr = {\"span1\": [], \"label\": [], \"one_hot_label\": []}\n",
        "        \n",
        "        if pad:\n",
        "            span_repr[\"span1_attention_mask\"] = []\n",
        "            span_repr[\"span2_attention_mask\"] = []\n",
        "        return span_repr\n",
        "\n",
        "    def extract_glove(self, tokenized_dataset, idx, span_start, span_end):\n",
        "        text = tokenized_dataset[idx][\"text\"]\n",
        "        embedding_dim = word_embedding.word_vectors.shape[-1]\n",
        "        span_len = span_end - span_start\n",
        "        hidden_states = torch.zeros(1, span_len, embedding_dim, device=self.device)  #(layers, span_len, embedding_dim)\n",
        "        # print(text[0:3])\n",
        "        for i in range(span_len):\n",
        "            word = text[span_start + i]\n",
        "            if word in word_embedding.dictionary:\n",
        "                hidden_states[0, i, :] = torch.tensor(word_embedding.word_vectors[word_embedding.dictionary[word]], device=self.device)\n",
        "            else:\n",
        "                pass\n",
        "                # print(\"UNKONW WORD:\", word)\n",
        "        return hidden_states\n",
        "\n",
        "    def extract_elmo(self, tokenized_dataset, idx, span_start, span_end):\n",
        "        text = \" \".join(tokenized_dataset[idx][\"text\"])\n",
        "        hidden_states = elmo.get_elmo_embedding(text)\n",
        "        return hidden_states[:, span_start:span_end, :]\n",
        "\n",
        "\n",
        "    def extract_batch(self, tokenized_dataset, idx, unique_batch_size=32):\n",
        "        # if \"glove\" in model_checkpoint:\n",
        "        #     return extract_batch_glove(tokenized_dataset, idx, unique_batch_size)\n",
        "        # if \"elmo\" in model_checkpoint:\n",
        "        #     return extract_batch_elmo(tokenized_dataset, idx, unique_batch_size)\n",
        "\n",
        "        # print(idx)\n",
        "        self.vprint(\"e1\")\n",
        "        dataset_len = len(tokenized_dataset)\n",
        "        unique_texts_in_batch = []\n",
        "        i = idx\n",
        "        while len(unique_texts_in_batch) < unique_batch_size and i < dataset_len:\n",
        "            # print(i)\n",
        "            text = tokenized_dataset[i][\"text\"]\n",
        "            if not text in unique_texts_in_batch:\n",
        "                unique_texts_in_batch.append(text)\n",
        "            i += 1\n",
        "        tokenizer.padding_side = 'right'  # Important: lef will change the span indices\n",
        "        tokenized_batch = tokenizer(unique_texts_in_batch, padding=True, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            if SEQ2SEQ_MODEL:\n",
        "                outputs = self.language_model(input_ids=tokenized_batch.input_ids, decoder_input_ids=tokenized_batch.input_ids, output_hidden_states=True)\n",
        "            else:\n",
        "                outputs = self.language_model(**tokenized_batch)\n",
        "        # torch.cuda.synchronize()\n",
        "        # current_hidden_states = np.asarray([val.detach().cpu().numpy() for val in outputs.hidden_states])\n",
        "        if SEQ2SEQ_MODEL:\n",
        "            encoder_hidden_states = torch.stack([val.detach() for val in outputs.encoder_hidden_states])\n",
        "            decoder_hidden_states = torch.stack([val.detach() for val in outputs.decoder_hidden_states])\n",
        "            current_hidden_states = torch.cat((encoder_hidden_states, decoder_hidden_states), dim=0)  # concat from layers\n",
        "        else:\n",
        "            current_hidden_states = torch.stack([val.detach() for val in outputs.hidden_states])\n",
        "        # self.vprint(current_hidden_states.shape)  # (13, 16, 34, 768)\n",
        "        \n",
        "        extracted_batch_embeddings = {}\n",
        "        for i, unique_text in enumerate(unique_texts_in_batch):\n",
        "            hashable_input = repr(unique_text)\n",
        "            if not hasattr(self, 'up_to_layer') or self.up_to_layer == -1:\n",
        "                extracted_batch_embeddings[hashable_input] = current_hidden_states[:, i, :, :]\n",
        "            else:\n",
        "                extracted_batch_embeddings[hashable_input] = current_hidden_states[:self.up_to_layer+1, i, :, :]\n",
        "        self.vprint(\"e2\")\n",
        "        return extracted_batch_embeddings\n",
        "    \n",
        "    def pad_sequence(list_of_torch, pad_len, pad_value=0):\n",
        "        shape = list_of_torch[0].shape\n",
        "        num_layers = shape[0]\n",
        "        span_original_len = shape[1]\n",
        "        embedding_dim = shape[2]\n",
        "        output = torch.zeros()\n",
        "\n",
        "    def extract_embeddings(self, tokenized_dataset, start_idx, end_idx, pad=True):\n",
        "        \"\"\" Extract raw embeddings for [start_idx, end_idx) of tokenized_dataset from language_model \n",
        "            \n",
        "        Returns:\n",
        "            extract_embeddings: DataFrame with cols (span1, span2?, label) and span shape is (range_len, (#layers, span_len, embedding_dim))\n",
        "        \"\"\"\n",
        "        num_of_spans = self.dataset_handler.dataset_info.num_of_spans\n",
        "        \n",
        "        if num_of_spans == 2:\n",
        "            max_span_len_in_batch = max(max(tokenized_dataset[start_idx:end_idx][\"span1_len\"]), max(tokenized_dataset[start_idx:end_idx][\"span2_len\"]))\n",
        "        elif num_of_spans == 1:\n",
        "            max_span_len_in_batch = max(tokenized_dataset[start_idx:end_idx][\"span1_len\"])\n",
        "        # print(\"max_span_len_in_batch\", max_span_len_in_batch)\n",
        "        \n",
        "\n",
        "        span_repr = self.init_span_dict(num_of_spans, pad)\n",
        "        self.vprint(\"f1\")\n",
        "        for i in range(start_idx, end_idx):\n",
        "            row = tokenized_dataset[i]\n",
        "            if \"glove\" in model_checkpoint:\n",
        "                span1_hidden_states = self.extract_glove(tokenized_dataset, i, row[\"span1\"][0], row[\"span1\"][1])\n",
        "            elif \"elmo\" in model_checkpoint:\n",
        "                span1_hidden_states = self.extract_elmo(tokenized_dataset, i, row[\"span1\"][0], row[\"span1\"][1])\n",
        "                self.vprint(\"f2\")\n",
        "            else:\n",
        "                hashable_input = repr(tokenized_dataset[i][\"text\"])\n",
        "                \n",
        "                if hashable_input in self.cached_embeddings:\n",
        "                    self.current_hidden_states = self.cached_embeddings[hashable_input]\n",
        "                else:\n",
        "                    if hashable_input not in self.extracted_batch_embeddings:\n",
        "                        self.extracted_batch_embeddings = self.extract_batch(tokenized_dataset, i)\n",
        "                        # if len(self.cached_embeddings) < CACHE_LEN:\n",
        "                        #     for key, value in self.extracted_batch_embeddings.items():\n",
        "                        #         self.cached_embeddings[key] = value\n",
        "                        #     print(f\"Cached {len(self.cached_embeddings)}\")\n",
        "                    self.current_hidden_states = self.extracted_batch_embeddings[hashable_input]\n",
        "                \n",
        "                # if hashable_input not in self.extracted_batch_embeddings:\n",
        "                #     self.extracted_batch_embeddings = self.extract_batch(tokenized_dataset, i)    \n",
        "                # self.current_hidden_states = self.extracted_batch_embeddings[hashable_input]\n",
        "\n",
        "                span1_hidden_states = self.current_hidden_states[:, row[\"span1\"][0]:row[\"span1\"][1], :]  # (#layer, span_len, embd_dim)\n",
        "            \n",
        "            if pad:\n",
        "                s1, a1 = self.pad_span(span1_hidden_states, max_span_len_in_batch)\n",
        "                span_repr[\"span1\"].append(s1)\n",
        "                span_repr[\"span1_attention_mask\"].append(a1)\n",
        "            else:\n",
        "                span_repr[\"span1\"].append(span1_hidden_states)\n",
        "\n",
        "            if num_of_spans == 2:\n",
        "                if \"glove\" in model_checkpoint:\n",
        "                    span2_hidden_states = self.extract_glove(tokenized_dataset, i, row[\"span2\"][0], row[\"span2\"][1])\n",
        "                elif \"elmo\" in model_checkpoint:\n",
        "                    span2_hidden_states = self.extract_elmo(tokenized_dataset, i, row[\"span2\"][0], row[\"span2\"][1])\n",
        "                    self.vprint(\"f3\")\n",
        "                else:\n",
        "                    span2_hidden_states = self.current_hidden_states[:, row[\"span2\"][0]:row[\"span2\"][1], :]\n",
        "                if pad:\n",
        "                    s2, a2 = self.pad_span(span2_hidden_states, max_span_len_in_batch)\n",
        "                    span_repr[\"span2\"].append(s2)\n",
        "                    span_repr[\"span2_attention_mask\"].append(a2)\n",
        "                else:\n",
        "                    span_repr[\"span2\"].append(span2_hidden_states)\n",
        "            span_repr[\"one_hot_label\"].append(row[\"one_hot_label\"])\n",
        "            span_repr[\"label\"].append(row[\"label\"])\n",
        "        self.vprint(\"f4\")\n",
        "        return span_repr\n",
        "\n",
        "    def save_history(self, history_dict, mdl=False):\n",
        "        if mdl == True:\n",
        "            prefix = \"mdl/mdl_mohsen_\"\n",
        "            history_dict = {\"mdl_history\": history_dict}\n",
        "        else:\n",
        "            prefix = \"edge_probing_results_mohsen/\"\n",
        "        file_name = prefix + model_checkpoint + \"_\" + self.dataset_handler.dataset_info.dataset_name + \"_\" + str(SEED)\n",
        "        history_dict[\"Model\"] = model_checkpoint,\n",
        "        history_dict[\"Batch Size\"] = BATCH_SIZE,\n",
        "        history_dict[\"Learning Rate\"] = LEARNING_RATE,\n",
        "        history_dict[\"seed\"] = SEED\n",
        "        if hasattr(self, 'edge_probe_model'):\n",
        "            history_dict[\"probe_summary\"] = self.edge_probe_model.summary(do_print=False)\n",
        "        elif hasattr(self, 'edge_probe_models'):\n",
        "            history_dict[\"probe_summary\"] = self.edge_probe_models[0].summary(do_print=False)\n",
        "        else:\n",
        "            print(\"No Probe Found to Summarize!\")\n",
        "        history_dict[\"dataset_name\"] = self.dataset_handler.dataset_info.dataset_name\n",
        "        history_dict[\"dataset_statistics\"] = str(self.dataset_handler.dataset)\n",
        "\n",
        "        from pathlib import Path\n",
        "        Path(file_name).mkdir(parents=True, exist_ok=True)\n",
        "        with open(f\"{file_name}.json\", \"w\") as json_file:\n",
        "            json.dump(history_dict, json_file, indent=4)\n",
        "        # with open(f\"{file_name}.pkl\", \"wb\") as pkl_file:\n",
        "        #     pickle.dump(history_dict, pkl_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmefJ5pCqq7D"
      },
      "source": [
        "# Edge Probe Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV_IoVoeLTot"
      },
      "source": [
        "class Edge_probe_trainer(Trainer):\n",
        "    # Public:\n",
        "    def __init__(self, language_model, dataset_handler: Dataset_handler, \n",
        "                 verbose=True, device='cuda', edge_probe_model_checkpoint=None,\n",
        "                 pool_method=\"attn\", start_eval = False, \n",
        "                 history_checkpoint=None, up_to_layer=-1, normalize_layers=False):\n",
        "        self.dataset_handler = dataset_handler\n",
        "        self.num_of_spans = self.dataset_handler.dataset_info.num_of_spans\n",
        "        self.up_to_layer = up_to_layer\n",
        "        self.language_model = language_model\n",
        "        self.language_model.config.output_hidden_states = True\n",
        "        self.device = device\n",
        "        self.verbose = verbose\n",
        "        self.start_eval = start_eval\n",
        "        def vprint(text):\n",
        "            if verbose:\n",
        "                print(datetime.datetime.now().time(), text)\n",
        "        self.vprint = vprint\n",
        "\n",
        "        self.current_hidden_states = None\n",
        "        self.last_input_ids = None\n",
        "        self.extracted_batch_embeddings = {}\n",
        "\n",
        "        self.cached_embeddings = {}\n",
        "\n",
        "        self.vprint(\"Moving to device\")\n",
        "        for param in self.language_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.language_model.eval()\n",
        "        self.language_model.to(self.device)\n",
        "        num_layers, input_span_len, embedding_dim, num_classes = self.get_language_model_properties()\n",
        "        self.MLP_device = self.device\n",
        "        if edge_probe_model_checkpoint == None:\n",
        "            print(\"Creating New EPM\")\n",
        "            self.edge_probe_model = Edge_probe_model(\n",
        "                num_of_spans = self.num_of_spans,\n",
        "                num_layers = num_layers,\n",
        "                input_span_len = input_span_len,\n",
        "                embedding_dim = embedding_dim, \n",
        "                num_classes = num_classes,\n",
        "                device = self.MLP_device,\n",
        "                pool_method = pool_method,\n",
        "                normalize_layers = normalize_layers\n",
        "            )\n",
        "        else:\n",
        "            print(\"Starting From a Pretrained EPM\")\n",
        "            self.edge_probe_model = edge_probe_model_checkpoint\n",
        "        \n",
        "\n",
        "        if history_checkpoint is None:\n",
        "            self.history = {\"loss\": {\"train\": [], \"dev\": [], \"test\": []}, \n",
        "                            \"metrics\": \n",
        "                            {\n",
        "                                \"micro_f1\": {\"dev\": [], \"test\": []},\n",
        "                                \"macro_f1\": {\"dev\": [], \"test\": []},\n",
        "                                \"accuracy\": {\"dev\": [], \"test\": []},\n",
        "                                \"report\": {\"dev\": [], \"test\": []}\n",
        "                            },\n",
        "\n",
        "                            \"layers_weights\": [],\n",
        "                            }\n",
        "            print(\"Creating New History\")\n",
        "        else:\n",
        "            print(\"Using History Checkpoint\")\n",
        "            self.history = history_checkpoint\n",
        "    \n",
        "    def train(self, batch_size, epochs=3):\n",
        "        tokenized_dataset = self.dataset_handler.tokenized_dataset[\"train\"]\n",
        "        tokenized_dataset_dev = self.dataset_handler.tokenized_dataset[\"dev\"]\n",
        "        tokenized_dataset_test = self.dataset_handler.tokenized_dataset[\"test\"]\n",
        "\n",
        "        # self.edge_probe_model.to(self.device)\n",
        "        self.edge_probe_model.to(self.MLP_device)\n",
        "        # self.vprint(\"Counting dataset rows\")\n",
        "        dataset_len = len(tokenized_dataset)\n",
        "        dev_dataset_len = len(tokenized_dataset_dev)\n",
        "        test_dataset_len = len(tokenized_dataset_test)\n",
        "        print(f\"Train on {dataset_len} samples, validate on {dev_dataset_len} samples, test on {test_dataset_len} samples\")\n",
        "        # dataset_len = 60\n",
        "        if self.start_eval:\n",
        "            self.update_history(epoch = 0)\n",
        "\n",
        "        step_counter = 0\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            steps = 0\n",
        "            print(\"----------------\\n\")\n",
        "            self.edge_probe_model.train()\n",
        "            for i in tqdm(range(0, dataset_len, batch_size), desc=f\"[Epoch {epoch + 1}/{epochs}]\"):\n",
        "                \n",
        "                # step_counter += 1\n",
        "                # if i > 0 and step_counter > 0 and step_counter % 1000 == 0:\n",
        "                #     self.update_history(epoch + 1, train_loss = running_loss / steps)\n",
        "                #     self.draw_weights(epoch)\n",
        "                \n",
        "                self.vprint(\"Start\")\n",
        "                step = batch_size\n",
        "                if i + batch_size > dataset_len:\n",
        "                    step = dataset_len - i\n",
        "                # print(f\"WWW[{i}, {i+step})\")\n",
        "                \n",
        "                self.vprint(\"Extracting\")\n",
        "                # self.vprint(\"prepare\")\n",
        "                spans_torch_dict = self.prepare_batch_data(tokenized_dataset, i, i + step, pad=True, cache_prefix=\"t\")\n",
        "                labels = spans_torch_dict[\"one_hot_labels\"]\n",
        "                # zero the parameter gradients\n",
        "                self.edge_probe_model.optimizer.zero_grad()\n",
        "    \n",
        "                # forward + backward + optimize\n",
        "                self.vprint(\"Forward MLP\")\n",
        "                # self.vprint(\"epm\")\n",
        "                outputs = self.edge_probe_model(spans_torch_dict)\n",
        "                self.vprint(\"Loss\")\n",
        "                loss = self.edge_probe_model.training_criterion(outputs.to(self.device), labels.float().to(self.device))\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.edge_probe_model.parameters(), 5.0)\n",
        "                self.edge_probe_model.optimizer.step()\n",
        "    \n",
        "                running_loss += loss.item()\n",
        "                steps += 1\n",
        "                self.vprint(\"Done\")\n",
        "                # print(f\"loss: {running_loss / steps}\")\n",
        "\n",
        "            self.update_history(epoch + 1, train_loss = running_loss / steps)\n",
        "            self.draw_weights(epoch)\n",
        "            self.save_history(self.history)\n",
        "\n",
        "    def calc_loss(self, tokenized_dataset, batch_size=16, print_metrics=False, just_micro=False, desc=\"\"):\n",
        "        self.edge_probe_model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0\n",
        "            dataset_len = len(tokenized_dataset[\"text\"])\n",
        "            steps = 0\n",
        "            preds = None\n",
        "            for i in tqdm(range(0, dataset_len, batch_size), desc=desc):\n",
        "                # if int(i / batch_size) % 100 == 0:\n",
        "                #     print(\"memory:\", psutil.virtual_memory().percent, gc.collect(), psutil.virtual_memory().percent)\n",
        "                step = batch_size\n",
        "                if i + batch_size > dataset_len:\n",
        "                    step = dataset_len - i\n",
        "                \n",
        "                if desc == \"Test Loss\":\n",
        "                    spans_torch_dict = self.prepare_batch_data(tokenized_dataset, i, i + step, pad=True, cache_prefix=\"test\")\n",
        "                else:\n",
        "                    spans_torch_dict = self.prepare_batch_data(tokenized_dataset, i, i + step, pad=True)\n",
        "                labels = spans_torch_dict[\"one_hot_labels\"]\n",
        "                # forward\n",
        "                outputs = self.edge_probe_model(spans_torch_dict)\n",
        "                \n",
        "                preds = outputs if i == 0 else torch.cat((preds, outputs), 0)\n",
        "                loss = self.edge_probe_model.training_criterion(outputs.to(self.device), labels.float().to(self.device))\n",
        "                running_loss += loss.item()\n",
        "                steps += 1\n",
        "\n",
        "        print(preds[0:4])\n",
        "        preds = preds.cpu().argmax(-1)\n",
        "        y_true = np.array(tokenized_dataset[\"one_hot_label\"]).argmax(-1)\n",
        "        print(preds[0:4])\n",
        "        print(y_true[0:4])\n",
        "        labels_list = self.dataset_handler.labels_list\n",
        "        if self.dataset_handler.dataset_info.dataset_name == \"semeval\":\n",
        "            other_idx = labels_list.index(\"Other\")\n",
        "            semeval_labels = [i for i in range(len(labels_list))]\n",
        "            semeval_labels.remove(other_idx)\n",
        "            print(f\"Other -> {other_idx} / {semeval_labels}\")\n",
        "            micro_f1 = f1_score(y_true, preds, average='micro', labels=semeval_labels)\n",
        "            macro_f1 = f1_score(y_true, preds, average='macro', labels=semeval_labels)\n",
        "        else:\n",
        "            micro_f1 = f1_score(y_true, preds, average='micro')\n",
        "            macro_f1 = f1_score(y_true, preds, average='macro')\n",
        "        \n",
        "        accuracy = sklearn.metrics.accuracy_score(y_true, preds)\n",
        "        \n",
        "        \n",
        "        report = classification_report(y_true, preds, target_names=labels_list, labels=range(len(labels_list)))\n",
        "        if print_metrics:\n",
        "            if not just_micro:\n",
        "                print(report)\n",
        "            print(\"MICRO F1:\", micro_f1)\n",
        "        return running_loss / steps, micro_f1, macro_f1, accuracy, report\n",
        "\n",
        "    # Private:\n",
        "    def update_history(self, epoch, train_loss = None):\n",
        "        if train_loss is None:\n",
        "            train_loss, train_f1 = self.calc_loss(self.dataset_handler.tokenized_dataset[\"train\"], print_metrics=True, desc=\"Train Loss\")\n",
        "        dev_loss, dev_f1, dev_macro_f1, dev_accuracy, dev_report = self.calc_loss(self.dataset_handler.tokenized_dataset[\"dev\"], print_metrics=True, desc=\"Dev Loss\")\n",
        "        test_loss, test_f1, test_macro_f1, test_accuracy, test_report = self.calc_loss(self.dataset_handler.tokenized_dataset[\"test\"], print_metrics=True, desc=\"Test Loss\")\n",
        "        self.history[\"loss\"][\"train\"].append(train_loss)\n",
        "        self.history[\"loss\"][\"dev\"].append(dev_loss)\n",
        "        self.history[\"loss\"][\"test\"].append(test_loss)\n",
        "\n",
        "        self.history[\"metrics\"][\"micro_f1\"][\"dev\"].append(dev_f1)\n",
        "        self.history[\"metrics\"][\"macro_f1\"][\"dev\"].append(dev_macro_f1)\n",
        "        self.history[\"metrics\"][\"accuracy\"][\"dev\"].append(dev_accuracy)\n",
        "        # self.history[\"metrics\"][\"report\"][\"dev\"].append(dev_report)\n",
        "\n",
        "        self.history[\"metrics\"][\"micro_f1\"][\"test\"].append(test_f1)\n",
        "        self.history[\"metrics\"][\"macro_f1\"][\"test\"].append(test_macro_f1)\n",
        "        self.history[\"metrics\"][\"accuracy\"][\"test\"].append(test_accuracy)\n",
        "        self.history[\"metrics\"][\"report\"][\"test\"].append(test_report)\n",
        "\n",
        "        self.history[\"layers_weights\"].append(torch.nn.functional.softmax(self.edge_probe_model.weighing_params).tolist())\n",
        "        print('[%d] loss: %.4f, val_loss: %.4f, test_loss: %.4f' % (epoch, self.history[\"loss\"][\"train\"][-1], self.history[\"loss\"][\"dev\"][-1], self.history[\"loss\"][\"test\"][-1]))\n",
        "\n",
        "    def draw_weights(self, epoch=0):\n",
        "        if(epoch % 1 == 0):\n",
        "            # w = self.edge_probe_model.weighing_params.tolist()\n",
        "            w = torch.nn.functional.softmax(self.edge_probe_model.weighing_params).cpu().detach().numpy()\n",
        "            print(w)\n",
        "            # print(self.history)\n",
        "            plt.bar(np.arange(len(w), dtype=int), w)\n",
        "            plt.ylabel('Weight')\n",
        "            plt.xlabel('Layer');\n",
        "            plt.show()\n",
        "\n",
        "            wsoft = nn.functional.softmax(self.edge_probe_model.weighing_params)\n",
        "            print(\"CG\", sum(idx*val for idx, val in enumerate(wsoft)))\n",
        "\n",
        "            print(\"Loss History\")\n",
        "            loss_history = self.history[\"loss\"]\n",
        "            x = range(len(loss_history[\"train\"]))\n",
        "            plt.plot(x, loss_history[\"train\"])\n",
        "            plt.plot(x, loss_history[\"dev\"])\n",
        "            plt.plot(x, loss_history[\"test\"])\n",
        "            plt.legend(['Train', 'Dev', 'Test'], loc='lower left')\n",
        "            plt.show()\n",
        "\n",
        "            print(\"Micro f1 History\")\n",
        "            f1_history = self.history[\"metrics\"][\"micro_f1\"]\n",
        "            x = range(len(f1_history[\"dev\"]))\n",
        "            plt.plot(x, f1_history[\"dev\"])\n",
        "            plt.plot(x, f1_history[\"test\"])\n",
        "            plt.legend(['Dev', 'Test'], loc='upper left')\n",
        "            plt.show()\n",
        "\n",
        "    def analyze_attention(self, dataset_part=\"train\", row_idx=0, new_text = None):\n",
        "        if new_text is not None:\n",
        "            new_dataset_info = Dataset_info(\"manual\", num_of_spans=1, manual_text=new_text)\n",
        "            new_dataset_handler = Dataset_handler(new_dataset_info);\n",
        "            dataset_part = \"test\"\n",
        "            row_idx=0\n",
        "            tokenized_dataset = new_dataset_handler.tokenized_dataset[dataset_part]\n",
        "        else:\n",
        "            tokenized_dataset = self.dataset_handler.tokenized_dataset[dataset_part]\n",
        "        \n",
        "        self.edge_probe_model.eval()\n",
        "        with torch.no_grad():\n",
        "            spans_torch_dict = self.prepare_batch_data(tokenized_dataset, row_idx, row_idx + 1, pad=True)\n",
        "            labels = spans_torch_dict[\"one_hot_labels\"]\n",
        "            # forward\n",
        "            outputs = self.edge_probe_model(spans_torch_dict)\n",
        "        preds = outputs\n",
        "        loss = self.edge_probe_model.training_criterion(outputs.to(self.device), labels.float().to(self.device))\n",
        "        running_loss = loss.item()\n",
        "        print(preds[0:9])\n",
        "        preds = preds.cpu().argmax(-1)\n",
        "        y_true = np.array(tokenized_dataset[\"one_hot_label\"]).argmax(-1)\n",
        "        # print(preds[0:9])\n",
        "        # print(y_true[row_idx])\n",
        "        if \"glove\" in model_checkpoint:\n",
        "            text = tokenized_dataset[row_idx][\"text\"]\n",
        "        else:\n",
        "            start = tokenized_dataset[row_idx][\"span1\"][0]\n",
        "            end = tokenized_dataset[row_idx][\"span1\"][1]\n",
        "            text = tokenizer.convert_ids_to_tokens(tokenized_dataset[row_idx][\"input_ids\"][start:end])\n",
        "        return text, self.edge_probe_model.span1_pooling_net.last_attention_wts, preds, y_true[row_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4M8CJNOhsur"
      },
      "source": [
        "my_edge_probe_trainer = None\n",
        "edge_probe_model_checkpoint = None\n",
        "history = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "# disk_cache.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyQ_rExaGWO2"
      },
      "source": [
        "# %timeit disk_cache[64]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG0ukbYWFpRW"
      },
      "source": [
        "try:\n",
        "    edge_probe_model_checkpoint = my_edge_probe_trainer.edge_probe_model\n",
        "except:\n",
        "    edge_probe_model_checkpoint = None\n",
        "my_edge_probe_trainer = Edge_probe_trainer(model,\n",
        "                                           my_dataset_handler, \n",
        "                                           device=DEVICE,\n",
        "                                           pool_method=POOL_METHOD,\n",
        "                                           edge_probe_model_checkpoint=edge_probe_model_checkpoint,\n",
        "                                           history_checkpoint=history,\n",
        "                                           up_to_layer = -1,\n",
        "                                           normalize_layers=True,\n",
        "                                           verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiQfH3OZtS-N"
      },
      "source": [
        "print(\"Model:\", model_checkpoint)\n",
        "print(\"Dataset:\", my_dataset_info.dataset_name)\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "a = my_edge_probe_trainer.edge_probe_model.summary(do_print=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiXy4GDCHZsr"
      },
      "source": [
        "my_edge_probe_trainer.train(batch_size = BATCH_SIZE, epochs=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xLwkoahUNgx"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! cp -R edge_probing_results_mohsen/ ./Mohsen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYVZDCxOdlJP"
      },
      "source": [
        "# torch.save(my_edge_probe_trainer.edge_probe_model.state_dict(), \"EPM_xlnet-large-cased-attn_epoch3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRGBY2i5EbMM"
      },
      "source": [
        "history = my_edge_probe_trainer.history\n",
        "history[\"metrics\"][\"report\"][\"dev\"] = []\n",
        "history[\"metrics\"][\"report\"][\"test\"] = []\n",
        "print(my_edge_probe_trainer.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1bmne4rz-CY"
      },
      "source": [
        "max(history['metrics']['micro_f1']['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ6Ic5hySOY_"
      },
      "source": [
        "### Attention Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHjmjXIznWMN"
      },
      "source": [
        "### Attention Analysis ###\n",
        "for i, row in enumerate(my_dataset_handler.dataset[\"train\"]):\n",
        "    if \"The ship is a wealth factory on the\" in row[\"text\"]:\n",
        "        print(i, row)\n",
        "        \n",
        "# text_tokens, attention_weights, pred, true = my_edge_probe_trainer.analyze_attention(dataset_part=\"test\", row_idx=58)\n",
        "text_tokens, attention_weights, pred, true = my_edge_probe_trainer.analyze_attention(new_text=\"Some day you'll go far, and I hope you stay there\")\n",
        "# text_tokens, attention_weights, pred, true = my_edge_probe_trainer.analyze_attention(new_text=\"I am not insulting you, i am describing you\")\n",
        "# text_tokens, attention_weights, pred, true = my_edge_probe_trainer.analyze_attention(new_text=\"I am not insulting you\")\n",
        "# text_tokens, attention_weights, pred, true = my_edge_probe_trainer.analyze_attention(new_text=\"Someday you will say something intelligent\")\n",
        "# text_tokens, attention_weights, pred, true = my_edge_probe_trainer.analyze_attention(new_text=\"If I had a dollar for everytime you said something smart, I'd be broke.\")\n",
        "\n",
        "print(\"Pred:\", pred, \"True Label:\", true)\n",
        "# text_tokens = text_tokens[1:-1]  # Delete CLS and SEP\n",
        "print (*text_tokens, sep =', ')\n",
        "print(len(text_tokens))\n",
        "# print(attention_weights.shape)\n",
        "# print(attention_weights)\n",
        "attention_weights = attention_weights[0, :, :, 0].cpu()\n",
        "print(attention_weights.shape)\n",
        "# print(attention_weights)\n",
        "\n",
        "plt.figure(figsize=(19, 8))\n",
        "plt.rcParams.update({'font.size': 13})\n",
        "plt.subplots_adjust(left=0.125,\n",
        "                    bottom=0.1, \n",
        "                    right=0.9, \n",
        "                    top=0.9, \n",
        "                    wspace=0.2, \n",
        "                    hspace=0.4)\n",
        "\n",
        "ws = []\n",
        "for i in range(0, len(attention_weights)):\n",
        "    ws.append(list(np.array(attention_weights[i].cpu())))\n",
        "print(ws)\n",
        "    \n",
        "for i in range(0, len(attention_weights), 4):\n",
        "    plt.subplot(2, 2, i/4+1)\n",
        "    plt.title(f\"Attention Weights in Layer {i}\")\n",
        "    plt.ylabel('Attention Weight')\n",
        "    x_r = range(len(text_tokens))\n",
        "    plt.bar(x_r, attention_weights[i], color=\"#8b0000\")\n",
        "    plt.xticks(x_r, text_tokens, rotation='80')\n",
        "plt.show()\n",
        "\n",
        "wsoft = my_edge_probe_trainer.edge_probe_model.weighing_params.cpu().detach()\n",
        "wtd_encoded_repr = 0\n",
        "soft_weight = nn.functional.softmax(wsoft, dim=0)\n",
        "for i in range(len(attention_weights)):\n",
        "    wtd_encoded_repr += soft_weight[i] * attention_weights[i]\n",
        "# wtd_encoded_repr += soft_weight[-1] * encoded_layers[:, -1, :]\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.title(f\"Mixed Attention Weights\")\n",
        "plt.ylabel('Attention Weight')\n",
        "x_r = range(len(text_tokens))\n",
        "print(\"mixed\", wtd_encoded_repr)\n",
        "plt.bar(x_r, wtd_encoded_repr, color=\"#8b0000\")\n",
        "plt.xticks(x_r, text_tokens, rotation='80')\n",
        "plt.show()\n",
        "\n",
        "plt.title(f\"Layer Weights\")\n",
        "plt.ylabel('Layer Weight')\n",
        "plt.bar(range(len(wsoft)), wsoft, color=\"blue\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vERfRerpbLUx"
      },
      "source": [
        "# MDL Probe Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMgZ1HNgbOxK"
      },
      "source": [
        "class MDL_probe_trainer(Trainer):\n",
        "    # Public:\n",
        "    def __init__(self, language_model, dataset_handler: Dataset_handler, \n",
        "                 verbose=True, device='cuda',\n",
        "                 pool_method=\"attn\", start_eval = False, normalize_layers=False, early_stopping_patience=2):\n",
        "        self.portion_ratios = [0.001, 0.002, 0.004, 0.008, 0.016, 0.032, 0.0625, 0.125, 0.25, 0.5, 1.0]\n",
        "        self.early_stopping_patience = early_stopping_patience\n",
        "        self.dataset_handler = dataset_handler\n",
        "        self.num_of_spans = self.dataset_handler.dataset_info.num_of_spans\n",
        "        self.language_model = language_model\n",
        "        self.language_model.config.output_hidden_states = True\n",
        "        self.device = device\n",
        "        self.verbose = verbose\n",
        "        self.start_eval = start_eval\n",
        "        def vprint(text):\n",
        "            if verbose:\n",
        "                print(datetime.datetime.now().time(), text)\n",
        "        self.vprint = vprint\n",
        "\n",
        "        self.current_hidden_states = None\n",
        "        self.last_input_ids = None\n",
        "        self.extracted_batch_embeddings = {}\n",
        "\n",
        "        self.cached_embeddings = {}\n",
        "\n",
        "        self.vprint(\"Moving to device\")\n",
        "        for param in self.language_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.language_model.eval()\n",
        "        self.language_model.to(self.device)\n",
        "        num_layers, input_span_len, embedding_dim, num_classes = self.get_language_model_properties()\n",
        "        print(num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.MLP_device = self.device\n",
        "        \n",
        "        print(\"Creating New EPM\")\n",
        "        self.edge_probe_models = []\n",
        "        for i in range(num_layers):\n",
        "            edge_probe_model = Edge_probe_model(\n",
        "                num_of_spans = self.num_of_spans,\n",
        "                num_layers = 1,\n",
        "                input_span_len = input_span_len,\n",
        "                embedding_dim = embedding_dim, \n",
        "                num_classes = num_classes,\n",
        "                device = self.MLP_device,\n",
        "                pool_method = pool_method,\n",
        "                normalize_layers = normalize_layers,\n",
        "                use_cross_entropy = True\n",
        "            )\n",
        "            self.edge_probe_models.append(edge_probe_model)\n",
        "        \n",
        "        self.history = []\n",
        "        for i in range(len(self.portion_ratios)):\n",
        "            self.history.append({\"loss\": {\"train\": [], \"test\": [], \"mdl\": []}, \n",
        "                            \"metrics\": \n",
        "                            {\"micro_f1\": {\"test\": []},\n",
        "                             \"online_codelength\": [], \n",
        "                             \"compression\": []}\n",
        "                            })\n",
        "            \n",
        "        print(\"Creating New History\")\n",
        "\n",
        "    def train(self, batch_size, epochs=3):\n",
        "        temp_dataset_train = self.dataset_handler.tokenized_dataset[\"train\"]\n",
        "        temp_dataset_dev = self.dataset_handler.tokenized_dataset[\"dev\"]\n",
        "        temp_dataset_test = self.dataset_handler.tokenized_dataset[\"test\"]\n",
        "        num_labels = len(self.dataset_handler.labels_list)\n",
        "\n",
        "        print(self.dataset_handler.tokenized_dataset)\n",
        "        # concatenated_dataset = datasets.concatenate_datasets([temp_dataset_train, temp_dataset_test])\n",
        "        concatenated_dataset = temp_dataset_train\n",
        "        print(concatenated_dataset)\n",
        "\n",
        "        for edge_probe_model in self.edge_probe_models:\n",
        "            edge_probe_model.to(self.MLP_device)\n",
        "        if self.start_eval:\n",
        "            self.update_history(epoch = 0)\n",
        "        for portion_idx, portion_ratio in enumerate(self.portion_ratios[0:-1]):\n",
        "            test_portion_ratio = self.portion_ratios[portion_idx + 1] - portion_ratio\n",
        "            train_test_dataset = concatenated_dataset.train_test_split(train_size=portion_ratio, test_size=test_portion_ratio, shuffle=False)\n",
        "            train_dataset = train_test_dataset[\"train\"]\n",
        "            test_dataset = train_test_dataset[\"test\"]\n",
        "            train_len = len(train_dataset)\n",
        "            test_len = len(test_dataset)\n",
        "            print(\"#########################################################\")\n",
        "            print(f\"[{portion_idx + 1}/{len(self.portion_ratios)}] Train Portion Ratio = {portion_ratio}, Test Portion Ratio = {test_portion_ratio}\")\n",
        "            print(f\"Train on {train_len} samples, test on {test_len} samples\")\n",
        "            print(\"#########################################################\")\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                running_loss = 0.0\n",
        "                steps = 0\n",
        "                print(\"----------------\\n\")\n",
        "                for edge_probe_model in self.edge_probe_models:\n",
        "                    edge_probe_model.train()\n",
        "                for i in tqdm(range(0, train_len, batch_size), desc=f\"[Epoch {epoch + 1}/{epochs}]\"):\n",
        "                    # if int(i / batch_size) % 1000 == 0:\n",
        "                    #     print(\"memory:\", psutil.virtual_memory().percent)\n",
        "                    self.vprint(\"Start\")\n",
        "                    step = batch_size\n",
        "                    if i + batch_size > train_len:\n",
        "                        step = train_len - i\n",
        "                    # print(f\"WWW[{i}, {i+step})\")\n",
        "                    \n",
        "                    self.vprint(\"Extracting\")\n",
        "                    # self.vprint(\"prepare\")\n",
        "                    spans_torch_dict = self.prepare_batch_data(train_dataset, i, i + step, pad=True, cache_prefix=\"mdl\")\n",
        "                    # print(spans_torch_dict[\"span1\"].shape, spans_torch_dict[\"span1_attention_mask\"].shape)\n",
        "                    labels = spans_torch_dict[\"one_hot_labels\"]\n",
        "                    labels = labels.argmax(dim=1).long()\n",
        "                    labels = labels.to(self.device)\n",
        "                    \n",
        "                    for epm_idx, edge_probe_model in enumerate(self.edge_probe_models):\n",
        "                        edge_probe_model.optimizer.zero_grad()\n",
        "            \n",
        "                        self.vprint(\"dict\")\n",
        "                        # print(spans_torch_dict[\"span1\"].shape) # torch.Size([32, 13, 9, 768])\n",
        "                        if self.num_of_spans == 2:\n",
        "                            span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :], \n",
        "                                            \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"],\n",
        "                                            \"span2\": spans_torch_dict[\"span2\"][:, epm_idx:epm_idx+1, :, :],\n",
        "                                            \"span2_attention_mask\": spans_torch_dict[\"span2_attention_mask\"],\n",
        "                                            }\n",
        "                        else:\n",
        "                            span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :], \n",
        "                                            \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"]}\n",
        "                        \n",
        "                        # forward + backward + optimize\n",
        "                        self.vprint(\"Forward MLP\")\n",
        "                        outputs = edge_probe_model(span_torch_dict)\n",
        "                        self.vprint(\"Loss\")\n",
        "                        loss = edge_probe_model.training_criterion(outputs.to(self.device), labels)\n",
        "                        loss.backward()\n",
        "                        # torch.nn.utils.clip_grad_norm_(edge_probe_model.parameters(), 5.0)\n",
        "                        edge_probe_model.optimizer.step()\n",
        "            \n",
        "                        running_loss += loss.item()\n",
        "                        steps += 1\n",
        "                    self.vprint(\"Done\")\n",
        "                    # print(f\"loss: {running_loss / steps}\")\n",
        "\n",
        "                \n",
        "                if epoch == epochs - 1 or self.check_early_stop(portion_idx):\n",
        "                    self.update_history(epoch + 1, portion_idx, train_dataset, test_dataset, train_loss = running_loss / steps, last_epoch_of_portion=True)\n",
        "                    self.save_history(self.history, mdl=True)\n",
        "                    break  # Early Stop\n",
        "                else:\n",
        "                    self.update_history(epoch + 1, portion_idx, train_dataset, test_dataset, train_loss = running_loss / steps)\n",
        "                self.draw_weights(epoch, portion_idx)\n",
        "                \n",
        "\n",
        "            self.draw_weights(0, portion_idx, comprehensive=True)\n",
        "            \n",
        "    # Private:\n",
        "    def check_early_stop(self, portion_idx):\n",
        "        current_portion_losses = self.history[portion_idx][\"loss\"][\"test\"]\n",
        "        return len(current_portion_losses) > self.early_stopping_patience and current_portion_losses[-self.early_stopping_patience] < current_portion_losses[-1]\n",
        "\n",
        "    def calc_loss(self, tokenized_dataset, batch_size=BATCH_SIZE, print_metrics=False, just_micro=False, desc=\"\"):\n",
        "        for edge_probe_model in self.edge_probe_models:\n",
        "            edge_probe_model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0\n",
        "            mdl_loss = [0 for _ in range(self.num_layers)]\n",
        "            dataset_len = len(tokenized_dataset[\"text\"])\n",
        "            steps = 0\n",
        "            preds = [[None] for _ in range(self.num_layers)]\n",
        "            micro_f1 = [[None] for _ in range(self.num_layers)]\n",
        "            for i in tqdm(range(0, dataset_len, batch_size), desc=desc):\n",
        "                # if int(i / batch_size) % 100 == 0:\n",
        "                #     print(\"memory:\", psutil.virtual_memory().percent, gc.collect(), psutil.virtual_memory().percent)\n",
        "                step = batch_size\n",
        "                if i + batch_size > dataset_len:\n",
        "                    step = dataset_len - i\n",
        "\n",
        "                spans_torch_dict = self.prepare_batch_data(tokenized_dataset, i, i + step, pad=True)\n",
        "                labels = spans_torch_dict[\"one_hot_labels\"]\n",
        "                labels = labels.argmax(dim=1).long()\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                for epm_idx, edge_probe_model in enumerate(self.edge_probe_models):\n",
        "                    if self.num_of_spans == 2:\n",
        "                        span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :],\n",
        "                                           \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"],\n",
        "                                           \"span2\": spans_torch_dict[\"span2\"][:, epm_idx:epm_idx+1, :, :],\n",
        "                                           \"span2_attention_mask\": spans_torch_dict[\"span2_attention_mask\"],\n",
        "                                           }\n",
        "                    else:\n",
        "                        span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :], \n",
        "                                           \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"]}\n",
        "\n",
        "                    # forward\n",
        "                    outputs = edge_probe_model(span_torch_dict)\n",
        "                    \n",
        "                    preds[epm_idx] = outputs if i == 0 else torch.cat((preds[epm_idx], outputs), 0)\n",
        "                    loss = edge_probe_model.training_criterion(outputs.to(self.device), labels)\n",
        "                    running_loss += loss.item()\n",
        "                    mdl_loss[epm_idx] += loss.item() * step  # MDL Loss won't be divided by steps\n",
        "                    steps += 1\n",
        "\n",
        "        y_true = np.array(tokenized_dataset[\"one_hot_label\"]).argmax(-1)\n",
        "        for idx, pred in enumerate(preds): \n",
        "            pred = pred.cpu().argmax(-1)\n",
        "            micro_f1[idx] = f1_score(y_true, pred, average='micro')\n",
        "        \n",
        "        if print_metrics:\n",
        "            # labels_list = self.dataset_handler.labels_list\n",
        "            # if not just_micro:\n",
        "            #     print(classification_report(y_true, preds, target_names=labels_list, labels=range(len(labels_list))))\n",
        "            print(\"MICRO F1:\", micro_f1)\n",
        "        return running_loss / steps, micro_f1, mdl_loss\n",
        "\n",
        "    def update_history(self, epoch, portion_idx, train_dataset, test_dataset, train_loss = None, last_epoch_of_portion=False):\n",
        "        test_loss, test_f1, test_mdl_loss = self.calc_loss(test_dataset, print_metrics=True, desc=\"Test Loss\")\n",
        "        self.history[portion_idx][\"loss\"][\"train\"].append(train_loss)\n",
        "        self.history[portion_idx][\"loss\"][\"test\"].append(test_loss) # Average of all layers\n",
        "        self.history[portion_idx][\"metrics\"][\"micro_f1\"][\"test\"].append(test_f1)\n",
        "        # self.history[\"layers_weights\"].append(self.edge_probe_model.weighing_params.tolist())\n",
        "        \n",
        "        # MDL Metric #\n",
        "        num_examples = len(train_dataset) + len(test_dataset)\n",
        "        num_labels = len(self.dataset_handler.labels_list)\n",
        "        uniform_codelength = num_examples * np.log2(num_labels)\n",
        "\n",
        "        self.history[portion_idx][\"loss\"][\"mdl\"].append(test_mdl_loss) # Includes all layers, multiplied by num_tests(step)\n",
        "\n",
        "        if last_epoch_of_portion:\n",
        "            if portion_idx == 0:\n",
        "                online_codelength = len(test_dataset) * np.log2(num_labels)\n",
        "            else:\n",
        "                print(portion_idx)\n",
        "                print(self.history[portion_idx][\"metrics\"][\"online_codelength\"])\n",
        "                online_codelength = self.history[portion_idx - 1][\"metrics\"][\"online_codelength\"][-1]\n",
        "\n",
        "            np_mdl = np.array(self.history[portion_idx][\"loss\"][\"mdl\"])\n",
        "            min_mdl_loss_in_batch = np_mdl.min(axis=0)\n",
        "            print(min_mdl_loss_in_batch.shape, np.array(test_mdl_loss).shape)\n",
        "\n",
        "            # online_codelength += min_mdl_loss_in_batch\n",
        "            online_codelength += min_mdl_loss_in_batch / np.log(2)\n",
        "            compression = uniform_codelength / online_codelength\n",
        "            self.history[portion_idx][\"metrics\"][\"online_codelength\"].append(list(online_codelength))\n",
        "            self.history[portion_idx][\"metrics\"][\"compression\"].append(list(compression))\n",
        "            print(\"Online codelength: {} kbits\".format(np.round(online_codelength / 1024, 2)))\n",
        "            print(\"Compression: {} \".format(np.round(compression, 2)))\n",
        "\n",
        "        print('[%d] loss:' % (epoch))\n",
        "        print(\"Train Loss:\", self.history[portion_idx][\"loss\"][\"train\"][-1])\n",
        "        print(\"Test Loss:\", self.history[portion_idx][\"loss\"][\"test\"][-1])\n",
        "        print(\"MDL Loss:\", self.history[portion_idx][\"loss\"][\"mdl\"][-1])\n",
        "\n",
        "\n",
        "    def draw_weights(self, epoch, portion_idx, comprehensive=False):\n",
        "        w = self.history[portion_idx][\"metrics\"][\"micro_f1\"][\"test\"][-1]\n",
        "        # print(self.history)\n",
        "        plt.bar(np.arange(len(w), dtype=int), w)\n",
        "        plt.ylabel('micro f1')\n",
        "        plt.xlabel('Layer');\n",
        "        plt.show()\n",
        "\n",
        "        if comprehensive:\n",
        "            print(self.history)\n",
        "            w = self.history[portion_idx][\"metrics\"][\"online_codelength\"][-1]\n",
        "            plt.bar(np.arange(len(w), dtype=int), w, color=\"magenta\")\n",
        "            plt.ylabel('Online Codelength')\n",
        "            plt.xlabel('Layer');\n",
        "            plt.show()\n",
        "\n",
        "            w = self.history[portion_idx][\"metrics\"][\"compression\"][-1]\n",
        "            plt.bar(np.arange(len(w), dtype=int), w, color=\"magenta\")\n",
        "            plt.ylabel('Compression')\n",
        "            plt.xlabel('Layer');\n",
        "            plt.show()\n",
        "\n",
        "        print(\"Loss History\")\n",
        "        loss_history = self.history[portion_idx][\"loss\"]\n",
        "        x = range(len(loss_history[\"train\"]))\n",
        "        plt.plot(x, loss_history[\"train\"])\n",
        "        plt.plot(x, loss_history[\"test\"])\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Test'], loc='lower left')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Full MDL Loss History\")\n",
        "        train_loss_history = []\n",
        "        test_loss_history = []\n",
        "        for i in range(len(self.history)):\n",
        "            train_loss_history.extend(self.history[i][\"loss\"][\"train\"])\n",
        "            test_loss_history.extend(self.history[i][\"loss\"][\"test\"])\n",
        "        x = range(len(train_loss_history))\n",
        "        plt.plot(x, train_loss_history)\n",
        "        plt.plot(x, test_loss_history)\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Test'], loc='lower left')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # def extract_embeddings(self, tokenized_dataset, start_idx, end_idx, pad=True):\n",
        "    #     \"\"\" Extract raw embeddings for [start_idx, end_idx) of tokenized_dataset from language_model \n",
        "            \n",
        "    #     Returns:\n",
        "    #         extract_embeddings: DataFrame with cols (span1, span2?, label) and span shape is (range_len, (#layers, span_len, embedding_dim))\n",
        "    #     \"\"\"\n",
        "    #     num_of_spans = self.dataset_handler.dataset_info.num_of_spans\n",
        "        \n",
        "    #     if num_of_spans == 2:\n",
        "    #         max_span_len_in_batch = max(max(tokenized_dataset[start_idx:end_idx][\"span1_len\"]), max(tokenized_dataset[start_idx:end_idx][\"span2_len\"]))\n",
        "    #     elif num_of_spans == 1:\n",
        "    #         max_span_len_in_batch = max(tokenized_dataset[start_idx:end_idx][\"span1_len\"])\n",
        "    #     # print(\"max_span_len_in_batch\", max_span_len_in_batch)\n",
        "        \n",
        "\n",
        "    #     span_repr = self.init_span_dict(num_of_spans, pad)\n",
        "    #     self.vprint(\"f1\")\n",
        "    #     for i in range(start_idx, end_idx):\n",
        "    #         hashable_input = repr(tokenized_dataset[i][\"text\"])\n",
        "\n",
        "    #         if hashable_input in self.cached_embeddings:\n",
        "    #             self.current_hidden_states = self.cached_embeddings[hashable_input]\n",
        "    #         else:\n",
        "    #             if hashable_input not in self.extracted_batch_embeddings:\n",
        "    #                 self.extracted_batch_embeddings = self.extract_batch(tokenized_dataset, i)\n",
        "    #                 if len(self.cached_embeddings) < CACHE_LEN:\n",
        "    #                     for key, value in self.extracted_batch_embeddings.items():\n",
        "    #                         self.cached_embeddings[key] = value\n",
        "    #                     print(f\"Cached {len(self.cached_embeddings)}\")\n",
        "    #             self.current_hidden_states = self.extracted_batch_embeddings[hashable_input]\n",
        "            \n",
        "    #         row = tokenized_dataset[i]\n",
        "    #         span1_hidden_states = self.current_hidden_states[:, row[\"span1\"][0]:row[\"span1\"][1], :]  # (#layer, span_len, embd_dim)\n",
        "    #         if pad:\n",
        "    #             s1, a1 = self.pad_span(span1_hidden_states, max_span_len_in_batch)\n",
        "    #             span_repr[\"span1\"].append(s1)\n",
        "    #             span_repr[\"span1_attention_mask\"].append(a1)\n",
        "    #         else:\n",
        "    #             span_repr[\"span1\"].append(span1_hidden_states)\n",
        "    #         if num_of_spans == 2:\n",
        "    #             span2_hidden_states = self.current_hidden_states[:, row[\"span2\"][0]:row[\"span2\"][1], :]\n",
        "    #             if pad:\n",
        "    #                 s2, a2 = self.pad_span(span2_hidden_states, max_span_len_in_batch)\n",
        "    #                 span_repr[\"span2\"].append(s2)\n",
        "    #                 span_repr[\"span2_attention_mask\"].append(a2)\n",
        "    #             else:\n",
        "    #                 span_repr[\"span2\"].append(span2_hidden_states)\n",
        "    #         span_repr[\"one_hot_label\"].append(row[\"one_hot_label\"])\n",
        "    #         span_repr[\"label\"].append(row[\"label\"])\n",
        "    #     self.vprint(\"f2\")\n",
        "    #     return span_repr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWXrJt5njyzk"
      },
      "source": [
        "my_mdl_probe_trainer = None\n",
        "gpu_cache = {}\n",
        "ram_cache = {}\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll1VZvl7bXkY"
      },
      "source": [
        "my_mdl_probe_trainer = MDL_probe_trainer(model,\n",
        "                                           my_dataset_handler, \n",
        "                                           device=DEVICE,\n",
        "                                           pool_method=POOL_METHOD,\n",
        "                                           normalize_layers=False,\n",
        "                                           verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD0ohseI6Az2"
      },
      "source": [
        "print(\"Model:\", model_checkpoint)\n",
        "print(\"Dataset:\", my_dataset_info.dataset_name)\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "my_mdl_probe_trainer.edge_probe_models[0].summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLGE0ZxU6Az4"
      },
      "source": [
        "my_mdl_probe_trainer.train(batch_size = BATCH_SIZE, epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrZYXmIVpNfd"
      },
      "source": [
        "! zip -r mdl.zip mdl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAxnx-zK6Az4"
      },
      "source": [
        "history = my_mdl_probe_trainer.history\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh01Wdi25IUE"
      },
      "source": [
        "import re\n",
        "simpledec = re.compile(r\"\\d*\\.\\d+\")\n",
        "def mround(match):\n",
        "    return \"{:.6f}\".format(float(match.group()))\n",
        "\n",
        "print(re.sub(simpledec, mround, str(history)).replace(\"\\n\", \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xr-q7Xpydjb"
      },
      "source": [
        "print(\"MDL Loss History\")\n",
        "plt.figure(figsize=(23, 14))\n",
        "for portion in range(len(history)):\n",
        "    plt.subplot(3, 4, portion+1)\n",
        "    np_mdl = np.array(history[portion][\"loss\"][\"mdl\"])\n",
        "    # print(portion, np_mdl.shape)\n",
        "    layers = len(np_mdl[0])\n",
        "    x = range(len(np_mdl))\n",
        "    for i in range(layers):\n",
        "        plt.plot(x, np_mdl[:, i])\n",
        "    plt.title(f\"Portion {portion}\")\n",
        "    plt.ylabel('MDL Loss')\n",
        "    plt.xlabel('Epoch');\n",
        "    plt.legend(range(layers), loc='lower left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU5hK_i_32oL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT6GHy0Kf8Ku"
      },
      "source": [
        "# Diagnostic Probe Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrlQj658BoKR"
      },
      "source": [
        "class Diagnostic_probe_trainer:\n",
        "    # Public:\n",
        "    def __init__(self, language_model, dataset_handler: Dataset_handler, \n",
        "                 verbose=True, device='cuda',\n",
        "                 pool_method=\"max\", start_eval = False, normalize_layers=False):\n",
        "        self.dataset_handler = dataset_handler\n",
        "        self.num_of_spans = self.dataset_handler.dataset_info.num_of_spans\n",
        "        self.language_model = language_model\n",
        "        self.language_model.config.output_hidden_states = True\n",
        "        self.device = device\n",
        "        self.verbose = verbose\n",
        "        self.start_eval = start_eval\n",
        "        def vprint(text):\n",
        "            if verbose:\n",
        "                print(datetime.datetime.now().time(), text)\n",
        "        self.vprint = vprint\n",
        "\n",
        "        self.current_hidden_states = None\n",
        "        self.last_input_ids = None\n",
        "        self.extracted_batch_embeddings = {}\n",
        "\n",
        "        self.vprint(\"Moving to device\")\n",
        "        for param in self.language_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.language_model.eval()\n",
        "        self.language_model.to(self.device)\n",
        "        num_layers, input_span_len, embedding_dim, num_classes = self.get_language_model_properties()\n",
        "        print(num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.MLP_device = self.device\n",
        "        \n",
        "        print(\"Creating New EPM\")\n",
        "        self.edge_probe_models = []\n",
        "        for i in range(num_layers):\n",
        "            edge_probe_model = Edge_probe_model(\n",
        "                num_of_spans = self.num_of_spans,\n",
        "                num_layers = 1,\n",
        "                input_span_len = input_span_len,\n",
        "                embedding_dim = embedding_dim, \n",
        "                num_classes = num_classes,\n",
        "                device = self.MLP_device,\n",
        "                pool_method = pool_method,\n",
        "                normalize_layers = normalize_layers\n",
        "            )\n",
        "            self.edge_probe_models.append(edge_probe_model)\n",
        "        \n",
        "        self.history = {\"loss\": {\"train\": [], \"dev\": [], \"test\": []}, \n",
        "                        \"metrics\": \n",
        "                        {\"micro_f1\": {\"dev\": [], \"test\": []}},\n",
        "                        \"layers_weights\": []\n",
        "                        }\n",
        "        print(\"Creating New History\")\n",
        "\n",
        "    def train(self, batch_size, epochs=3):\n",
        "        tokenized_dataset = self.dataset_handler.tokenized_dataset[\"train\"]\n",
        "        tokenized_dataset_dev = self.dataset_handler.tokenized_dataset[\"dev\"]\n",
        "        tokenized_dataset_test = self.dataset_handler.tokenized_dataset[\"test\"]\n",
        "\n",
        "        # self.edge_probe_model.to(self.device)\n",
        "        for edge_probe_model in self.edge_probe_models:\n",
        "            edge_probe_model.to(self.MLP_device)\n",
        "        # self.vprint(\"Counting dataset rows\")\n",
        "        dataset_len = len(tokenized_dataset)\n",
        "        dev_dataset_len = len(tokenized_dataset_dev)\n",
        "        test_dataset_len = len(tokenized_dataset_test)\n",
        "        print(f\"Train on {dataset_len} samples, validate on {dev_dataset_len} samples, test on {test_dataset_len} samples\")\n",
        "        # dataset_len = 60\n",
        "        if self.start_eval:\n",
        "            self.update_history(epoch = 0)\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            steps = 0\n",
        "            self.draw_weights(epoch)\n",
        "            print(\"----------------\\n\")\n",
        "            for edge_probe_model in self.edge_probe_models:\n",
        "                edge_probe_model.train()\n",
        "            for i in tqdm(range(0, dataset_len, batch_size), desc=f\"[Epoch {epoch + 1}/{epochs}]\"):\n",
        "                # if int(i / batch_size) % 1000 == 0:\n",
        "                #     print(\"memory:\", psutil.virtual_memory().percent)\n",
        "                self.vprint(\"Start\")\n",
        "                step = batch_size\n",
        "                if i + batch_size > dataset_len:\n",
        "                    step = dataset_len - i\n",
        "                # print(f\"WWW[{i}, {i+step})\")\n",
        "                \n",
        "                self.vprint(\"Extracting\")\n",
        "                # self.vprint(\"prepare\")\n",
        "                spans_torch_dict = self.prepare_batch_data(tokenized_dataset, i, i + step, pad=True)\n",
        "                # print(spans_torch_dict[\"span1\"].shape, spans_torch_dict[\"span1_attention_mask\"].shape)\n",
        "                labels = spans_torch_dict[\"one_hot_labels\"]\n",
        "                labels = labels.float().to(self.device)\n",
        "                \n",
        "                for epm_idx, edge_probe_model in enumerate(self.edge_probe_models):\n",
        "                    # zero the parameter gradients\n",
        "                    # for param_tensor in edge_probe_model.state_dict():\n",
        "                    #     print(epm_idx, param_tensor, \"\\t\", edge_probe_model.state_dict()[param_tensor].size(), torch.norm(edge_probe_model.state_dict()[param_tensor]))\n",
        "                    # print(epm_idx, edge_probe_model.state_dict()[\"label_net.4.bias\"])\n",
        "                    edge_probe_model.optimizer.zero_grad()\n",
        "        \n",
        "                    self.vprint(\"dict\")\n",
        "                    # print(spans_torch_dict[\"span1\"].shape) # torch.Size([32, 13, 9, 768])\n",
        "                    if self.num_of_spans == 2:\n",
        "                        span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :], \n",
        "                                           \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"],\n",
        "                                           \"span2\": spans_torch_dict[\"span2\"][:, epm_idx:epm_idx+1, :, :],\n",
        "                                           \"span2_attention_mask\": spans_torch_dict[\"span2_attention_mask\"],\n",
        "                                           }\n",
        "                    else:\n",
        "                        span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :], \n",
        "                                           \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"]}\n",
        "                    \n",
        "                    # forward + backward + optimize\n",
        "                    self.vprint(\"Forward MLP\")\n",
        "                    outputs = edge_probe_model(span_torch_dict)\n",
        "                    self.vprint(\"Loss\")\n",
        "                    loss = edge_probe_model.training_criterion(outputs.to(self.device), labels)\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(edge_probe_model.parameters(), 5.0)\n",
        "                    edge_probe_model.optimizer.step()\n",
        "        \n",
        "                    running_loss += loss.item()\n",
        "                    steps += 1\n",
        "                self.vprint(\"Done\")\n",
        "                # print(f\"loss: {running_loss / steps}\")\n",
        "\n",
        "            self.update_history(epoch + 1, train_loss = running_loss / steps)\n",
        "            \n",
        "\n",
        "    def calc_loss(self, tokenized_dataset, batch_size=16, print_metrics=False, just_micro=False, desc=\"\", print_preds=False):\n",
        "        for edge_probe_model in self.edge_probe_models:\n",
        "            edge_probe_model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0\n",
        "            dataset_len = len(tokenized_dataset[\"input_ids\"])\n",
        "            steps = 0\n",
        "            preds = [None] * self.num_layers\n",
        "            micro_f1 = [None] * self.num_layers\n",
        "            for i in tqdm(range(0, dataset_len, batch_size), desc=desc):\n",
        "                # if int(i / batch_size) % 100 == 0:\n",
        "                #     print(\"memory:\", psutil.virtual_memory().percent, gc.collect(), psutil.virtual_memory().percent)\n",
        "                step = batch_size\n",
        "                if i + batch_size > dataset_len:\n",
        "                    step = dataset_len - i\n",
        "\n",
        "                spans_torch_dict = self.prepare_batch_data(tokenized_dataset, i, i + step, pad=True)\n",
        "                labels = spans_torch_dict[\"one_hot_labels\"]\n",
        "                labels = labels.float().to(self.device)\n",
        "\n",
        "                for epm_idx, edge_probe_model in enumerate(self.edge_probe_models):\n",
        "                    if self.num_of_spans == 2:\n",
        "                        span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :], \n",
        "                                           \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"],\n",
        "                                           \"span2\": spans_torch_dict[\"span2\"][:, epm_idx:epm_idx+1, :, :],\n",
        "                                           \"span2_attention_mask\": spans_torch_dict[\"span2_attention_mask\"],\n",
        "                                           }\n",
        "                    else:\n",
        "                        span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :], \n",
        "                                           \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"]}\n",
        "\n",
        "                    # forward\n",
        "                    outputs = edge_probe_model(span_torch_dict)\n",
        "                    \n",
        "                    preds[epm_idx] = outputs if i == 0 else torch.cat((preds[epm_idx], outputs), 0)\n",
        "                    loss = edge_probe_model.training_criterion(outputs.to(self.device), labels)\n",
        "                    running_loss += loss.item()\n",
        "                    steps += 1\n",
        "\n",
        "        if print_preds:\n",
        "            for epm_idx, edge_probe_model in enumerate(self.edge_probe_models):\n",
        "                print(preds[epm_idx][0])\n",
        "\n",
        "        y_true = np.array(tokenized_dataset[\"one_hot_label\"]).argmax(-1)\n",
        "        for idx, pred in enumerate(preds): \n",
        "            pred = pred.cpu().argmax(-1)\n",
        "            micro_f1[idx] = f1_score(y_true, pred, average='micro')\n",
        "        \n",
        "        if print_metrics:\n",
        "            # labels_list = self.dataset_handler.labels_list\n",
        "            # if not just_micro:\n",
        "            #     print(classification_report(y_true, preds, target_names=labels_list, labels=range(len(labels_list))))\n",
        "            print(\"MICRO F1:\", micro_f1)\n",
        "        return running_loss / steps, micro_f1\n",
        "\n",
        "    # Private:\n",
        "    def update_history(self, epoch, train_loss = None):\n",
        "        if train_loss is None:\n",
        "            train_loss, train_f1 = self.calc_loss(self.dataset_handler.tokenized_dataset[\"train\"], print_metrics=True, desc=\"Train Loss\")\n",
        "        dev_loss, dev_f1 = self.calc_loss(self.dataset_handler.tokenized_dataset[\"dev\"], print_metrics=True, desc=\"Dev Loss\")\n",
        "        test_loss, test_f1 = self.calc_loss(self.dataset_handler.tokenized_dataset[\"test\"], print_metrics=True, desc=\"Test Loss\")\n",
        "        self.history[\"loss\"][\"train\"].append(train_loss)\n",
        "        self.history[\"loss\"][\"dev\"].append(dev_loss)\n",
        "        self.history[\"loss\"][\"test\"].append(test_loss)\n",
        "        self.history[\"metrics\"][\"micro_f1\"][\"dev\"].append(dev_f1)\n",
        "        self.history[\"metrics\"][\"micro_f1\"][\"test\"].append(test_f1)\n",
        "        # self.history[\"layers_weights\"].append(self.edge_probe_model.weighing_params.tolist())\n",
        "        print('[%d] loss:' % (epoch))\n",
        "        print(\"Train Loss:\", self.history[\"loss\"][\"train\"][-1])\n",
        "        print(\"Dev Loss:\", self.history[\"loss\"][\"dev\"][-1])\n",
        "        print(\"Test Loss:\", self.history[\"loss\"][\"test\"][-1])\n",
        "        # print('[%d] loss: %.4f, val_loss: %.4f, test_loss: %.4f' % (epoch, self.history[\"loss\"][\"train\"][-1], self.history[\"loss\"][\"dev\"][-1], self.history[\"loss\"][\"test\"][-1]))\n",
        "\n",
        "    def draw_weights(self, epoch=0):\n",
        "        if(epoch > 0):\n",
        "            # w = self.edge_probe_models.weighing_params.tolist()\n",
        "            # print(w)\n",
        "            w = self.history[\"metrics\"][\"micro_f1\"][\"test\"][-1]\n",
        "            print(self.history)\n",
        "            plt.bar(np.arange(len(w), dtype=int), w)\n",
        "            plt.ylabel('f1')\n",
        "            plt.xlabel('Layer');\n",
        "            plt.show()\n",
        "\n",
        "            # wsoft = nn.functional.softmax(self.edge_probe_model.weighing_params)\n",
        "            # print(\"CG\", sum(idx*val for idx, val in enumerate(wsoft)))\n",
        "\n",
        "            print(\"Loss History\")\n",
        "            loss_history = self.history[\"loss\"]\n",
        "            x = range(len(loss_history[\"train\"]))\n",
        "            plt.plot(x, loss_history[\"train\"])\n",
        "            plt.plot(x, loss_history[\"dev\"])\n",
        "            plt.plot(x, loss_history[\"test\"])\n",
        "            plt.legend(['Train', 'Dev', 'Test'], loc='lower left')\n",
        "            plt.show()\n",
        "\n",
        "            # print(\"Micro f1 History\")\n",
        "            # f1_history = self.history[\"metrics\"][\"micro_f1\"]\n",
        "            # x = range(len(f1_history[\"dev\"]))\n",
        "            # plt.plot(x, f1_history[\"dev\"])\n",
        "            # plt.plot(x, f1_history[\"test\"])\n",
        "            # plt.legend(['Dev', 'Test'], loc='upper left')\n",
        "            # plt.show()\n",
        "\n",
        "    def prepare_batch_data(self, tokenized_dataset, start_idx, end_idx, pad=False):\n",
        "        # self.vprint(\"Extracting From Model\")\n",
        "        span_representations_dict = self.extract_embeddings(tokenized_dataset, start_idx, end_idx, pad=True)\n",
        "        # self.vprint(\"To Device\")\n",
        "        span1_torch = torch.stack(span_representations_dict[\"span1\"]).float().to(self.MLP_device)  # (batch_size, #layers, max_span_len, embd_dim)\n",
        "        span1_attention_mask_torch = torch.stack(span_representations_dict[\"span1_attention_mask\"])\n",
        "        one_hot_labels_torch = torch.tensor(np.array(span_representations_dict[\"one_hot_label\"]))\n",
        "        if self.num_of_spans == 2:\n",
        "            span2_torch = torch.stack(span_representations_dict[\"span2\"]).float().to(self.MLP_device)\n",
        "            span2_attention_mask_torch = torch.stack(span_representations_dict[\"span1_attention_mask\"])\n",
        "            spans_torch_dict = {\"span1\": span1_torch, \n",
        "                                \"span2\": span2_torch, \n",
        "                                \"span1_attention_mask\": span1_attention_mask_torch, \n",
        "                                \"span2_attention_mask\": span2_attention_mask_torch, \n",
        "                                \"one_hot_labels\": one_hot_labels_torch}\n",
        "        elif self.num_of_spans == 1:\n",
        "            spans_torch_dict = {\"span1\": span1_torch, \n",
        "                                \"span1_attention_mask\": span1_attention_mask_torch, \n",
        "                                \"one_hot_labels\": one_hot_labels_torch}\n",
        "\n",
        "        return spans_torch_dict\n",
        "\n",
        "    def get_language_model_properties(self):\n",
        "        span_representations_dict = self.extract_embeddings(self.dataset_handler.tokenized_dataset[\"train\"], 0, 3, pad=True)\n",
        "        for i in span_representations_dict[\"span1\"]:\n",
        "            print(i.shape)\n",
        "        span1_torch = span_representations_dict[\"span1\"]\n",
        "        num_layers = span1_torch[0].shape[0]\n",
        "        span_len = span1_torch[0].shape[1]\n",
        "        embedding_dim = span1_torch[0].shape[2]\n",
        "        # if self.verbose:\n",
        "        #     display(pd.DataFrame(span_representations_dict))\n",
        "        return num_layers, span_len, embedding_dim, len(self.dataset_handler.labels_list)\n",
        "\n",
        "    def pad_span(self, span_repr, max_len):\n",
        "        \"\"\" pad spans in embeddings to max_len \n",
        "        input:\n",
        "            span_representation: df with shape (#layers, span_len, embedding_dim)\n",
        "        returns:\n",
        "            padded_spans: np with shape (batch_len, num_layers, max_len, embedding_dim)\n",
        "            attention_mask: np with shape (max_len), values = 1: data, 0: padding\n",
        "        \"\"\"\n",
        "        shape = span_repr.shape\n",
        "        num_layers = shape[0]\n",
        "        span_original_len = shape[1]\n",
        "        embedding_dim = shape[2]\n",
        "        # padded_span_repr = np.zeros((num_layers, max_len, embedding_dim))\n",
        "        # if span_original_len > max_len:\n",
        "        #     raise Exception(f\"Error: {span_original_len} is more than max_span_len {max_len}\\n{span_repr.shape}\")\n",
        "        attention_mask = torch.tensor(np.array([1] * span_original_len + [0] * (max_len - span_original_len)), dtype=torch.int8, device=self.device)\n",
        "        padded_span_repr = torch.cat((span_repr, torch.zeros((num_layers, max_len - span_original_len, embedding_dim), device=self.device)), axis=1)\n",
        "        # assert attention_mask.shape == (max_len, ), f\"{attention_mask}, {attention_mask.shape} != ({max_len}, )\"\n",
        "        # assert padded_span_repr.shape == (num_layers, max_len, embedding_dim)\n",
        "        return padded_span_repr, attention_mask\n",
        "\n",
        "    def init_span_dict(self, num_of_spans, pad):\n",
        "        if num_of_spans == 2:\n",
        "            span_repr = {\"span1\": [], \"span2\": [], \"label\": [], \"one_hot_label\": []}\n",
        "        else:\n",
        "            span_repr = {\"span1\": [], \"label\": [], \"one_hot_label\": []}\n",
        "        \n",
        "        if pad:\n",
        "            span_repr[\"span1_attention_mask\"] = []\n",
        "            span_repr[\"span2_attention_mask\"] = []\n",
        "        return span_repr\n",
        "\n",
        "    def extract_batch(self, tokenized_dataset, idx, unique_batch_size=32):\n",
        "        # print(idx)\n",
        "        self.vprint(\"e1\")\n",
        "        dataset_len = len(tokenized_dataset)\n",
        "        unique_texts_in_batch = []\n",
        "        i = idx\n",
        "        while len(unique_texts_in_batch) < unique_batch_size and i < dataset_len:\n",
        "            # print(i)\n",
        "            text = tokenized_dataset[i][\"text\"]\n",
        "            if not text in unique_texts_in_batch:\n",
        "                unique_texts_in_batch.append(text)\n",
        "            i += 1\n",
        "        tokenizer.padding_side = 'right'  # Important: lef will change the span indices\n",
        "        tokenized_batch = tokenizer(unique_texts_in_batch, padding=True, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.language_model(**tokenized_batch)\n",
        "        # torch.cuda.synchronize()\n",
        "        # current_hidden_states = np.asarray([val.detach().cpu().numpy() for val in outputs.hidden_states])\n",
        "        current_hidden_states = torch.stack([val.detach() for val in outputs.hidden_states])  # TODO: use only stack, no list \n",
        "        # self.vprint(current_hidden_states.shape)  # (13, 16, 34, 768)\n",
        "        \n",
        "        extracted_batch_embeddings = {}\n",
        "        for i, unique_text in enumerate(unique_texts_in_batch):\n",
        "            hashable_input = repr(unique_text)\n",
        "            extracted_batch_embeddings[hashable_input] = current_hidden_states[:, i, :, :]\n",
        "        self.vprint(\"e2\")\n",
        "        return extracted_batch_embeddings\n",
        "    \n",
        "    def pad_sequence(list_of_torch, pad_len, pad_value=0):\n",
        "        shape = list_of_torch[0].shape\n",
        "        num_layers = shape[0]\n",
        "        span_original_len = shape[1]\n",
        "        embedding_dim = shape[2]\n",
        "        output = torch.zeros()\n",
        "\n",
        "    def extract_embeddings(self, tokenized_dataset, start_idx, end_idx, pad=True):\n",
        "        \"\"\" Extract raw embeddings for [start_idx, end_idx) of tokenized_dataset from language_model \n",
        "            \n",
        "        Returns:\n",
        "            extract_embeddings: DataFrame with cols (span1, span2?, label) and span shape is (range_len, (#layers, span_len, embedding_dim))\n",
        "        \"\"\"\n",
        "        num_of_spans = self.dataset_handler.dataset_info.num_of_spans\n",
        "        \n",
        "        if num_of_spans == 2:\n",
        "            max_span_len_in_batch = max(max(tokenized_dataset[start_idx:end_idx][\"span1_len\"]), max(tokenized_dataset[start_idx:end_idx][\"span2_len\"]))\n",
        "        elif num_of_spans == 1:\n",
        "            max_span_len_in_batch = max(tokenized_dataset[start_idx:end_idx][\"span1_len\"])\n",
        "        # print(\"max_span_len_in_batch\", max_span_len_in_batch)\n",
        "        \n",
        "\n",
        "        span_repr = self.init_span_dict(num_of_spans, pad)\n",
        "        self.vprint(\"f1\")\n",
        "        for i in range(start_idx, end_idx):\n",
        "            hashable_input = repr(tokenized_dataset[i][\"text\"])\n",
        "            \n",
        "            self.vprint(\"f2\")\n",
        "            if hashable_input not in self.extracted_batch_embeddings:\n",
        "                self.extracted_batch_embeddings = self.extract_batch(tokenized_dataset, i)\n",
        "            self.vprint(\"f3\")  \n",
        "            self.current_hidden_states = self.extracted_batch_embeddings[hashable_input]\n",
        "            \n",
        "            row = tokenized_dataset[i]\n",
        "            span1_hidden_states = self.current_hidden_states[:, row[\"span1\"][0]:row[\"span1\"][1], :]  # (#layer, span_len, embd_dim)\n",
        "            if pad:\n",
        "                s1, a1 = self.pad_span(span1_hidden_states, max_span_len_in_batch)\n",
        "                span_repr[\"span1\"].append(s1)\n",
        "                span_repr[\"span1_attention_mask\"].append(a1)\n",
        "            else:\n",
        "                span_repr[\"span1\"].append(span1_hidden_states)\n",
        "            if num_of_spans == 2:\n",
        "                span2_hidden_states = self.current_hidden_states[:, row[\"span2\"][0]:row[\"span2\"][1], :]\n",
        "                if pad:\n",
        "                    s2, a2 = self.pad_span(span2_hidden_states, max_span_len_in_batch)\n",
        "                    span_repr[\"span2\"].append(s2)\n",
        "                    span_repr[\"span2_attention_mask\"].append(a2)\n",
        "                else:\n",
        "                    span_repr[\"span2\"].append(span2_hidden_states)\n",
        "            span_repr[\"one_hot_label\"].append(row[\"one_hot_label\"])\n",
        "            span_repr[\"label\"].append(row[\"label\"])\n",
        "        self.vprint(\"f4\")\n",
        "        return span_repr\n",
        "\n",
        "    def analyze_layers(self, dataset_part=\"train\", row_idx=0, new_text = None):\n",
        "        if new_text is not None:\n",
        "            new_dataset_info = Dataset_info(\"manual\", num_of_spans=1, manual_text=new_text)\n",
        "            new_dataset_handler = Dataset_handler(new_dataset_info);\n",
        "            dataset_part = \"test\"\n",
        "            row_idx=0\n",
        "            tokenized_dataset = new_dataset_handler.tokenized_dataset[dataset_part]\n",
        "        else:\n",
        "            tokenized_dataset = self.dataset_handler.tokenized_dataset[dataset_part]\n",
        "        \n",
        "        self.calc_loss(tokenized_dataset, print_metrics=True, desc=\"Manual Test Loss\", print_preds=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fjbKKcagPM3"
      },
      "source": [
        "my_diagnostic_probe_trainer = Diagnostic_probe_trainer(model,\n",
        "                                           my_dataset_handler, \n",
        "                                           device=DEVICE,\n",
        "                                           pool_method=POOL_METHOD,\n",
        "                                           normalize_layers=False,\n",
        "                                           verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg40gSSfzuAC"
      },
      "source": [
        "print(\"Model:\", model_checkpoint)\n",
        "print(\"Dataset:\", my_dataset_info.dataset_name)\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "my_diagnostic_probe_trainer.edge_probe_models[0].summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXoDIKvsigKw"
      },
      "source": [
        "my_diagnostic_probe_trainer.train(batch_size = BATCH_SIZE, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSBeN4XS5DI3"
      },
      "source": [
        "history = my_diagnostic_probe_trainer.history\n",
        "print(my_diagnostic_probe_trainer.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk0GqsF2Legu"
      },
      "source": [
        "my_diagnostic_probe_trainer.analyze_layers(new_text=\"trump has offered cnn an outsourcing waiver\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnWHI6Vd5Lz6"
      },
      "source": [
        "print(\"Loss History\")\n",
        "loss_history = my_diagnostic_probe_trainer.history[\"loss\"]\n",
        "print(loss_history)\n",
        "print(\"Train Loss:\", loss_history[\"train\"])\n",
        "print(\"Dev Loss:\", loss_history[\"dev\"])\n",
        "print(\"Test Loss:\", loss_history[\"test\"])\n",
        "\n",
        "x = range(len(loss_history[\"train\"]))\n",
        "plt.plot(x, loss_history[\"train\"])\n",
        "plt.plot(x, loss_history[\"dev\"])\n",
        "plt.plot(x, loss_history[\"test\"])\n",
        "plt.legend(['Train', 'Dev', 'Test'], loc='lower left')\n",
        "plt.show()\n",
        "print(\".\")\n",
        "\n",
        "print(\"Micro f1 History\")\n",
        "f1_history = my_diagnostic_probe_trainer.history[\"metrics\"][\"micro_f1\"]\n",
        "print(f1_history)\n",
        "print(\"Dev f1:\", f1_history[\"dev\"])\n",
        "print(\"Test f1:\", f1_history[\"test\"])\n",
        "\n",
        "\n",
        "\n",
        "x = range(len(f1_history[\"dev\"][-1]))\n",
        "plt.plot(x, f1_history[\"dev\"][-1])\n",
        "plt.plot(x, f1_history[\"test\"][-1])\n",
        "plt.legend(['Dev', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "print(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}