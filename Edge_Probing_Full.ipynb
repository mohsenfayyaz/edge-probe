{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Edge_Probing_Full.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r5t_gYLZLT9S"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenfayyaz/edge-probe/blob/main/Edge_Probing_Full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUqdV6uy7PZJ"
      },
      "source": [
        "# Installations & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljSyeFv7JTtf"
      },
      "source": [
        "! nproc\n",
        "! lscpu\n",
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6oStMxWkSQx"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from google.colab import auth\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# auth.authenticate_user()\n",
        "! gdown --id ***\n",
        "! tar -xzf ontonotes_data.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C9hC77g65Je"
      },
      "source": [
        "! git clone https://github.com/mohsenfayyaz/edge-probing-datasets.git\n",
        "! pip install datasets\n",
        "! pip install transformers\n",
        "! pip install sentencepiece\n",
        "# ! pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTbdjm8I7mka"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import torch\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os\n",
        "import datasets\n",
        "import json\n",
        "import gc\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from abc import ABC, abstractmethod\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import f1_score\n",
        "import psutil  # RAM usage\n",
        "# import wandb\n",
        "# wandb.init()\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41xul5KY7xvt"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ3e1TgD9fbe"
      },
      "source": [
        "class Dataset_info:\n",
        "    def __init__(self, dataset_name, num_of_spans, max_span_length=5, ignore_classes=[]):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.num_of_spans = num_of_spans\n",
        "        self.ignore_classes = ignore_classes  # ignore other class in rel (semeval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x1M223x7zcD"
      },
      "source": [
        "# model_checkpoint = 'xlnet-base-cased'\n",
        "# model_checkpoint = \"xlnet-large-cased\"\n",
        "# model_checkpoint = \"distilbert-base-cased\"\n",
        "# model_checkpoint = \"bert-base-cased\"\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "# model_checkpoint = \"bert-large-uncased\"\n",
        "# model_checkpoint = \"albert-base-v2\"\n",
        "# model_checkpoint = \"albert-large-v2\"\n",
        "# model_checkpoint = \"albert-xxlarge-v2\"\n",
        "# model_checkpoint = \"t5-small\"\n",
        "# model_checkpoint = \"t5-large\"\n",
        "# model_checkpoint = \"roberta-large\"\n",
        "# model_checkpoint = \"google/electra-large-discriminator\"\n",
        "\n",
        "\n",
        "# model_checkpoint = \"mohsenfayyaz/toxicity-classifier\"\n",
        "# model_checkpoint = \"mohsenfayyaz/bert-base-uncased-toxicity\"\n",
        "# model_checkpoint = \"textattack/xlnet-base-cased-SST-2\"\n",
        "\n",
        "# model_checkpoint = \"mrm8488/albert-base-v2-finetuned-mnli-pabee\"\n",
        "\n",
        "\n",
        "my_dataset_info = Dataset_info(\"ud\", num_of_spans=2)  # Dependency Labeling\n",
        "# my_dataset_info = Dataset_info(\"ner\", num_of_spans=1)  # Named Entity Labeling\n",
        "# my_dataset_info = Dataset_info(\"srl\", num_of_spans=2)  # Semantic Role Labeling\n",
        "# my_dataset_info = Dataset_info(\"coref\", num_of_spans=2)  # Coreference Ontonotes\n",
        "# my_dataset_info = Dataset_info(\"dpr\", num_of_spans=2)  # Coreference Winograd\n",
        "# my_dataset_info = Dataset_info(\"semeval\", num_of_spans=2, ignore_classes=[\"Other\"])  # Relation Classification\n",
        "\n",
        "\n",
        "\n",
        "POOL_METHOD = \"attn\"  # 'max', 'attn'\n",
        "BATCH_SIZE = 32\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRAmX2Cl8XHh"
      },
      "source": [
        "# Prepare Dataset & Spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxwmMoFV8YwW"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
        "\n",
        "model = AutoModel.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMnmCWdW12cK"
      },
      "source": [
        "# model.save_pretrained(model_checkpoint)\n",
        "# tokenizer.save_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxlLr5yvkLiK"
      },
      "source": [
        "class Utils:\n",
        "    def one_hot(idx, length):\n",
        "        import numpy as np\n",
        "        o = np.zeros(length, dtype=np.int8)\n",
        "        o[idx] = 1\n",
        "        return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJHdbDyeCZOJ"
      },
      "source": [
        "class Dataset_handler:\n",
        "    def __init__(self, dataset_info: Dataset_info):\n",
        "        self.dataset = datasets.DatasetDict()\n",
        "        self.tokenized_dataset = None\n",
        "        self.dataset_info = dataset_info\n",
        "        self.labels_list = None\n",
        "\n",
        "        if dataset_info.dataset_name == \"dpr\":\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/dpr_data/train.json', data_type=\"train\")\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/dpr_data/dev.json', data_type=\"dev\")\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/dpr_data/test.json', data_type=\"test\")\n",
        "        elif dataset_info.dataset_name == \"ud\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/ud_data/en_ewt-ud-train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/ud_data/en_ewt-ud-dev.json', data_type=\"dev\", fraction = frac)\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/ud_data/en_ewt-ud-test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"semeval\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/semeval_data/train.all.json', data_type=\"train\", fraction = frac, ignore_classes = self.dataset_info.ignore_classes)\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/semeval_data/test.json', data_type=\"dev\", fraction = 0.01, ignore_classes = self.dataset_info.ignore_classes)\n",
        "            self.json_to_dataset('./edge-probing-datasets/data/semeval_data/test.json', data_type=\"test\", fraction = frac, ignore_classes = self.dataset_info.ignore_classes)\n",
        "        elif dataset_info.dataset_name == \"srl\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./ontonotes_data/srl/train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/srl/test.json', data_type=\"dev\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/srl/conll-2012-test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"ner\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./ontonotes_data/ner/train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/ner/conll-2012-test.json', data_type=\"dev\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/ner/test.json', data_type=\"test\", fraction = frac)\n",
        "        elif dataset_info.dataset_name == \"coref\":\n",
        "            frac = 1\n",
        "            self.json_to_dataset('./ontonotes_data/coref/train.json', data_type=\"train\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/coref/development.json', data_type=\"dev\", fraction = frac)\n",
        "            self.json_to_dataset('./ontonotes_data/coref/test.json', data_type=\"test\", fraction = frac)\n",
        "        else:\n",
        "            throw(\"Error: Unkown dataset name!\")\n",
        "\n",
        "        print(\"⌛ Tokenizing Dataset and Adding One Hot Representation of Labels\")\n",
        "        self.tokenized_dataset = self.tokenize_input_and_one_hot_labels(self.dataset)\n",
        "        # self.tokenized_dataset = self.tokenize_dataset(self.dataset)\n",
        "        # print(\"⌛ Adding One Hot Representation of Labels\")\n",
        "        # self.tokenized_dataset = self.one_hot_dataset_labels(self.tokenized_dataset)\n",
        "        \n",
        "\n",
        "    # Public:\n",
        "    def json_to_dataset(self, json_path, data_type=\"train\", fraction=1, ignore_classes=[]):\n",
        "        data_df = self.json_to_df(json_path)\n",
        "        data_df = data_df[~data_df[\"label\"].isin(ignore_classes)]\n",
        "        # print(data_type, \"text max length:\", data_df[\"text\"].str.len().max())  # max length of texts\n",
        "        if fraction != 1:\n",
        "            data_df = data_df.sample(frac=fraction, random_state=1).sort_index().reset_index(drop=True)\n",
        "        self.dataset[data_type] = datasets.Dataset.from_pandas(data_df)\n",
        "        return self.dataset\n",
        "    \n",
        "    def tokenize_input_and_one_hot_labels(self, dataset):\n",
        "        train_df = pd.DataFrame(dataset[\"train\"][\"label\"], columns=['label'])\n",
        "        dev_df = pd.DataFrame(dataset[\"dev\"][\"label\"], columns=['label'])\n",
        "        test_df = pd.DataFrame(dataset[\"test\"][\"label\"], columns=['label'])\n",
        "        self.labels_list = list(set(train_df[\"label\"].unique()).union\n",
        "                               (set(dev_df[\"label\"].unique())).union\n",
        "                               (set(test_df[\"label\"].unique())))\n",
        "        self.label_to_index = dict()\n",
        "        for idx, l in enumerate(self.labels_list):\n",
        "            self.label_to_index[l] = idx\n",
        "        tokenized_one_hot_dataset = dataset.map(tokenize_and_one_hot,\n",
        "                                                fn_kwargs={\"label_to_index\": self.label_to_index,\n",
        "                                                           \"labels_len\": len(self.label_to_index),\n",
        "                                                           \"tokenizer\": tokenizer,\n",
        "                                                           \"one_hot_func\": Utils.one_hot,\n",
        "                                                           \"num_of_spans\": self.dataset_info.num_of_spans\n",
        "                                                           },\n",
        "                                                batched=False,\n",
        "                                                num_proc=None)\n",
        "        return tokenized_one_hot_dataset\n",
        "\n",
        "    # Private:\n",
        "    def json_to_df(self, json_path):\n",
        "        with open(json_path, encoding='utf-8') as file:\n",
        "            data_list = list()\n",
        "            for line in file:\n",
        "                instance = json.loads(line)\n",
        "                for target in instance[\"targets\"]:\n",
        "                    if self.dataset_info.num_of_spans == 2:\n",
        "                        data_list.append({\"text\": instance[\"text\"],\n",
        "                                        \"span1\": target[\"span1\"],\n",
        "                                        \"span2\": target[\"span2\"],\n",
        "                                        \"label\": target[\"label\"]})\n",
        "                    elif self.dataset_info.num_of_spans == 1:\n",
        "                        data_list.append({\"text\": instance[\"text\"],\n",
        "                                        \"span1\": target[\"span1\"],\n",
        "                                        \"label\": target[\"label\"]})\n",
        "        return pd.DataFrame.from_dict(data_list)\n",
        "\n",
        "def tokenize_and_one_hot(examples, **fn_kwargs):\n",
        "    # tokenize and align spans\n",
        "    thread_tokenizer = fn_kwargs[\"tokenizer\"]\n",
        "    one_hot_func = fn_kwargs[\"one_hot_func\"]\n",
        "    num_of_spans = fn_kwargs[\"num_of_spans\"]\n",
        "    tokenized_inputs = thread_tokenizer(examples[\"text\"].split(), is_split_into_words=True)  # Must be splitted for tokenizer to word_ids works fine. (test e-mail!)\n",
        "    # tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, is_split_into_words=True, padding=\"max_length\", max_length=210)\n",
        "    def align_span(word_ids, start_word_id, end_word_id):\n",
        "        span = [0, 0]\n",
        "        span[0] = word_ids.index(start_word_id)  # First occurance\n",
        "        span[1] = len(word_ids) - 1 - word_ids[::-1].index(end_word_id - 1) + 1  # Last occurance (+1 for open range)\n",
        "        return span\n",
        "\n",
        "    # tokenized_inputs[\"span1\"] = [0, 0]\n",
        "    # tokenized_inputs[\"span1\"][0] = word_ids.index(examples[\"span1\"][0])  # First occurance\n",
        "    # tokenized_inputs[\"span1\"][1] = len(word_ids) - 1 - word_ids[::-1].index(examples[\"span1\"][1] - 1) + 1  # Last occurance (+1 for open range)\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "    tokenized_inputs[\"span1\"] = align_span(word_ids, examples[\"span1\"][0], examples[\"span1\"][1])\n",
        "    tokenized_inputs[\"span1_len\"] = tokenized_inputs[\"span1\"][1] - tokenized_inputs[\"span1\"][0]\n",
        "    if num_of_spans == 2:\n",
        "        # tokenized_inputs[\"span2\"] = [0, 0]\n",
        "        # tokenized_inputs[\"span2\"][0] = word_ids.index(examples[\"span2\"][0])  # First occurance\n",
        "        # tokenized_inputs[\"span2\"][1] = len(word_ids) - 1 - word_ids[::-1].index(examples[\"span2\"][1] - 1) + 1  # Last occurance\n",
        "        tokenized_inputs[\"span2\"] = align_span(word_ids, examples[\"span2\"][0], examples[\"span2\"][1])\n",
        "        tokenized_inputs[\"span2_len\"] = tokenized_inputs[\"span2\"][1] - tokenized_inputs[\"span2\"][0]\n",
        "    # One hot\n",
        "    label_to_index = fn_kwargs[\"label_to_index\"]\n",
        "    labels_len = fn_kwargs[\"labels_len\"]\n",
        "    tokenized_inputs[\"one_hot_label\"] = one_hot_func(label_to_index[examples[\"label\"]], labels_len)\n",
        "    return tokenized_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEYV7wo1LiaL"
      },
      "source": [
        "my_dataset_handler = Dataset_handler(my_dataset_info);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmmRDDuFjd2"
      },
      "source": [
        "# Check\n",
        "rnd_idx = np.random.randint(100)\n",
        "# rnd_idx = 17\n",
        "part = \"test\"\n",
        "\n",
        "display(pd.DataFrame(my_dataset_handler.tokenized_dataset[part][0:3]))\n",
        "print(\"idx =\", rnd_idx)\n",
        "print(my_dataset_handler.tokenized_dataset)\n",
        "print(\"Original Spans:\", my_dataset_handler.dataset[part][rnd_idx])\n",
        "print(\"Tokenized Spans:\", my_dataset_handler.tokenized_dataset[part][rnd_idx])\n",
        "test_tokens = tokenizer.convert_ids_to_tokens(my_dataset_handler.tokenized_dataset[part][rnd_idx][\"input_ids\"])\n",
        "print(test_tokens)\n",
        "\n",
        "s10, s11 = my_dataset_handler.tokenized_dataset[part][rnd_idx][\"span1\"][0], my_dataset_handler.tokenized_dataset[part][rnd_idx][\"span1\"][-1]\n",
        "print(\"span1:\", s10, s11, test_tokens[s10:s11])\n",
        "if my_dataset_info.num_of_spans == 2:\n",
        "    s20, s21 = my_dataset_handler.tokenized_dataset[part][rnd_idx][\"span2\"][0], my_dataset_handler.tokenized_dataset[part][rnd_idx][\"span2\"][-1]\n",
        "    print(\"span2:\", s20, s21, test_tokens[s20:s21])\n",
        "print(\"label:\", my_dataset_handler.tokenized_dataset[part][rnd_idx][\"label\"])\n",
        "\n",
        "pd.DataFrame(my_dataset_handler.tokenized_dataset[part][\"label\"], columns=['label'])[\"label\"].value_counts().plot(kind='barh', color=\"green\", figsize=(10, 9));\n",
        "\n",
        "#  What if Google Morphed Into GoogleOS ? cc \n",
        "# span1: 11 12 ['and']\n",
        "# span2: 13 14 ['e']\n",
        "# label: cc\n",
        "# BUG? 17"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5t_gYLZLT9S"
      },
      "source": [
        "# Edge Probe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okFPTNCIkUru"
      },
      "source": [
        "class SpanRepr(ABC, nn.Module):\n",
        "    \"\"\"Abstract class describing span representation.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, use_proj=False, proj_dim=256):\n",
        "        super(SpanRepr, self).__init__()\n",
        "        self.input_dim = input_dim  # embedding dim or proj dim\n",
        "        self.proj_dim = proj_dim\n",
        "        self.use_proj = use_proj\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, spans, attention_mask):\n",
        "        \"\"\" \n",
        "        input:\n",
        "            spans: [batch_size, layers, span_max_len, proj_dim/embedding_dim] ~ [32, 13, 4, 256]\n",
        "            attention_mask: [batch_size, span_max_len] ~ [32, 4]\n",
        "        returns:\n",
        "            [32, 13, 256]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_input_dim(self):\n",
        "        return self.input_dim\n",
        "\n",
        "class MaxSpanRepr(SpanRepr, nn.Module):\n",
        "    \"\"\"Class implementing the max-pool span representation.\"\"\"\n",
        "\n",
        "    def forward(self, spans, attention_mask):\n",
        "        # TODO: Vectorize this\n",
        "        # for i in range(len(attention_mask)):\n",
        "        #     for j in range(len(attention_mask[i])):\n",
        "        #         if attention_mask[i][j] == 0:\n",
        "        #             spans[i, :, j, :] = -1e10\n",
        "\n",
        "        span_masks_shape = attention_mask.shape\n",
        "        span_masks = attention_mask.reshape(\n",
        "            span_masks_shape[0],\n",
        "            1,\n",
        "            span_masks_shape[1],\n",
        "            1\n",
        "        ).expand_as(spans)\n",
        "        attention_spans = spans * span_masks - 1e10 * (1 - span_masks)\n",
        "\n",
        "        max_span_repr, max_idxs = torch.max(attention_spans, dim=-2)\n",
        "        # print(max_span_repr.shape)\n",
        "        return max_span_repr\n",
        "\n",
        "class AttnSpanRepr(SpanRepr, nn.Module):\n",
        "    \"\"\"Class implementing the attention-based span representation.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, use_proj=False, proj_dim=256, use_endpoints=False):\n",
        "        \"\"\"If use_endpoints is true then concatenate the end points to attention-pooled span repr.\n",
        "        Otherwise just return the attention pooled term. (use_endpoints Not Implemented)\n",
        "        \"\"\"\n",
        "        super(AttnSpanRepr, self).__init__(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "        self.use_endpoints = use_endpoints\n",
        "        # input_dim is embedding_dim or proj dim\n",
        "        # print(\"input_dim\", input_dim)\n",
        "        self.attention_params = nn.Linear(input_dim, 1)  # Learn a weight for each token: z(k)i = W(k)att e(k)i\n",
        "        # Initialize weight to zero weight\n",
        "        # self.attention_params.weight.data.fill_(0)\n",
        "        # self.attention_params.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, spans, attention_mask):\n",
        "        \"\"\" \n",
        "        input:\n",
        "            spans: [batch_size, layers, span_max_len, proj_dim/embedding_dim] ~ [32, 13, 4, 256]\n",
        "            attention_mask: [batch_size, span_max_len] ~ [32, 4]\n",
        "        returns:\n",
        "            [32, 13, 256]\n",
        "        \"\"\"\n",
        "        if self.use_proj:\n",
        "            encoded_input = self.proj(encoded_input)\n",
        "\n",
        "        # span_mask = get_span_mask(start_ids, end_ids, encoded_input.shape[1])\n",
        "        # attn_mask = torch.zeros(spans.shape, device=DEVICE)\n",
        "        # print(datetime.datetime.now().time(), \"a1\")\n",
        "        # print(attention_mask.shape)\n",
        "        # for i in range(len(attention_mask)):\n",
        "        #     for j in range(len(attention_mask[i])):\n",
        "        #         if attention_mask[i][j] == 0:\n",
        "        #             attn_mask[i, :, j, :] = -1e10\n",
        "\n",
        "        span_masks_shape = attention_mask.shape\n",
        "        span_masks = attention_mask.reshape(\n",
        "            span_masks_shape[0],\n",
        "            1,\n",
        "            span_masks_shape[1],\n",
        "            1\n",
        "        ).expand_as(spans)\n",
        "        attn_mask = - 1e10 * (1 - span_masks)\n",
        "        \n",
        "        # print(datetime.datetime.now().time(), \"a2\")\n",
        "\n",
        "        # attn_mask = (1 - span_mask) * (-1e10)\n",
        "        attn_logits = self.attention_params(spans) + attn_mask  # Decreasing the attention of padded spans by -1e10\n",
        "        attention_wts = nn.functional.softmax(attn_logits, dim=-2)\n",
        "        attention_term = torch.sum(attention_wts * spans, dim=-2)\n",
        "        \n",
        "        # if self.use_endpoints:\n",
        "        #     batch_size = encoded_input.shape[0]\n",
        "        #     h_start = encoded_input[torch.arange(batch_size), start_ids, :]\n",
        "        #     h_end = encoded_input[torch.arange(batch_size), end_ids, :]\n",
        "        #     return torch.cat([h_start, h_end, attention_term], dim=1)\n",
        "        # else:\n",
        "        #     return attention_term\n",
        "\n",
        "        # print(spans.shape, attn_mask.shape)\n",
        "        # print(\"attn_mask\", attn_mask.shape)\n",
        "        # print(attn_mask[sidx, :, :, 0:2])\n",
        "        # print(\"attn_logits\", attn_logits.shape)\n",
        "        # print(attn_logits[sidx])\n",
        "        # print(\"attention_wts\", attention_wts.shape)\n",
        "        # print(attention_wts[sidx, :, :, 0:2])\n",
        "        # print(\"attention_term\", attention_term.shape)\n",
        "        # print(attention_term[sidx, :, 0:2])\n",
        "        return attention_term.float()\n",
        "\n",
        "def get_span_module(input_dim, method=\"max\", use_proj=False, proj_dim=256):\n",
        "    \"\"\"Initializes the appropriate span representation class and returns the object.\n",
        "    \"\"\"\n",
        "    if method == \"avg\":\n",
        "        return AvgSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"max\":\n",
        "        return MaxSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"diff\":\n",
        "        return DiffSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"diff_sum\":\n",
        "        return DiffSumSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"endpoint\":\n",
        "        return EndPointRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"coherent\":\n",
        "        return CoherentSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"coherent_original\":\n",
        "        return CoherentOrigSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"attn\":\n",
        "        return AttnSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim)\n",
        "    elif method == \"coref\":\n",
        "        return AttnSpanRepr(input_dim, use_proj=use_proj, proj_dim=proj_dim, use_endpoints=True)\n",
        "    else:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYq1nlkAOKzw"
      },
      "source": [
        "class Edge_probe_model(nn.Module):\n",
        "    def __init__(self, num_of_spans, num_layers, input_span_len, embedding_dim, \n",
        "                 num_classes, pool_method='max', use_proj=True, proj_dim=256, \n",
        "                 hidden_dim=256, device='cuda', normalize_layers=False):\n",
        "        super(Edge_probe_model, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        self.num_of_spans = num_of_spans\n",
        "        self.weighing_params = nn.Parameter(torch.ones(self.num_layers))\n",
        "        self.input_dim = embedding_dim * num_of_spans\n",
        "        self.use_proj = use_proj\n",
        "        self.proj_dim = proj_dim\n",
        "        self.normalize_layers = normalize_layers\n",
        "\n",
        "        ## Projection\n",
        "        if use_proj:\n",
        "            # Apply a projection layer to output of pretrained models\n",
        "            # print(embedding_dim, num_layers, proj_dim)\n",
        "            self.proj1 = nn.Linear(embedding_dim, proj_dim)\n",
        "            if self.num_of_spans == 2:\n",
        "                self.proj2 = nn.Linear(embedding_dim, proj_dim)\n",
        "            # Update the input_dim\n",
        "            self.input_dim = proj_dim * num_of_spans\n",
        "\n",
        "        ## Pooling\n",
        "        self.pool_method = pool_method\n",
        "        input_dim = proj_dim if use_proj else embedding_dim\n",
        "        self.span1_pooling_net = get_span_module(input_dim, method=pool_method).to(device)\n",
        "        if self.num_of_spans == 2:\n",
        "            self.span2_pooling_net = get_span_module(input_dim, method=pool_method).to(device)\n",
        "\n",
        "        ## Classification\n",
        "        self.label_net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, self.num_classes),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.training_criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=5e-4, weight_decay=0)\n",
        "\n",
        "    def forward(self, spans_torch_dict):\n",
        "        span1_reprs = spans_torch_dict[\"span1\"]\n",
        "        span1_attention_mask = spans_torch_dict[\"span1_attention_mask\"]\n",
        "        if self.num_of_spans == 2:\n",
        "            span2_reprs = spans_torch_dict[\"span2\"]\n",
        "            span2_attention_mask = spans_torch_dict[\"span2_attention_mask\"]\n",
        "        # print(span1_reprs.shape)\n",
        "        \n",
        "        ## Projection\n",
        "        if self.use_proj:\n",
        "            span1_reprs = self.proj1(span1_reprs)\n",
        "            if self.num_of_spans == 2:\n",
        "                span2_reprs = self.proj2(span2_reprs)\n",
        "        \n",
        "        ## Pooling\n",
        "        pooled_span1 = self.span1_pooling_net(span1_reprs, span1_attention_mask)\n",
        "        if self.num_of_spans == 2:\n",
        "            pooled_span2 = self.span2_pooling_net(span2_reprs, span2_attention_mask)\n",
        "\n",
        "        # print(my_dataset_handler.tokenized_dataset[\"train\"][0])\n",
        "        # print(\"SPAN1\", span1_reprs[2, :, :, 0:5])\n",
        "        # print(\"SPAN2\", span2_reprs[2, :, :, 0:5])\n",
        "        # print(\"MAX1\", pooled_span1[2, :, 0:5])\n",
        "        # print(\"MAX2\", pooled_span2[2, :, 0:5])\n",
        "        # raise \"E\"\n",
        "        if self.normalize_layers:\n",
        "            pooled_span1 = torch.nn.functional.normalize(pooled_span1, dim=-1)\n",
        "            if self.num_of_spans == 2:\n",
        "                pooled_span2 = torch.nn.functional.normalize(pooled_span2, dim=-1)\n",
        "\n",
        "        if self.num_of_spans == 2:\n",
        "            output = torch.cat((pooled_span1, pooled_span2), dim=-1)\n",
        "        elif self.num_of_spans == 1:\n",
        "            output = pooled_span1\n",
        "        # print(output.shape)  # torch.Size([32, 13, 512])\n",
        "\n",
        "        ## Mixing Weights\n",
        "        wtd_encoded_repr = 0\n",
        "        soft_weight = nn.functional.softmax(self.weighing_params, dim=0)\n",
        "        for i in range(self.num_layers):\n",
        "            # print(i, output[:, i, :].shape, torch.norm(output[:, i, :]), torch.norm(s1))\n",
        "            # print(output[:, i, :][0, 0:10])\n",
        "            # print(s1[0, 0:10])\n",
        "            wtd_encoded_repr += soft_weight[i] * output[:, i, :]\n",
        "        # wtd_encoded_repr += soft_weight[-1] * encoded_layers[:, -1, :]\n",
        "        output = wtd_encoded_repr\n",
        "\n",
        "        ## Classification\n",
        "        pred_label = self.label_net(output)\n",
        "        pred_label = torch.squeeze(pred_label, dim=-1)\n",
        "        return pred_label\n",
        "\n",
        "    def summary(self):\n",
        "        print(self)\n",
        "        pytorch_total_params = sum(p.numel() for p in self.parameters())\n",
        "        pytorch_total_params_trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(\"Total Parameters:    \", pytorch_total_params)\n",
        "        print(\"Trainable Parameters:\", pytorch_total_params_trainable)\n",
        "        print(\"Pool Method:\", self.pool_method)\n",
        "        print(\"Projection:\", self.use_proj, self.proj_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmefJ5pCqq7D"
      },
      "source": [
        "# Edge Probe Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV_IoVoeLTot"
      },
      "source": [
        "class Trainer(ABC):\n",
        "    \"\"\" Abstract Trainer Class \"\"\"\n",
        "    @abstractmethod\n",
        "    def __init__(self, language_model, dataset_handler: Dataset_handler, \n",
        "                 verbose=True, device='cuda', edge_probe_model_checkpoint=None, \n",
        "                 pool_method=\"max\", start_eval = False, \n",
        "                 history_checkpoint=None, up_to_layer=-1, normalize_layers=False):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Edge_probe_trainer:\n",
        "    # Public:\n",
        "    def __init__(self, language_model, dataset_handler: Dataset_handler, \n",
        "                 verbose=True, device='cuda', edge_probe_model_checkpoint=None, \n",
        "                 pool_method=\"max\", start_eval = False, \n",
        "                 history_checkpoint=None, up_to_layer=-1, normalize_layers=False):\n",
        "        self.dataset_handler = dataset_handler\n",
        "        self.num_of_spans = self.dataset_handler.dataset_info.num_of_spans\n",
        "        self.up_to_layer = up_to_layer\n",
        "        self.language_model = language_model\n",
        "        self.language_model.config.output_hidden_states = True\n",
        "        self.device = device\n",
        "        self.verbose = verbose\n",
        "        self.start_eval = start_eval\n",
        "        def vprint(text):\n",
        "            if verbose:\n",
        "                print(datetime.datetime.now().time(), text)\n",
        "        self.vprint = vprint\n",
        "\n",
        "        self.current_hidden_states = None\n",
        "        self.last_input_ids = None\n",
        "        self.extracted_batch_embeddings = {}\n",
        "\n",
        "        self.vprint(\"Moving to device\")\n",
        "        for param in self.language_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.language_model.eval()\n",
        "        self.language_model.to(self.device)\n",
        "        num_layers, input_span_len, embedding_dim, num_classes = self.get_language_model_properties()\n",
        "        self.MLP_device = self.device\n",
        "        if edge_probe_model_checkpoint == None:\n",
        "            print(\"Creating New EPM\")\n",
        "            self.edge_probe_model = Edge_probe_model(\n",
        "                num_of_spans = self.num_of_spans,\n",
        "                num_layers = num_layers,\n",
        "                input_span_len = input_span_len,\n",
        "                embedding_dim = embedding_dim, \n",
        "                num_classes = num_classes,\n",
        "                device = self.MLP_device,\n",
        "                pool_method = pool_method,\n",
        "                normalize_layers = normalize_layers\n",
        "            )\n",
        "        else:\n",
        "            print(\"Starting From a Pretrained EPM\")\n",
        "            self.edge_probe_model = edge_probe_model_checkpoint\n",
        "        \n",
        "\n",
        "        if history_checkpoint is None:\n",
        "            self.history = {\"loss\": {\"train\": [], \"dev\": [], \"test\": []}, \n",
        "                            \"metrics\": \n",
        "                            {\"micro_f1\": {\"dev\": [], \"test\": []}},\n",
        "                            \"layers_weights\": []\n",
        "                            }\n",
        "            print(\"Creating New History\")\n",
        "        else:\n",
        "            print(\"Using History Checkpoint\")\n",
        "            self.history = history_checkpoint\n",
        "    \n",
        "    def train(self, batch_size, epochs=3):\n",
        "        tokenized_dataset = self.dataset_handler.tokenized_dataset[\"train\"]\n",
        "        tokenized_dataset_dev = self.dataset_handler.tokenized_dataset[\"dev\"]\n",
        "        tokenized_dataset_test = self.dataset_handler.tokenized_dataset[\"test\"]\n",
        "\n",
        "        # self.edge_probe_model.to(self.device)\n",
        "        self.edge_probe_model.to(self.MLP_device)\n",
        "        # self.vprint(\"Counting dataset rows\")\n",
        "        dataset_len = len(tokenized_dataset)\n",
        "        dev_dataset_len = len(tokenized_dataset_dev)\n",
        "        test_dataset_len = len(tokenized_dataset_test)\n",
        "        print(f\"Train on {dataset_len} samples, validate on {dev_dataset_len} samples, test on {test_dataset_len} samples\")\n",
        "        # dataset_len = 60\n",
        "        if self.start_eval:\n",
        "            self.update_history(epoch = 0)\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            steps = 0\n",
        "            self.draw_weights(epoch)\n",
        "            print(\"----------------\\n\")\n",
        "            self.edge_probe_model.train()\n",
        "            for i in tqdm(range(0, dataset_len, batch_size), desc=f\"[Epoch {epoch + 1}/{epochs}]\"):\n",
        "                # if int(i / batch_size) % 1000 == 0:\n",
        "                #     print(\"memory:\", psutil.virtual_memory().percent)\n",
        "                self.vprint(\"Start\")\n",
        "                step = batch_size\n",
        "                if i + batch_size > dataset_len:\n",
        "                    step = dataset_len - i\n",
        "                # print(f\"WWW[{i}, {i+step})\")\n",
        "                \n",
        "                self.vprint(\"Extracting\")\n",
        "                # self.vprint(\"prepare\")\n",
        "                spans_torch_dict = self.prepare_batch_data(tokenized_dataset, i, i + step, pad=True)\n",
        "                labels = spans_torch_dict[\"one_hot_labels\"]\n",
        "                # zero the parameter gradients\n",
        "                self.edge_probe_model.optimizer.zero_grad()\n",
        "    \n",
        "                # forward + backward + optimize\n",
        "                self.vprint(\"Forward MLP\")\n",
        "                # self.vprint(\"epm\")\n",
        "                outputs = self.edge_probe_model(spans_torch_dict)\n",
        "                self.vprint(\"Loss\")\n",
        "                loss = self.edge_probe_model.training_criterion(outputs.to(self.device), labels.float().to(self.device))\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.edge_probe_model.parameters(), 5.0)\n",
        "                self.edge_probe_model.optimizer.step()\n",
        "    \n",
        "                running_loss += loss.item()\n",
        "                steps += 1\n",
        "                self.vprint(\"Done\")\n",
        "                # print(f\"loss: {running_loss / steps}\")\n",
        "\n",
        "            self.update_history(epoch + 1, train_loss = running_loss / steps)\n",
        "            \n",
        "\n",
        "    def calc_loss(self, tokenized_dataset, batch_size=16, print_metrics=False, just_micro=False, desc=\"\"):\n",
        "        self.edge_probe_model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0\n",
        "            dataset_len = len(tokenized_dataset[\"input_ids\"])\n",
        "            steps = 0\n",
        "            preds = None\n",
        "            for i in tqdm(range(0, dataset_len, batch_size), desc=desc):\n",
        "                # if int(i / batch_size) % 100 == 0:\n",
        "                #     print(\"memory:\", psutil.virtual_memory().percent, gc.collect(), psutil.virtual_memory().percent)\n",
        "                step = batch_size\n",
        "                if i + batch_size > dataset_len:\n",
        "                    step = dataset_len - i\n",
        "\n",
        "                spans_torch_dict = self.prepare_batch_data(tokenized_dataset, i, i + step, pad=True)\n",
        "                labels = spans_torch_dict[\"one_hot_labels\"]\n",
        "                # forward\n",
        "                outputs = self.edge_probe_model(spans_torch_dict)\n",
        "                \n",
        "                preds = outputs if i == 0 else torch.cat((preds, outputs), 0)\n",
        "                loss = self.edge_probe_model.training_criterion(outputs.to(self.device), labels.float().to(self.device))\n",
        "                running_loss += loss.item()\n",
        "                steps += 1\n",
        "\n",
        "        preds = preds.cpu().argmax(-1)\n",
        "        y_true = np.array(tokenized_dataset[\"one_hot_label\"]).argmax(-1)\n",
        "        print(preds[0:9])\n",
        "        print(y_true[0:9])\n",
        "        micro_f1 = f1_score(y_true, preds, average='micro')\n",
        "        \n",
        "        if print_metrics:\n",
        "            labels_list = self.dataset_handler.labels_list\n",
        "            if not just_micro:\n",
        "                print(classification_report(y_true, preds, target_names=labels_list, labels=range(len(labels_list))))\n",
        "            print(\"MICRO F1:\", micro_f1)\n",
        "        return running_loss / steps, micro_f1\n",
        "\n",
        "    # Private:\n",
        "    def update_history(self, epoch, train_loss = None):\n",
        "        if train_loss is None:\n",
        "            train_loss, train_f1 = self.calc_loss(self.dataset_handler.tokenized_dataset[\"train\"], print_metrics=True, desc=\"Train Loss\")\n",
        "        dev_loss, dev_f1 = self.calc_loss(self.dataset_handler.tokenized_dataset[\"dev\"], print_metrics=True, desc=\"Dev Loss\")\n",
        "        test_loss, test_f1 = self.calc_loss(self.dataset_handler.tokenized_dataset[\"test\"], print_metrics=True, desc=\"Test Loss\")\n",
        "        self.history[\"loss\"][\"train\"].append(train_loss)\n",
        "        self.history[\"loss\"][\"dev\"].append(dev_loss)\n",
        "        self.history[\"loss\"][\"test\"].append(test_loss)\n",
        "        self.history[\"metrics\"][\"micro_f1\"][\"dev\"].append(dev_f1)\n",
        "        self.history[\"metrics\"][\"micro_f1\"][\"test\"].append(test_f1)\n",
        "        self.history[\"layers_weights\"].append(self.edge_probe_model.weighing_params.tolist())\n",
        "        print('[%d] loss: %.4f, val_loss: %.4f, test_loss: %.4f' % (epoch, self.history[\"loss\"][\"train\"][-1], self.history[\"loss\"][\"dev\"][-1], self.history[\"loss\"][\"test\"][-1]))\n",
        "\n",
        "    def draw_weights(self, epoch=0):\n",
        "        if(epoch % 1 == 0):\n",
        "            w = self.edge_probe_model.weighing_params.tolist()\n",
        "            print(w)\n",
        "            print(self.history)\n",
        "            plt.bar(np.arange(len(w), dtype=int), w)\n",
        "            plt.ylabel('Weight')\n",
        "            plt.xlabel('Layer');\n",
        "            plt.show()\n",
        "\n",
        "            wsoft = nn.functional.softmax(self.edge_probe_model.weighing_params)\n",
        "            print(\"CG\", sum(idx*val for idx, val in enumerate(wsoft)))\n",
        "\n",
        "            print(\"Loss History\")\n",
        "            loss_history = self.history[\"loss\"]\n",
        "            x = range(len(loss_history[\"train\"]))\n",
        "            plt.plot(x, loss_history[\"train\"])\n",
        "            plt.plot(x, loss_history[\"dev\"])\n",
        "            plt.plot(x, loss_history[\"test\"])\n",
        "            plt.legend(['Train', 'Dev', 'Test'], loc='lower left')\n",
        "            plt.show()\n",
        "\n",
        "            print(\"Micro f1 History\")\n",
        "            f1_history = self.history[\"metrics\"][\"micro_f1\"]\n",
        "            x = range(len(f1_history[\"dev\"]))\n",
        "            plt.plot(x, f1_history[\"dev\"])\n",
        "            plt.plot(x, f1_history[\"test\"])\n",
        "            plt.legend(['Dev', 'Test'], loc='upper left')\n",
        "            plt.show()\n",
        "\n",
        "    def prepare_batch_data(self, tokenized_dataset, start_idx, end_idx, pad=False):\n",
        "        # self.vprint(\"Extracting From Model\")\n",
        "        span_representations_dict = self.extract_embeddings(tokenized_dataset, start_idx, end_idx, pad=True)\n",
        "        # self.vprint(\"To Device\")\n",
        "        span1_torch = torch.stack(span_representations_dict[\"span1\"]).float().to(self.MLP_device)  # (batch_size, #layers, max_span_len, embd_dim)\n",
        "        span1_attention_mask_torch = torch.stack(span_representations_dict[\"span1_attention_mask\"])\n",
        "        one_hot_labels_torch = torch.tensor(np.array(span_representations_dict[\"one_hot_label\"]))\n",
        "        if self.num_of_spans == 2:\n",
        "            span2_torch = torch.stack(span_representations_dict[\"span2\"]).float().to(self.MLP_device)\n",
        "            span2_attention_mask_torch = torch.stack(span_representations_dict[\"span1_attention_mask\"])\n",
        "            spans_torch_dict = {\"span1\": span1_torch, \n",
        "                                \"span2\": span2_torch, \n",
        "                                \"span1_attention_mask\": span1_attention_mask_torch, \n",
        "                                \"span2_attention_mask\": span2_attention_mask_torch, \n",
        "                                \"one_hot_labels\": one_hot_labels_torch}\n",
        "        elif self.num_of_spans == 1:\n",
        "            spans_torch_dict = {\"span1\": span1_torch, \n",
        "                                \"span1_attention_mask\": span1_attention_mask_torch, \n",
        "                                \"one_hot_labels\": one_hot_labels_torch}\n",
        "\n",
        "        return spans_torch_dict\n",
        "\n",
        "    def get_language_model_properties(self):\n",
        "        span_representations_dict = self.extract_embeddings(self.dataset_handler.tokenized_dataset[\"train\"], 0, 3, pad=True)\n",
        "        for i in span_representations_dict[\"span1\"]:\n",
        "            print(i.shape)\n",
        "        span1_torch = span_representations_dict[\"span1\"]\n",
        "        num_layers = span1_torch[0].shape[0]\n",
        "        span_len = span1_torch[0].shape[1]\n",
        "        embedding_dim = span1_torch[0].shape[2]\n",
        "        # if self.verbose:\n",
        "        #     display(pd.DataFrame(span_representations_dict))\n",
        "        return num_layers, span_len, embedding_dim, len(self.dataset_handler.labels_list)\n",
        "\n",
        "    def pad_span(self, span_repr, max_len):\n",
        "        \"\"\" pad spans in embeddings to max_len \n",
        "        input:\n",
        "            span_representation: df with shape (#layers, span_len, embedding_dim)\n",
        "        returns:\n",
        "            padded_spans: np with shape (batch_len, num_layers, max_len, embedding_dim)\n",
        "            attention_mask: np with shape (max_len), values = 1: data, 0: padding\n",
        "        \"\"\"\n",
        "        shape = span_repr.shape\n",
        "        num_layers = shape[0]\n",
        "        span_original_len = shape[1]\n",
        "        embedding_dim = shape[2]\n",
        "        # padded_span_repr = np.zeros((num_layers, max_len, embedding_dim))\n",
        "        # if span_original_len > max_len:\n",
        "        #     raise Exception(f\"Error: {span_original_len} is more than max_span_len {max_len}\\n{span_repr.shape}\")\n",
        "        attention_mask = torch.tensor(np.array([1] * span_original_len + [0] * (max_len - span_original_len)), dtype=torch.int8, device=self.device)\n",
        "        padded_span_repr = torch.cat((span_repr, torch.zeros((num_layers, max_len - span_original_len, embedding_dim), device=self.device)), axis=1)\n",
        "        # assert attention_mask.shape == (max_len, ), f\"{attention_mask}, {attention_mask.shape} != ({max_len}, )\"\n",
        "        # assert padded_span_repr.shape == (num_layers, max_len, embedding_dim)\n",
        "        return padded_span_repr, attention_mask\n",
        "\n",
        "    def init_span_dict(self, num_of_spans, pad):\n",
        "        if num_of_spans == 2:\n",
        "            span_repr = {\"span1\": [], \"span2\": [], \"label\": [], \"one_hot_label\": []}\n",
        "        else:\n",
        "            span_repr = {\"span1\": [], \"label\": [], \"one_hot_label\": []}\n",
        "        \n",
        "        if pad:\n",
        "            span_repr[\"span1_attention_mask\"] = []\n",
        "            span_repr[\"span2_attention_mask\"] = []\n",
        "        return span_repr\n",
        "\n",
        "    def extract_batch(self, tokenized_dataset, idx, unique_batch_size=32):\n",
        "        # print(idx)\n",
        "        self.vprint(\"e1\")\n",
        "        dataset_len = len(tokenized_dataset)\n",
        "        unique_texts_in_batch = []\n",
        "        i = idx\n",
        "        while len(unique_texts_in_batch) < unique_batch_size and i < dataset_len:\n",
        "            # print(i)\n",
        "            text = tokenized_dataset[i][\"text\"]\n",
        "            if not text in unique_texts_in_batch:\n",
        "                unique_texts_in_batch.append(text)\n",
        "            i += 1\n",
        "        tokenizer.padding_side = 'right'  # Important: lef will change the span indices\n",
        "        tokenized_batch = tokenizer(unique_texts_in_batch, padding=True, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.language_model(**tokenized_batch)\n",
        "        # torch.cuda.synchronize()\n",
        "        # current_hidden_states = np.asarray([val.detach().cpu().numpy() for val in outputs.hidden_states])\n",
        "        current_hidden_states = torch.stack([val.detach() for val in outputs.hidden_states])  # TODO: use only stack, no list \n",
        "        # self.vprint(current_hidden_states.shape)  # (13, 16, 34, 768)\n",
        "        \n",
        "        extracted_batch_embeddings = {}\n",
        "        for i, unique_text in enumerate(unique_texts_in_batch):\n",
        "            hashable_input = repr(unique_text)\n",
        "            if self.up_to_layer == -1:\n",
        "                extracted_batch_embeddings[hashable_input] = current_hidden_states[:, i, :, :]\n",
        "            else:\n",
        "                extracted_batch_embeddings[hashable_input] = current_hidden_states[:self.up_to_layer+1, i, :, :]\n",
        "        self.vprint(\"e2\")\n",
        "        return extracted_batch_embeddings\n",
        "    \n",
        "    def pad_sequence(list_of_torch, pad_len, pad_value=0):\n",
        "        shape = list_of_torch[0].shape\n",
        "        num_layers = shape[0]\n",
        "        span_original_len = shape[1]\n",
        "        embedding_dim = shape[2]\n",
        "        output = torch.zeros()\n",
        "\n",
        "    def extract_embeddings(self, tokenized_dataset, start_idx, end_idx, pad=True):\n",
        "        \"\"\" Extract raw embeddings for [start_idx, end_idx) of tokenized_dataset from language_model \n",
        "            \n",
        "        Returns:\n",
        "            extract_embeddings: DataFrame with cols (span1, span2?, label) and span shape is (range_len, (#layers, span_len, embedding_dim))\n",
        "        \"\"\"\n",
        "        num_of_spans = self.dataset_handler.dataset_info.num_of_spans\n",
        "        \n",
        "        if num_of_spans == 2:\n",
        "            max_span_len_in_batch = max(max(tokenized_dataset[start_idx:end_idx][\"span1_len\"]), max(tokenized_dataset[start_idx:end_idx][\"span2_len\"]))\n",
        "        elif num_of_spans == 1:\n",
        "            max_span_len_in_batch = max(tokenized_dataset[start_idx:end_idx][\"span1_len\"])\n",
        "        # print(\"max_span_len_in_batch\", max_span_len_in_batch)\n",
        "        \n",
        "\n",
        "        span_repr = self.init_span_dict(num_of_spans, pad)\n",
        "        self.vprint(\"f1\")\n",
        "        for i in range(start_idx, end_idx):\n",
        "            hashable_input = repr(tokenized_dataset[i][\"text\"])\n",
        "            \n",
        "            if hashable_input not in self.extracted_batch_embeddings:\n",
        "                self.extracted_batch_embeddings = self.extract_batch(tokenized_dataset, i)\n",
        "                            \n",
        "            self.current_hidden_states = self.extracted_batch_embeddings[hashable_input]\n",
        "            \n",
        "            row = tokenized_dataset[i]\n",
        "            span1_hidden_states = self.current_hidden_states[:, row[\"span1\"][0]:row[\"span1\"][1], :]  # (#layer, span_len, embd_dim)\n",
        "            if pad:\n",
        "                s1, a1 = self.pad_span(span1_hidden_states, max_span_len_in_batch)\n",
        "                span_repr[\"span1\"].append(s1)\n",
        "                span_repr[\"span1_attention_mask\"].append(a1)\n",
        "            else:\n",
        "                span_repr[\"span1\"].append(span1_hidden_states)\n",
        "            if num_of_spans == 2:\n",
        "                span2_hidden_states = self.current_hidden_states[:, row[\"span2\"][0]:row[\"span2\"][1], :]\n",
        "                if pad:\n",
        "                    s2, a2 = self.pad_span(span2_hidden_states, max_span_len_in_batch)\n",
        "                    span_repr[\"span2\"].append(s2)\n",
        "                    span_repr[\"span2_attention_mask\"].append(a2)\n",
        "                else:\n",
        "                    span_repr[\"span2\"].append(span2_hidden_states)\n",
        "            span_repr[\"one_hot_label\"].append(row[\"one_hot_label\"])\n",
        "            span_repr[\"label\"].append(row[\"label\"])\n",
        "        self.vprint(\"f2\")\n",
        "        return span_repr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4M8CJNOhsur"
      },
      "source": [
        "my_edge_probe_trainer = None\n",
        "edge_probe_model_checkpoint = None\n",
        "history = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG0ukbYWFpRW"
      },
      "source": [
        "try:\n",
        "    edge_probe_model_checkpoint = my_edge_probe_trainer.edge_probe_model\n",
        "except:\n",
        "    edge_probe_model_checkpoint = None\n",
        "my_edge_probe_trainer = Edge_probe_trainer(model,\n",
        "                                           my_dataset_handler, \n",
        "                                           device=DEVICE,\n",
        "                                           pool_method=POOL_METHOD,\n",
        "                                           edge_probe_model_checkpoint=edge_probe_model_checkpoint,\n",
        "                                           history_checkpoint=history,\n",
        "                                           up_to_layer = 6,\n",
        "                                           normalize_layers=True,\n",
        "                                           verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiQfH3OZtS-N"
      },
      "source": [
        "print(\"Model:\", model_checkpoint)\n",
        "print(\"Dataset:\", my_dataset_info.dataset_name)\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "my_edge_probe_trainer.edge_probe_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiXy4GDCHZsr"
      },
      "source": [
        "my_edge_probe_trainer.train(batch_size = BATCH_SIZE, epochs=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYVZDCxOdlJP"
      },
      "source": [
        "torch.save(my_edge_probe_trainer.edge_probe_model.state_dict(), \"EPM_xlnet-large-cased-attn_epoch3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRGBY2i5EbMM"
      },
      "source": [
        "history = my_edge_probe_trainer.history\n",
        "print(my_edge_probe_trainer.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POIzA5NWN28E"
      },
      "source": [
        "print(\"Loss History\")\n",
        "loss_history = my_edge_probe_trainer.history[\"loss\"]\n",
        "print(loss_history)\n",
        "print(\"Train Loss:\", loss_history[\"train\"])\n",
        "print(\"Dev Loss:\", loss_history[\"dev\"])\n",
        "print(\"Test Loss:\", loss_history[\"test\"])\n",
        "\n",
        "x = range(len(loss_history[\"train\"]))\n",
        "plt.plot(x, loss_history[\"train\"])\n",
        "plt.plot(x, loss_history[\"dev\"])\n",
        "plt.plot(x, loss_history[\"test\"])\n",
        "plt.legend(['Train', 'Dev', 'Test'], loc='lower left')\n",
        "plt.show()\n",
        "print(\".\")\n",
        "\n",
        "print(\"Micro f1 History\")\n",
        "f1_history = my_edge_probe_trainer.history[\"metrics\"][\"micro_f1\"]\n",
        "print(f1_history)\n",
        "print(\"Dev f1:\", f1_history[\"dev\"])\n",
        "print(\"Test f1:\", f1_history[\"test\"])\n",
        "\n",
        "\n",
        "\n",
        "x = range(len(f1_history[\"dev\"]))\n",
        "plt.plot(x, f1_history[\"dev\"])\n",
        "plt.plot(x, f1_history[\"test\"])\n",
        "plt.legend(['Dev', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "print(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT6GHy0Kf8Ku"
      },
      "source": [
        "# Diagnostic Probe Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrlQj658BoKR"
      },
      "source": [
        "class Diagnostic_probe_trainer:\n",
        "    # Public:\n",
        "    def __init__(self, language_model, dataset_handler: Dataset_handler, \n",
        "                 verbose=True, device='cuda',\n",
        "                 pool_method=\"max\", start_eval = False, normalize_layers=False):\n",
        "        self.dataset_handler = dataset_handler\n",
        "        self.num_of_spans = self.dataset_handler.dataset_info.num_of_spans\n",
        "        self.language_model = language_model\n",
        "        self.language_model.config.output_hidden_states = True\n",
        "        self.device = device\n",
        "        self.verbose = verbose\n",
        "        self.start_eval = start_eval\n",
        "        def vprint(text):\n",
        "            if verbose:\n",
        "                print(datetime.datetime.now().time(), text)\n",
        "        self.vprint = vprint\n",
        "\n",
        "        self.current_hidden_states = None\n",
        "        self.last_input_ids = None\n",
        "        self.extracted_batch_embeddings = {}\n",
        "\n",
        "        self.vprint(\"Moving to device\")\n",
        "        for param in self.language_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.language_model.eval()\n",
        "        self.language_model.to(self.device)\n",
        "        num_layers, input_span_len, embedding_dim, num_classes = self.get_language_model_properties()\n",
        "        print(num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.MLP_device = self.device\n",
        "        \n",
        "        print(\"Creating New EPM\")\n",
        "        self.edge_probe_models = []\n",
        "        for i in range(num_layers):\n",
        "            edge_probe_model = Edge_probe_model(\n",
        "                num_of_spans = self.num_of_spans,\n",
        "                num_layers = 1,\n",
        "                input_span_len = input_span_len,\n",
        "                embedding_dim = embedding_dim, \n",
        "                num_classes = num_classes,\n",
        "                device = self.MLP_device,\n",
        "                pool_method = pool_method,\n",
        "                normalize_layers = normalize_layers\n",
        "            )\n",
        "            self.edge_probe_models.append(edge_probe_model)\n",
        "        \n",
        "        self.history = {\"loss\": {\"train\": [], \"dev\": [], \"test\": []}, \n",
        "                        \"metrics\": \n",
        "                        {\"micro_f1\": {\"dev\": [], \"test\": []}},\n",
        "                        \"layers_weights\": []\n",
        "                        }\n",
        "        print(\"Creating New History\")\n",
        "\n",
        "    def train(self, batch_size, epochs=3):\n",
        "        tokenized_dataset = self.dataset_handler.tokenized_dataset[\"train\"]\n",
        "        tokenized_dataset_dev = self.dataset_handler.tokenized_dataset[\"dev\"]\n",
        "        tokenized_dataset_test = self.dataset_handler.tokenized_dataset[\"test\"]\n",
        "\n",
        "        # self.edge_probe_model.to(self.device)\n",
        "        for edge_probe_model in self.edge_probe_models:\n",
        "            edge_probe_model.to(self.MLP_device)\n",
        "        # self.vprint(\"Counting dataset rows\")\n",
        "        dataset_len = len(tokenized_dataset)\n",
        "        dev_dataset_len = len(tokenized_dataset_dev)\n",
        "        test_dataset_len = len(tokenized_dataset_test)\n",
        "        print(f\"Train on {dataset_len} samples, validate on {dev_dataset_len} samples, test on {test_dataset_len} samples\")\n",
        "        # dataset_len = 60\n",
        "        if self.start_eval:\n",
        "            self.update_history(epoch = 0)\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            steps = 0\n",
        "            self.draw_weights(epoch)\n",
        "            print(\"----------------\\n\")\n",
        "            for edge_probe_model in self.edge_probe_models:\n",
        "                edge_probe_model.train()\n",
        "            for i in tqdm(range(0, dataset_len, batch_size), desc=f\"[Epoch {epoch + 1}/{epochs}]\"):\n",
        "                # if int(i / batch_size) % 1000 == 0:\n",
        "                #     print(\"memory:\", psutil.virtual_memory().percent)\n",
        "                self.vprint(\"Start\")\n",
        "                step = batch_size\n",
        "                if i + batch_size > dataset_len:\n",
        "                    step = dataset_len - i\n",
        "                # print(f\"WWW[{i}, {i+step})\")\n",
        "                \n",
        "                self.vprint(\"Extracting\")\n",
        "                # self.vprint(\"prepare\")\n",
        "                spans_torch_dict = self.prepare_batch_data(tokenized_dataset, i, i + step, pad=True)\n",
        "                # print(spans_torch_dict[\"span1\"].shape, spans_torch_dict[\"span1_attention_mask\"].shape)\n",
        "                labels = spans_torch_dict[\"one_hot_labels\"]\n",
        "                labels = labels.float().to(self.device)\n",
        "                \n",
        "                for epm_idx, edge_probe_model in enumerate(self.edge_probe_models):\n",
        "                    # zero the parameter gradients\n",
        "                    # for param_tensor in edge_probe_model.state_dict():\n",
        "                    #     print(epm_idx, param_tensor, \"\\t\", edge_probe_model.state_dict()[param_tensor].size(), torch.norm(edge_probe_model.state_dict()[param_tensor]))\n",
        "                    # print(epm_idx, edge_probe_model.state_dict()[\"label_net.4.bias\"])\n",
        "                    edge_probe_model.optimizer.zero_grad()\n",
        "        \n",
        "                    self.vprint(\"dict\")\n",
        "                    # print(spans_torch_dict[\"span1\"].shape) # torch.Size([32, 13, 9, 768])\n",
        "                    if self.num_of_spans == 2:\n",
        "                        span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :], \n",
        "                                           \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"],\n",
        "                                           \"span2\": spans_torch_dict[\"span2\"][:, epm_idx:epm_idx+1, :, :],\n",
        "                                           \"span2_attention_mask\": spans_torch_dict[\"span2_attention_mask\"],\n",
        "                                           }\n",
        "                    else:\n",
        "                        span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :], \n",
        "                                           \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"]}\n",
        "                    \n",
        "                    # forward + backward + optimize\n",
        "                    self.vprint(\"Forward MLP\")\n",
        "                    outputs = edge_probe_model(span_torch_dict)\n",
        "                    self.vprint(\"Loss\")\n",
        "                    loss = edge_probe_model.training_criterion(outputs.to(self.device), labels)\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(edge_probe_model.parameters(), 5.0)\n",
        "                    edge_probe_model.optimizer.step()\n",
        "        \n",
        "                    running_loss += loss.item()\n",
        "                    steps += 1\n",
        "                self.vprint(\"Done\")\n",
        "                # print(f\"loss: {running_loss / steps}\")\n",
        "\n",
        "            self.update_history(epoch + 1, train_loss = running_loss / steps)\n",
        "            \n",
        "\n",
        "    def calc_loss(self, tokenized_dataset, batch_size=16, print_metrics=False, just_micro=False, desc=\"\"):\n",
        "        for edge_probe_model in self.edge_probe_models:\n",
        "            edge_probe_model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0\n",
        "            dataset_len = len(tokenized_dataset[\"input_ids\"])\n",
        "            steps = 0\n",
        "            preds = [None] * self.num_layers\n",
        "            micro_f1 = [None] * self.num_layers\n",
        "            for i in tqdm(range(0, dataset_len, batch_size), desc=desc):\n",
        "                # if int(i / batch_size) % 100 == 0:\n",
        "                #     print(\"memory:\", psutil.virtual_memory().percent, gc.collect(), psutil.virtual_memory().percent)\n",
        "                step = batch_size\n",
        "                if i + batch_size > dataset_len:\n",
        "                    step = dataset_len - i\n",
        "\n",
        "                spans_torch_dict = self.prepare_batch_data(tokenized_dataset, i, i + step, pad=True)\n",
        "                labels = spans_torch_dict[\"one_hot_labels\"]\n",
        "                labels = labels.float().to(self.device)\n",
        "\n",
        "                for epm_idx, edge_probe_model in enumerate(self.edge_probe_models):\n",
        "                    if self.num_of_spans == 2:\n",
        "                        span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :], \n",
        "                                           \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"],\n",
        "                                           \"span2\": spans_torch_dict[\"span2\"][:, epm_idx:epm_idx+1, :, :],\n",
        "                                           \"span2_attention_mask\": spans_torch_dict[\"span2_attention_mask\"],\n",
        "                                           }\n",
        "                    else:\n",
        "                        span_torch_dict = {\"span1\": spans_torch_dict[\"span1\"][:, epm_idx:epm_idx+1, :, :], \n",
        "                                           \"span1_attention_mask\": spans_torch_dict[\"span1_attention_mask\"]}\n",
        "\n",
        "                    # forward\n",
        "                    outputs = edge_probe_model(span_torch_dict)\n",
        "                    \n",
        "                    preds[epm_idx] = outputs if i == 0 else torch.cat((preds[epm_idx], outputs), 0)\n",
        "                    loss = edge_probe_model.training_criterion(outputs.to(self.device), labels)\n",
        "                    running_loss += loss.item()\n",
        "                    steps += 1\n",
        "\n",
        "        y_true = np.array(tokenized_dataset[\"one_hot_label\"]).argmax(-1)\n",
        "        for idx, pred in enumerate(preds): \n",
        "            pred = pred.cpu().argmax(-1)\n",
        "            micro_f1[idx] = f1_score(y_true, pred, average='micro')\n",
        "        \n",
        "        if print_metrics:\n",
        "            # labels_list = self.dataset_handler.labels_list\n",
        "            # if not just_micro:\n",
        "            #     print(classification_report(y_true, preds, target_names=labels_list, labels=range(len(labels_list))))\n",
        "            print(\"MICRO F1:\", micro_f1)\n",
        "        return running_loss / steps, micro_f1\n",
        "\n",
        "    # Private:\n",
        "    def update_history(self, epoch, train_loss = None):\n",
        "        if train_loss is None:\n",
        "            train_loss, train_f1 = self.calc_loss(self.dataset_handler.tokenized_dataset[\"train\"], print_metrics=True, desc=\"Train Loss\")\n",
        "        dev_loss, dev_f1 = self.calc_loss(self.dataset_handler.tokenized_dataset[\"dev\"], print_metrics=True, desc=\"Dev Loss\")\n",
        "        test_loss, test_f1 = self.calc_loss(self.dataset_handler.tokenized_dataset[\"test\"], print_metrics=True, desc=\"Test Loss\")\n",
        "        self.history[\"loss\"][\"train\"].append(train_loss)\n",
        "        self.history[\"loss\"][\"dev\"].append(dev_loss)\n",
        "        self.history[\"loss\"][\"test\"].append(test_loss)\n",
        "        self.history[\"metrics\"][\"micro_f1\"][\"dev\"].append(dev_f1)\n",
        "        self.history[\"metrics\"][\"micro_f1\"][\"test\"].append(test_f1)\n",
        "        # self.history[\"layers_weights\"].append(self.edge_probe_model.weighing_params.tolist())\n",
        "        print('[%d] loss:' % (epoch))\n",
        "        print(\"Train Loss:\", self.history[\"loss\"][\"train\"][-1])\n",
        "        print(\"Dev Loss:\", self.history[\"loss\"][\"dev\"][-1])\n",
        "        print(\"Test Loss:\", self.history[\"loss\"][\"test\"][-1])\n",
        "        # print('[%d] loss: %.4f, val_loss: %.4f, test_loss: %.4f' % (epoch, self.history[\"loss\"][\"train\"][-1], self.history[\"loss\"][\"dev\"][-1], self.history[\"loss\"][\"test\"][-1]))\n",
        "\n",
        "    def draw_weights(self, epoch=0):\n",
        "        if(epoch > 0):\n",
        "            # w = self.edge_probe_models.weighing_params.tolist()\n",
        "            # print(w)\n",
        "            w = self.history[\"metrics\"][\"micro_f1\"][\"test\"][-1]\n",
        "            print(self.history)\n",
        "            plt.bar(np.arange(len(w), dtype=int), w)\n",
        "            plt.ylabel('f1')\n",
        "            plt.xlabel('Layer');\n",
        "            plt.show()\n",
        "\n",
        "            # wsoft = nn.functional.softmax(self.edge_probe_model.weighing_params)\n",
        "            # print(\"CG\", sum(idx*val for idx, val in enumerate(wsoft)))\n",
        "\n",
        "            print(\"Loss History\")\n",
        "            loss_history = self.history[\"loss\"]\n",
        "            x = range(len(loss_history[\"train\"]))\n",
        "            plt.plot(x, loss_history[\"train\"])\n",
        "            plt.plot(x, loss_history[\"dev\"])\n",
        "            plt.plot(x, loss_history[\"test\"])\n",
        "            plt.legend(['Train', 'Dev', 'Test'], loc='lower left')\n",
        "            plt.show()\n",
        "\n",
        "            # print(\"Micro f1 History\")\n",
        "            # f1_history = self.history[\"metrics\"][\"micro_f1\"]\n",
        "            # x = range(len(f1_history[\"dev\"]))\n",
        "            # plt.plot(x, f1_history[\"dev\"])\n",
        "            # plt.plot(x, f1_history[\"test\"])\n",
        "            # plt.legend(['Dev', 'Test'], loc='upper left')\n",
        "            # plt.show()\n",
        "\n",
        "    def prepare_batch_data(self, tokenized_dataset, start_idx, end_idx, pad=False):\n",
        "        # self.vprint(\"Extracting From Model\")\n",
        "        span_representations_dict = self.extract_embeddings(tokenized_dataset, start_idx, end_idx, pad=True)\n",
        "        # self.vprint(\"To Device\")\n",
        "        span1_torch = torch.stack(span_representations_dict[\"span1\"]).float().to(self.MLP_device)  # (batch_size, #layers, max_span_len, embd_dim)\n",
        "        span1_attention_mask_torch = torch.stack(span_representations_dict[\"span1_attention_mask\"])\n",
        "        one_hot_labels_torch = torch.tensor(np.array(span_representations_dict[\"one_hot_label\"]))\n",
        "        if self.num_of_spans == 2:\n",
        "            span2_torch = torch.stack(span_representations_dict[\"span2\"]).float().to(self.MLP_device)\n",
        "            span2_attention_mask_torch = torch.stack(span_representations_dict[\"span1_attention_mask\"])\n",
        "            spans_torch_dict = {\"span1\": span1_torch, \n",
        "                                \"span2\": span2_torch, \n",
        "                                \"span1_attention_mask\": span1_attention_mask_torch, \n",
        "                                \"span2_attention_mask\": span2_attention_mask_torch, \n",
        "                                \"one_hot_labels\": one_hot_labels_torch}\n",
        "        elif self.num_of_spans == 1:\n",
        "            spans_torch_dict = {\"span1\": span1_torch, \n",
        "                                \"span1_attention_mask\": span1_attention_mask_torch, \n",
        "                                \"one_hot_labels\": one_hot_labels_torch}\n",
        "\n",
        "        return spans_torch_dict\n",
        "\n",
        "    def get_language_model_properties(self):\n",
        "        span_representations_dict = self.extract_embeddings(self.dataset_handler.tokenized_dataset[\"train\"], 0, 3, pad=True)\n",
        "        for i in span_representations_dict[\"span1\"]:\n",
        "            print(i.shape)\n",
        "        span1_torch = span_representations_dict[\"span1\"]\n",
        "        num_layers = span1_torch[0].shape[0]\n",
        "        span_len = span1_torch[0].shape[1]\n",
        "        embedding_dim = span1_torch[0].shape[2]\n",
        "        # if self.verbose:\n",
        "        #     display(pd.DataFrame(span_representations_dict))\n",
        "        return num_layers, span_len, embedding_dim, len(self.dataset_handler.labels_list)\n",
        "\n",
        "    def pad_span(self, span_repr, max_len):\n",
        "        \"\"\" pad spans in embeddings to max_len \n",
        "        input:\n",
        "            span_representation: df with shape (#layers, span_len, embedding_dim)\n",
        "        returns:\n",
        "            padded_spans: np with shape (batch_len, num_layers, max_len, embedding_dim)\n",
        "            attention_mask: np with shape (max_len), values = 1: data, 0: padding\n",
        "        \"\"\"\n",
        "        shape = span_repr.shape\n",
        "        num_layers = shape[0]\n",
        "        span_original_len = shape[1]\n",
        "        embedding_dim = shape[2]\n",
        "        # padded_span_repr = np.zeros((num_layers, max_len, embedding_dim))\n",
        "        # if span_original_len > max_len:\n",
        "        #     raise Exception(f\"Error: {span_original_len} is more than max_span_len {max_len}\\n{span_repr.shape}\")\n",
        "        attention_mask = torch.tensor(np.array([1] * span_original_len + [0] * (max_len - span_original_len)), dtype=torch.int8, device=self.device)\n",
        "        padded_span_repr = torch.cat((span_repr, torch.zeros((num_layers, max_len - span_original_len, embedding_dim), device=self.device)), axis=1)\n",
        "        # assert attention_mask.shape == (max_len, ), f\"{attention_mask}, {attention_mask.shape} != ({max_len}, )\"\n",
        "        # assert padded_span_repr.shape == (num_layers, max_len, embedding_dim)\n",
        "        return padded_span_repr, attention_mask\n",
        "\n",
        "    def init_span_dict(self, num_of_spans, pad):\n",
        "        if num_of_spans == 2:\n",
        "            span_repr = {\"span1\": [], \"span2\": [], \"label\": [], \"one_hot_label\": []}\n",
        "        else:\n",
        "            span_repr = {\"span1\": [], \"label\": [], \"one_hot_label\": []}\n",
        "        \n",
        "        if pad:\n",
        "            span_repr[\"span1_attention_mask\"] = []\n",
        "            span_repr[\"span2_attention_mask\"] = []\n",
        "        return span_repr\n",
        "\n",
        "    def extract_batch(self, tokenized_dataset, idx, unique_batch_size=32):\n",
        "        # print(idx)\n",
        "        self.vprint(\"e1\")\n",
        "        dataset_len = len(tokenized_dataset)\n",
        "        unique_texts_in_batch = []\n",
        "        i = idx\n",
        "        while len(unique_texts_in_batch) < unique_batch_size and i < dataset_len:\n",
        "            # print(i)\n",
        "            text = tokenized_dataset[i][\"text\"]\n",
        "            if not text in unique_texts_in_batch:\n",
        "                unique_texts_in_batch.append(text)\n",
        "            i += 1\n",
        "        tokenizer.padding_side = 'right'  # Important: lef will change the span indices\n",
        "        tokenized_batch = tokenizer(unique_texts_in_batch, padding=True, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.language_model(**tokenized_batch)\n",
        "        # torch.cuda.synchronize()\n",
        "        # current_hidden_states = np.asarray([val.detach().cpu().numpy() for val in outputs.hidden_states])\n",
        "        current_hidden_states = torch.stack([val.detach() for val in outputs.hidden_states])  # TODO: use only stack, no list \n",
        "        # self.vprint(current_hidden_states.shape)  # (13, 16, 34, 768)\n",
        "        \n",
        "        extracted_batch_embeddings = {}\n",
        "        for i, unique_text in enumerate(unique_texts_in_batch):\n",
        "            hashable_input = repr(unique_text)\n",
        "            extracted_batch_embeddings[hashable_input] = current_hidden_states[:, i, :, :]\n",
        "        self.vprint(\"e2\")\n",
        "        return extracted_batch_embeddings\n",
        "    \n",
        "    def pad_sequence(list_of_torch, pad_len, pad_value=0):\n",
        "        shape = list_of_torch[0].shape\n",
        "        num_layers = shape[0]\n",
        "        span_original_len = shape[1]\n",
        "        embedding_dim = shape[2]\n",
        "        output = torch.zeros()\n",
        "\n",
        "    def extract_embeddings(self, tokenized_dataset, start_idx, end_idx, pad=True):\n",
        "        \"\"\" Extract raw embeddings for [start_idx, end_idx) of tokenized_dataset from language_model \n",
        "            \n",
        "        Returns:\n",
        "            extract_embeddings: DataFrame with cols (span1, span2?, label) and span shape is (range_len, (#layers, span_len, embedding_dim))\n",
        "        \"\"\"\n",
        "        num_of_spans = self.dataset_handler.dataset_info.num_of_spans\n",
        "        \n",
        "        if num_of_spans == 2:\n",
        "            max_span_len_in_batch = max(max(tokenized_dataset[start_idx:end_idx][\"span1_len\"]), max(tokenized_dataset[start_idx:end_idx][\"span2_len\"]))\n",
        "        elif num_of_spans == 1:\n",
        "            max_span_len_in_batch = max(tokenized_dataset[start_idx:end_idx][\"span1_len\"])\n",
        "        # print(\"max_span_len_in_batch\", max_span_len_in_batch)\n",
        "        \n",
        "\n",
        "        span_repr = self.init_span_dict(num_of_spans, pad)\n",
        "        self.vprint(\"f1\")\n",
        "        for i in range(start_idx, end_idx):\n",
        "            hashable_input = repr(tokenized_dataset[i][\"text\"])\n",
        "            \n",
        "            if hashable_input not in self.extracted_batch_embeddings:\n",
        "                self.extracted_batch_embeddings = self.extract_batch(tokenized_dataset, i)\n",
        "                            \n",
        "            self.current_hidden_states = self.extracted_batch_embeddings[hashable_input]\n",
        "            \n",
        "            row = tokenized_dataset[i]\n",
        "            span1_hidden_states = self.current_hidden_states[:, row[\"span1\"][0]:row[\"span1\"][1], :]  # (#layer, span_len, embd_dim)\n",
        "            if pad:\n",
        "                s1, a1 = self.pad_span(span1_hidden_states, max_span_len_in_batch)\n",
        "                span_repr[\"span1\"].append(s1)\n",
        "                span_repr[\"span1_attention_mask\"].append(a1)\n",
        "            else:\n",
        "                span_repr[\"span1\"].append(span1_hidden_states)\n",
        "            if num_of_spans == 2:\n",
        "                span2_hidden_states = self.current_hidden_states[:, row[\"span2\"][0]:row[\"span2\"][1], :]\n",
        "                if pad:\n",
        "                    s2, a2 = self.pad_span(span2_hidden_states, max_span_len_in_batch)\n",
        "                    span_repr[\"span2\"].append(s2)\n",
        "                    span_repr[\"span2_attention_mask\"].append(a2)\n",
        "                else:\n",
        "                    span_repr[\"span2\"].append(span2_hidden_states)\n",
        "            span_repr[\"one_hot_label\"].append(row[\"one_hot_label\"])\n",
        "            span_repr[\"label\"].append(row[\"label\"])\n",
        "        self.vprint(\"f2\")\n",
        "        return span_repr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fjbKKcagPM3"
      },
      "source": [
        "my_diagnostic_probe_trainer = Diagnostic_probe_trainer(model,\n",
        "                                           my_dataset_handler, \n",
        "                                           device=DEVICE,\n",
        "                                           pool_method=POOL_METHOD,\n",
        "                                           normalize_layers=False,\n",
        "                                           verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg40gSSfzuAC"
      },
      "source": [
        "print(\"Model:\", model_checkpoint)\n",
        "print(\"Dataset:\", my_dataset_info.dataset_name)\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "my_diagnostic_probe_trainer.edge_probe_models[0].summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXoDIKvsigKw"
      },
      "source": [
        "my_diagnostic_probe_trainer.train(batch_size = BATCH_SIZE, epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSBeN4XS5DI3"
      },
      "source": [
        "history = my_diagnostic_probe_trainer.history\n",
        "print(my_diagnostic_probe_trainer.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnWHI6Vd5Lz6"
      },
      "source": [
        "print(\"Loss History\")\n",
        "loss_history = my_diagnostic_probe_trainer.history[\"loss\"]\n",
        "print(loss_history)\n",
        "print(\"Train Loss:\", loss_history[\"train\"])\n",
        "print(\"Dev Loss:\", loss_history[\"dev\"])\n",
        "print(\"Test Loss:\", loss_history[\"test\"])\n",
        "\n",
        "x = range(len(loss_history[\"train\"]))\n",
        "plt.plot(x, loss_history[\"train\"])\n",
        "plt.plot(x, loss_history[\"dev\"])\n",
        "plt.plot(x, loss_history[\"test\"])\n",
        "plt.legend(['Train', 'Dev', 'Test'], loc='lower left')\n",
        "plt.show()\n",
        "print(\".\")\n",
        "\n",
        "print(\"Micro f1 History\")\n",
        "f1_history = my_diagnostic_probe_trainer.history[\"metrics\"][\"micro_f1\"]\n",
        "print(f1_history)\n",
        "print(\"Dev f1:\", f1_history[\"dev\"])\n",
        "print(\"Test f1:\", f1_history[\"test\"])\n",
        "\n",
        "\n",
        "\n",
        "x = range(len(f1_history[\"dev\"][-1]))\n",
        "plt.plot(x, f1_history[\"dev\"][-1])\n",
        "plt.plot(x, f1_history[\"test\"][-1])\n",
        "plt.legend(['Dev', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "print(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlFgB6DA5pDt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}